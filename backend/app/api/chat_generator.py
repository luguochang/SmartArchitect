from fastapi import APIRouter, HTTPException
from typing import Optional
import logging
import asyncio

from app.models.schemas import (
    FlowTemplateList,
    ChatGenerationRequest,
    ChatGenerationResponse
)
from app.services.chat_generator import create_chat_generator_service
from fastapi.responses import StreamingResponse
import json
from app.services.ai_vision import create_vision_service

router = APIRouter()
logger = logging.getLogger(__name__)


@router.get("/chat-generator/templates", response_model=FlowTemplateList)
async def get_flow_templates():
    """è·å–æ‰€æœ‰æµç¨‹å›¾ç¤ºä¾‹æ¨¡æ¿

    Returns:
        FlowTemplateList: åŒ…å«é¢„è®¾æ¨¡æ¿ï¼ˆOOMæ’æŸ¥ã€è®¢å•æµç¨‹ç­‰ï¼‰
    """
    try:
        service = create_chat_generator_service()
        return service.get_all_templates()
    except Exception as e:
        logger.error(f"Failed to get templates: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to load templates: {str(e)}"
        )


@router.post("/chat-generator/generate", response_model=ChatGenerationResponse)
async def generate_flowchart(request: ChatGenerationRequest):
    """æ ¹æ®ç”¨æˆ·æè¿°ç”Ÿæˆæµç¨‹å›¾

    Args:
        request: åŒ…å«ç”¨æˆ·è¾“å…¥ã€æ¨¡æ¿ IDã€providerã€API key ç­‰é…ç½®

    Returns:
        ChatGenerationResponse: ç”Ÿæˆçš„æµç¨‹å›¾èŠ‚ç‚¹ã€è¾¹ã€Mermaid ä»£ç 
    """
    try:
        logger.info(f"Generating flowchart from user input: {request.user_input[:50]}...")

        service = create_chat_generator_service()
        result = await service.generate_flowchart(
            request=request,
            provider=request.provider or "gemini",
            api_key=request.api_key,
            base_url=request.base_url,
            model_name=request.model_name
        )

        if not result.success:
            raise HTTPException(
                status_code=400,
                detail=result.message or "Generation failed"
            )

        logger.info(f"Generated: {len(result.nodes)} nodes, {len(result.edges)} edges")
        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Flowchart generation failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Generation error: {str(e)}"
        )


@router.post("/chat-generator/generate-stream")
async def generate_flowchart_stream(request: ChatGenerationRequest):
    """Streaming version of flowchart generation (SSE-like text/event-stream)."""

    async def event_stream():
        try:
            logger.info(f"[STREAM] Starting stream generation for: {request.user_input[:50]}")
            logger.info(f"[STREAM] Provider: {request.provider}, Model: {request.model_name}")

            service = create_chat_generator_service()
            selected_provider = request.provider or "gemini"

            # Determine effective diagram type
            effective_diagram_type = request.diagram_type or "flow"
            if request.template_id:
                tpl = service.get_template(request.template_id)
                if tpl and tpl.category == "architecture":
                    effective_diagram_type = "architecture"

            logger.info(f"Stream generation: diagram_type={effective_diagram_type}, provider={selected_provider}")

            prompt_request = request.model_copy(update={"diagram_type": effective_diagram_type})
            prompt = service._build_generation_prompt(prompt_request)

            yield "data: [START] building prompt\n\n"
            logger.info("[STREAM] Sent START event")
            yield "data: [CALL] contacting provider...\n\n"
            logger.info("[STREAM] Sent CALL event")

            # Prefer true stream for OpenAI-compatible providers
            if selected_provider in ["siliconflow", "openai", "custom"]:
                try:
                    vision_service = create_vision_service(
                        provider=selected_provider,
                        api_key=request.api_key,
                        base_url=request.base_url,
                        model_name=request.model_name
                    )

                    # Stream tokens
                    accumulated = ""
                    logger.info(f"[STREAM] Creating streaming request to {selected_provider} with model {vision_service.model_name}")

                    stream = vision_service.client.chat.completions.create(
                        model=vision_service.model_name,
                        messages=[{"role": "user", "content": prompt}],
                        max_tokens=4096,
                        temperature=0.2,
                        stream=True,
                    )
                except Exception as init_error:
                    logger.error(f"[STREAM] Failed to initialize streaming: {init_error}", exc_info=True)
                    yield f"data: [ERROR] Failed to initialize AI streaming: {str(init_error)}\n\n"
                    return

                try:
                    for chunk in stream:
                        # Some providers may emit empty heartbeats; guard against missing choices
                        if not getattr(chunk, "choices", None):
                            continue
                        delta = chunk.choices[0].delta.content if chunk.choices[0].delta else None
                        if not delta:
                            continue
                        text = "".join(delta)
                        accumulated += text
                        yield f"data: [TOKEN] {text}\n\n"
                except Exception as stream_error:
                    logger.error(f"[STREAM] Error during streaming: {stream_error}", exc_info=True)
                    yield f"data: [ERROR] Streaming interrupted: {str(stream_error)}\n\n"
                    return

                # Parse final JSON and normalize; if parse fails, fall back to non-stream path
                try:
                    if not accumulated.strip():
                        raise ValueError("Empty stream output")

                    logger.info(f"[STREAM] Accumulated response length: {len(accumulated)}")
                    logger.info(f"[STREAM] First 500 chars: {accumulated[:500]}")
                    ai_data = service._safe_json(accumulated)
                    logger.info(f"[STREAM] Parsed AI data keys: {list(ai_data.keys())}")

                    # Use correct normalization based on diagram type
                    if effective_diagram_type == "architecture":
                        nodes, edges, mermaid_code = service._normalize_architecture_graph(ai_data)
                        edges = []  # Architecture diagrams don't show edges
                    else:
                        nodes, edges, mermaid_code = service._normalize_ai_graph(ai_data)

                    logger.info(f"[STREAM] After normalization: {len(nodes)} nodes, {len(edges)} edges")

                    # ğŸ¬ æµå¼å‘é€èŠ‚ç‚¹å’Œè¾¹ï¼Œå®ç°çœŸæ­£çš„æµå¼ç”»å›¾æ•ˆæœ
                    # 1. å…ˆå‘é€å®Œæ•´æ•°æ®ç”¨äºå‰ç«¯å¸ƒå±€è®¡ç®—
                    layout_data = {
                        "nodes": [n.model_dump() if hasattr(n, 'model_dump') else n for n in nodes],
                        "edges": [e.model_dump() if hasattr(e, 'model_dump') else e for e in edges],
                        "diagram_type": effective_diagram_type,
                        "mermaid_code": mermaid_code
                    }
                    yield f"data: [LAYOUT_DATA] {json.dumps(layout_data, ensure_ascii=False)}\n\n"
                    logger.info(f"[STREAM] Sent LAYOUT_DATA with {len(nodes)} nodes")

                    # 2. çŸ­æš‚ç­‰å¾…ï¼Œè®©å‰ç«¯å®Œæˆå¸ƒå±€è®¡ç®—
                    await asyncio.sleep(0.15)

                    # 3. é€ä¸ªå‘é€èŠ‚ç‚¹æ˜¾ç¤ºæŒ‡ä»¤
                    yield f"data: [RESULT] nodes={len(nodes)}, edges={len(edges)}\n\n"
                    for i, node in enumerate(nodes):
                        node_data = node.model_dump() if hasattr(node, 'model_dump') else node
                        node_id = node_data.get('id')
                        yield f"data: [NODE_SHOW] {node_id}\n\n"
                        logger.info(f"[STREAM] Sent NODE_SHOW {i+1}/{len(nodes)}: {node_id}")
                        # æ§åˆ¶æ˜¾ç¤ºé€Ÿåº¦ï¼š200ms/èŠ‚ç‚¹
                        if i < len(nodes) - 1:
                            await asyncio.sleep(0.2)

                    # 4. çŸ­æš‚å»¶è¿Ÿåå¼€å§‹æ˜¾ç¤ºè¾¹
                    await asyncio.sleep(0.3)

                    # 5. é€ä¸ªå‘é€è¾¹æ˜¾ç¤ºæŒ‡ä»¤
                    for i, edge in enumerate(edges):
                        edge_data = edge.model_dump() if hasattr(edge, 'model_dump') else edge
                        edge_id = edge_data.get('id')
                        yield f"data: [EDGE_SHOW] {edge_id}\n\n"
                        logger.info(f"[STREAM] Sent EDGE_SHOW {i+1}/{len(edges)}: {edge_id}")
                        # æ§åˆ¶æ˜¾ç¤ºé€Ÿåº¦ï¼š100ms/è¾¹
                        if i < len(edges) - 1:
                            await asyncio.sleep(0.1)

                    yield "data: [END] done\n\n"
                    logger.info("[STREAM] Completed progressive rendering")
                    return
                except Exception as parse_err:
                    logger.warning(f"[STREAM] JSON parse failed; falling back to non-stream: {parse_err}")
                    logger.exception(parse_err)
                    # explicitly fall through to non-stream path

            # Fallback: call existing generator (non-stream) but send staged events
            logger.info(f"[STREAM] Falling back to non-stream generate_flowchart()")
            result = await service.generate_flowchart(
                request=prompt_request,
                provider=selected_provider,
                api_key=request.api_key,
                base_url=request.base_url,
                model_name=request.model_name
            )

            # ğŸ¬ åŒæ ·ä½¿ç”¨æµå¼å‘é€
            # 1. å…ˆå‘é€å¸ƒå±€æ•°æ®
            layout_data = {
                "nodes": [n.model_dump() if hasattr(n, 'model_dump') else n for n in result.nodes],
                "edges": [e.model_dump() if hasattr(e, 'model_dump') else e for e in result.edges],
                "diagram_type": effective_diagram_type,
                "mermaid_code": result.mermaid_code
            }
            yield f"data: [LAYOUT_DATA] {json.dumps(layout_data, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.15)

            # 2. é€ä¸ªå‘é€èŠ‚ç‚¹
            yield f"data: [RESULT] nodes={len(result.nodes)}, edges={len(result.edges)}\n\n"
            for i, node in enumerate(result.nodes):
                node_data = node.model_dump() if hasattr(node, 'model_dump') else node
                node_id = node_data.get('id')
                yield f"data: [NODE_SHOW] {node_id}\n\n"
                if i < len(result.nodes) - 1:
                    await asyncio.sleep(0.2)

            # 3. å»¶è¿Ÿåå‘é€è¾¹
            await asyncio.sleep(0.3)
            for i, edge in enumerate(result.edges):
                edge_data = edge.model_dump() if hasattr(edge, 'model_dump') else edge
                edge_id = edge_data.get('id')
                yield f"data: [EDGE_SHOW] {edge_id}\n\n"
                if i < len(result.edges) - 1:
                    await asyncio.sleep(0.1)

            yield "data: [END] done\n\n"
        except Exception as e:
            logger.error(f"Stream generation failed: {e}", exc_info=True)
            yield f"data: [ERROR] {str(e)}\n\n"

    return StreamingResponse(
        event_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache, no-store, must-revalidate, max-age=0",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Content-Encoding": "none",
            "Transfer-Encoding": "chunked",
        },
    )
