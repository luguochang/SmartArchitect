from fastapi import APIRouter, HTTPException
from typing import Optional
import logging
import asyncio

from app.models.schemas import (
    FlowTemplateList,
    ChatGenerationRequest,
    ChatGenerationResponse
)
from app.services.chat_generator import create_chat_generator_service
from fastapi.responses import StreamingResponse
import json
from app.services.ai_vision import create_vision_service

router = APIRouter()
logger = logging.getLogger(__name__)


@router.get("/chat-generator/templates", response_model=FlowTemplateList)
async def get_flow_templates():
    """è·å–æ‰€æœ‰æµç¨‹å›¾ç¤ºä¾‹æ¨¡æ¿

    Returns:
        FlowTemplateList: åŒ…å«é¢„è®¾æ¨¡æ¿ï¼ˆOOMæ’æŸ¥ã€è®¢å•æµç¨‹ç­‰ï¼‰
    """
    try:
        service = create_chat_generator_service()
        return service.get_all_templates()
    except Exception as e:
        logger.error(f"Failed to get templates: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to load templates: {str(e)}"
        )


@router.post("/chat-generator/generate", response_model=ChatGenerationResponse)
async def generate_flowchart(request: ChatGenerationRequest):
    """æ ¹æ®ç”¨æˆ·æè¿°ç”Ÿæˆæµç¨‹å›¾

    Args:
        request: åŒ…å«ç”¨æˆ·è¾“å…¥ã€æ¨¡æ¿ IDã€providerã€API key ç­‰é…ç½®

    Returns:
        ChatGenerationResponse: ç”Ÿæˆçš„æµç¨‹å›¾èŠ‚ç‚¹ã€è¾¹ã€Mermaid ä»£ç 
    """
    try:
        logger.info(f"Generating flowchart from user input: {request.user_input[:50]}...")

        service = create_chat_generator_service()
        result = await service.generate_flowchart(
            request=request,
            provider=request.provider or "gemini",
            api_key=request.api_key,
            base_url=request.base_url,
            model_name=request.model_name
        )

        if not result.success:
            raise HTTPException(
                status_code=400,
                detail=result.message or "Generation failed"
            )

        logger.info(f"Generated: {len(result.nodes)} nodes, {len(result.edges)} edges")
        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Flowchart generation failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Generation error: {str(e)}"
        )


@router.post("/chat-generator/generate-stream")
async def generate_flowchart_stream(request: ChatGenerationRequest):
    """Streaming version of flowchart generation (SSE-like text/event-stream)."""

    async def event_stream():
        try:
            logger.info(f"[STREAM] Starting stream generation for: {request.user_input[:50]}")
            logger.info(f"[STREAM] Provider: {request.provider}, Model: {request.model_name}")

            service = create_chat_generator_service()
            selected_provider = request.provider or "gemini"

            # Determine effective diagram type
            effective_diagram_type = request.diagram_type or "flow"
            if request.template_id:
                tpl = service.get_template(request.template_id)
                if tpl and tpl.category == "architecture":
                    effective_diagram_type = "architecture"

            logger.info(f"Stream generation: diagram_type={effective_diagram_type}, provider={selected_provider}")

            prompt_request = request.model_copy(update={"diagram_type": effective_diagram_type})
            prompt = service._build_generation_prompt(prompt_request)

            yield "data: [START] building prompt\n\n"
            logger.info("[STREAM] Sent START event")
            yield "data: [CALL] contacting provider...\n\n"
            logger.info("[STREAM] Sent CALL event")

            # Prefer true stream for OpenAI-compatible providers
            if selected_provider in ["siliconflow", "openai", "custom"]:
                try:
                    vision_service = create_vision_service(
                        provider=selected_provider,
                        api_key=request.api_key,
                        base_url=request.base_url,
                        model_name=request.model_name
                    )

                    # Stream tokens
                    accumulated = ""
                    logger.info(f"[STREAM] Creating streaming request to {selected_provider} with model {vision_service.model_name}")

                    # æ£€æµ‹æ˜¯å¦éœ€è¦ä½¿ç”¨ raw HTTPï¼ˆikuncode.cc ä¼šé˜»æ‹¦ SDKï¼‰
                    use_raw_http = (
                        selected_provider == "custom" and
                        vision_service.custom_base_url and
                        "ikuncode.cc" in vision_service.custom_base_url.lower()
                    )

                    if use_raw_http:
                        # ikuncode.ccï¼šä½¿ç”¨ raw HTTP streaming
                        logger.info("[STREAM] Detected ikuncode.cc, using raw HTTP streaming")
                        import httpx

                        # æ¸…ç† base_url
                        clean_base_url = vision_service.custom_base_url.rstrip('/')
                        if clean_base_url.endswith('/v1'):
                            clean_base_url = clean_base_url[:-3]

                        headers = {
                            "x-api-key": vision_service.custom_api_key,
                            "anthropic-version": "2023-06-01",
                            "content-type": "application/json"
                        }

                        data = {
                            "model": vision_service.model_name,
                            "max_tokens": 4096,
                            "messages": [{"role": "user", "content": prompt}],
                            "stream": True
                        }

                        logger.info(f"[STREAM] Sending request to: {clean_base_url}/v1/messages")

                        # ç›´æ¥åœ¨è¿™é‡Œå¤„ç†æµå¼å“åº”
                        async with httpx.AsyncClient(timeout=120.0) as http_client:
                            async with http_client.stream("POST", f"{clean_base_url}/v1/messages", headers=headers, json=data) as response:
                                if response.status_code != 200:
                                    error_text = await response.aread()
                                    logger.error(f"[STREAM] API error: {response.status_code} - {error_text}")
                                    yield f"data: [ERROR] API request failed: {response.status_code}\n\n"
                                    return

                                logger.info("[STREAM] Starting raw HTTP token streaming...")
                                current_event = None
                                buffer = ""

                                async for chunk in response.aiter_bytes():
                                    buffer += chunk.decode('utf-8')

                                    while '\n' in buffer:
                                        line, buffer = buffer.split('\n', 1)
                                        line = line.strip()

                                        if not line:
                                            continue

                                        if line.startswith("event: "):
                                            current_event = line[7:]
                                        elif line.startswith("data: "):
                                            data_str = line[6:]
                                            try:
                                                data_json = json.loads(data_str)
                                                if current_event == "content_block_delta":
                                                    delta = data_json.get("delta", {})
                                                    if delta.get("type") == "text_delta":
                                                        text = delta.get("text", "")
                                                        if text:
                                                            accumulated += text
                                                            yield f"data: [TOKEN] {text}\n\n"
                                            except json.JSONDecodeError:
                                                continue

                    else:
                        # æ£€æµ‹æ˜¯å¦æ˜¯ Claude æ¨¡å‹ï¼ˆlinkflow.run ç­‰ï¼‰
                        is_claude_model = selected_provider == "custom" and vision_service.model_name and "claude" in vision_service.model_name.lower()

                        if is_claude_model:
                            # ä½¿ç”¨ Anthropic streaming API
                            logger.info("[STREAM] Using Anthropic streaming API")
                            stream = vision_service.client.messages.stream(
                                model=vision_service.model_name,
                                messages=[{"role": "user", "content": prompt}],
                                max_tokens=4096,
                                temperature=0.2,
                            )
                        else:
                            # ä½¿ç”¨ OpenAI streaming API
                            stream = vision_service.client.chat.completions.create(
                                model=vision_service.model_name,
                                messages=[{"role": "user", "content": prompt}],
                                max_tokens=4096,
                                temperature=0.2,
                                stream=True,
                            )
                except Exception as init_error:
                    logger.error(f"[STREAM] Failed to initialize streaming: {init_error}", exc_info=True)
                    yield f"data: [ERROR] Failed to initialize AI streaming: {str(init_error)}\n\n"
                    return

                # åªæœ‰é raw HTTP çš„æƒ…å†µæ‰éœ€è¦è¿™é‡Œå¤„ç†æµå¼
                if not use_raw_http:
                    try:
                        if is_claude_model:
                            # Anthropic streaming APIä½¿ç”¨context manager
                            with stream as s:
                                for text in s.text_stream:
                                    accumulated += text
                                    yield f"data: [TOKEN] {text}\n\n"
                        else:
                            # OpenAI streaming API
                            for chunk in stream:
                                # Some providers may emit empty heartbeats; guard against missing choices
                                if not getattr(chunk, "choices", None):
                                    continue
                                delta = chunk.choices[0].delta.content if chunk.choices[0].delta else None
                                if not delta:
                                    continue
                                text = "".join(delta)
                                accumulated += text
                                yield f"data: [TOKEN] {text}\n\n"
                    except Exception as stream_error:
                        logger.error(f"[STREAM] Error during streaming: {stream_error}", exc_info=True)
                        yield f"data: [ERROR] Streaming interrupted: {str(stream_error)}\n\n"
                        return

                # Parse final JSON and normalize; if parse fails, fall back to non-stream path
                try:
                    if not accumulated.strip():
                        raise ValueError("Empty stream output")

                    logger.info(f"[STREAM] Accumulated response length: {len(accumulated)}")
                    logger.info(f"[STREAM] First 500 chars: {accumulated[:500]}")
                    ai_data = service._safe_json(accumulated)
                    logger.info(f"[STREAM] Parsed AI data keys: {list(ai_data.keys())}")

                    # Use correct normalization based on diagram type
                    if effective_diagram_type == "architecture":
                        nodes, edges, mermaid_code = service._normalize_architecture_graph(ai_data)
                        edges = []  # Architecture diagrams don't show edges
                    else:
                        nodes, edges, mermaid_code = service._normalize_ai_graph(ai_data)

                    logger.info(f"[STREAM] After normalization: {len(nodes)} nodes, {len(edges)} edges")

                    # ğŸ¬ æµå¼å‘é€èŠ‚ç‚¹å’Œè¾¹ï¼Œå®ç°çœŸæ­£çš„æµå¼ç”»å›¾æ•ˆæœ
                    # 1. å…ˆå‘é€å®Œæ•´æ•°æ®ç”¨äºå‰ç«¯å¸ƒå±€è®¡ç®—
                    layout_data = {
                        "nodes": [n.model_dump() if hasattr(n, 'model_dump') else n for n in nodes],
                        "edges": [e.model_dump() if hasattr(e, 'model_dump') else e for e in edges],
                        "diagram_type": effective_diagram_type,
                        "mermaid_code": mermaid_code
                    }
                    yield f"data: [LAYOUT_DATA] {json.dumps(layout_data, ensure_ascii=False)}\n\n"
                    logger.info(f"[STREAM] Sent LAYOUT_DATA with {len(nodes)} nodes")

                    # 2. çŸ­æš‚ç­‰å¾…ï¼Œè®©å‰ç«¯å®Œæˆå¸ƒå±€è®¡ç®—
                    await asyncio.sleep(0.15)

                    # 3. é€ä¸ªå‘é€èŠ‚ç‚¹æ˜¾ç¤ºæŒ‡ä»¤
                    yield f"data: [RESULT] nodes={len(nodes)}, edges={len(edges)}\n\n"
                    for i, node in enumerate(nodes):
                        node_data = node.model_dump() if hasattr(node, 'model_dump') else node
                        node_id = node_data.get('id')
                        yield f"data: [NODE_SHOW] {node_id}\n\n"
                        logger.info(f"[STREAM] Sent NODE_SHOW {i+1}/{len(nodes)}: {node_id}")
                        # æ§åˆ¶æ˜¾ç¤ºé€Ÿåº¦ï¼š200ms/èŠ‚ç‚¹
                        if i < len(nodes) - 1:
                            await asyncio.sleep(0.2)

                    # 4. çŸ­æš‚å»¶è¿Ÿåå¼€å§‹æ˜¾ç¤ºè¾¹
                    await asyncio.sleep(0.3)

                    # 5. é€ä¸ªå‘é€è¾¹æ˜¾ç¤ºæŒ‡ä»¤
                    for i, edge in enumerate(edges):
                        edge_data = edge.model_dump() if hasattr(edge, 'model_dump') else edge
                        edge_id = edge_data.get('id')
                        yield f"data: [EDGE_SHOW] {edge_id}\n\n"
                        logger.info(f"[STREAM] Sent EDGE_SHOW {i+1}/{len(edges)}: {edge_id}")
                        # æ§åˆ¶æ˜¾ç¤ºé€Ÿåº¦ï¼š100ms/è¾¹
                        if i < len(edges) - 1:
                            await asyncio.sleep(0.1)

                    yield "data: [END] done\n\n"
                    logger.info("[STREAM] Completed progressive rendering")
                    return
                except Exception as parse_err:
                    logger.warning(f"[STREAM] JSON parse failed; falling back to non-stream: {parse_err}")
                    logger.exception(parse_err)
                    # explicitly fall through to non-stream path

            # Fallback: call existing generator (non-stream) but send staged events
            logger.info(f"[STREAM] Falling back to non-stream generate_flowchart()")
            result = await service.generate_flowchart(
                request=prompt_request,
                provider=selected_provider,
                api_key=request.api_key,
                base_url=request.base_url,
                model_name=request.model_name
            )

            # ğŸ¬ åŒæ ·ä½¿ç”¨æµå¼å‘é€
            # 1. å…ˆå‘é€å¸ƒå±€æ•°æ®
            layout_data = {
                "nodes": [n.model_dump() if hasattr(n, 'model_dump') else n for n in result.nodes],
                "edges": [e.model_dump() if hasattr(e, 'model_dump') else e for e in result.edges],
                "diagram_type": effective_diagram_type,
                "mermaid_code": result.mermaid_code
            }
            yield f"data: [LAYOUT_DATA] {json.dumps(layout_data, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.15)

            # 2. é€ä¸ªå‘é€èŠ‚ç‚¹
            yield f"data: [RESULT] nodes={len(result.nodes)}, edges={len(result.edges)}\n\n"
            for i, node in enumerate(result.nodes):
                node_data = node.model_dump() if hasattr(node, 'model_dump') else node
                node_id = node_data.get('id')
                yield f"data: [NODE_SHOW] {node_id}\n\n"
                if i < len(result.nodes) - 1:
                    await asyncio.sleep(0.2)

            # 3. å»¶è¿Ÿåå‘é€è¾¹
            await asyncio.sleep(0.3)
            for i, edge in enumerate(result.edges):
                edge_data = edge.model_dump() if hasattr(edge, 'model_dump') else edge
                edge_id = edge_data.get('id')
                yield f"data: [EDGE_SHOW] {edge_id}\n\n"
                if i < len(result.edges) - 1:
                    await asyncio.sleep(0.1)

            yield "data: [END] done\n\n"
        except Exception as e:
            logger.error(f"Stream generation failed: {e}", exc_info=True)
            yield f"data: [ERROR] {str(e)}\n\n"

    return StreamingResponse(
        event_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache, no-store, must-revalidate, max-age=0",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Content-Encoding": "none",
            "Transfer-Encoding": "chunked",
        },
    )
