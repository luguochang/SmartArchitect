import asyncio
import base64
import json
import re
from typing import Optional, Dict, Any, List
import logging

# AI SDK imports
try:
    import google.generativeai as genai
except ImportError:
    genai = None

try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

try:
    from anthropic import Anthropic
except ImportError:
    Anthropic = None

from app.core.config import settings
from app.models.schemas import (
    ImageAnalysisResponse,
    Node,
    Edge,
    Position,
    NodeData,
    AIAnalysis,
    ArchitectureBottleneck,
    OptimizationSuggestion
)

logger = logging.getLogger(__name__)


class AIVisionService:
    """AI Vision å¤šæ¨¡æ€å›¾ç‰‡åˆ†ææœåŠ¡"""

    def __init__(
        self,
        provider: str = "gemini",
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model_name: Optional[str] = None
    ):
        self.provider = provider
        self.client = None
        self.custom_api_key = api_key
        self.custom_base_url = base_url
        self.custom_model_name = model_name
        default_model = (
            "claude-3-5-sonnet-20241022"
            if provider == "claude"
            else "Qwen/Qwen3-VL-32B-Thinking"
            if provider == "siliconflow"
            else "gpt-4o-mini"
        )
        self.model_name = model_name or default_model
        self.request_timeout = 60.0  # Increased from 8 to 60 seconds to handle slow AI responses
        # Allow fast fallback when running with placeholder keys (e.g., tests)
        self.mock_mode = api_key == "invalid"
        self._init_client()

    def _init_client(self):
        """åˆå§‹åŒ– AI å®¢æˆ·ç«¯"""
        try:
            if self.provider == "gemini":
                if not genai:
                    raise ImportError("google-generativeai not installed")
                # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ api_keyï¼Œå¦åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡
                api_key = self.custom_api_key or settings.GEMINI_API_KEY
                if not api_key:
                    raise ValueError("GEMINI_API_KEY not configured. Please set in UI or .env file")
                genai.configure(api_key=api_key)
                self.client = genai.GenerativeModel("gemini-2.0-flash-exp")
                logger.info("Gemini client initialized")

            elif self.provider == "openai":
                if not OpenAI:
                    raise ImportError("openai not installed")
                # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ api_keyï¼Œå¦åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡
                api_key = self.custom_api_key or settings.OPENAI_API_KEY
                if not api_key:
                    raise ValueError("OPENAI_API_KEY not configured. Please set in UI or .env file")
                self.client = OpenAI(
                    api_key=api_key,
                    timeout=120.0,  # 2 minutes timeout
                    max_retries=2   # Limit retries
                )
                logger.info("OpenAI client initialized")

            elif self.provider == "claude":
                if not Anthropic:
                    raise ImportError("anthropic not installed")
                # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ api_keyï¼Œå¦åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡
                api_key = self.custom_api_key or settings.ANTHROPIC_API_KEY
                if not api_key:
                    raise ValueError("ANTHROPIC_API_KEY not configured. Please set in UI or .env file")
                self.client = Anthropic(api_key=api_key)
                logger.info("Claude client initialized")

            elif self.provider == "siliconflow":
                if not OpenAI:
                    raise ImportError("openai not installed")
                api_key = self.custom_api_key or settings.SILICONFLOW_API_KEY
                if not api_key:
                    raise ValueError("SILICONFLOW_API_KEY not configured. Please set in UI or .env file")
                base_url = self.custom_base_url or settings.SILICONFLOW_BASE_URL
                self.client = OpenAI(
                    api_key=api_key,
                    base_url=base_url,
                    timeout=240.0,  # å¢åŠ åˆ° 240 ç§’åŒ¹é… flowchart_timeout
                    max_retries=0   # ç¦ç”¨é‡è¯•ï¼ˆè§†è§‰æ¨¡å‹å¤„ç†æ…¢ï¼Œé‡è¯•ä¼šå¯¼è‡´è¶…æ—¶ï¼‰
                )
                logger.info(f"SiliconFlow client initialized with base_url: {base_url}")

            elif self.provider == "custom":
                # è‡ªå®šä¹‰ provider
                # å¦‚æœæ¨¡å‹åç§°åŒ…å« claudeï¼Œä½¿ç”¨ Anthropic SDKï¼ˆå…¼å®¹ Claude API ä»£ç†ï¼‰
                # å¦åˆ™ä½¿ç”¨ OpenAI SDKï¼ˆå…¼å®¹ OpenAI APIï¼‰
                if not self.custom_api_key:
                    raise ValueError("Custom API key not provided")
                if not self.custom_base_url:
                    raise ValueError("Custom base URL not provided")

                # æ£€æµ‹æ˜¯å¦æ˜¯ Claude æ¨¡å‹
                is_claude_model = "claude" in self.model_name.lower()

                if is_claude_model:
                    # ä½¿ç”¨ Anthropic SDK å¤„ç† Claude ä»£ç†ï¼ˆå¦‚ linkflow.runï¼‰
                    if not Anthropic:
                        raise ImportError("anthropic not installed")

                    # Anthropic SDK ä¼šè‡ªåŠ¨æ·»åŠ  /v1 è·¯å¾„ï¼Œæ‰€ä»¥éœ€è¦å»æ‰ base_url ä¸­çš„ /v1
                    clean_base_url = self.custom_base_url.rstrip('/')
                    if clean_base_url.endswith('/v1'):
                        clean_base_url = clean_base_url[:-3]

                    self.client = Anthropic(
                        api_key=self.custom_api_key,
                        base_url=clean_base_url
                    )
                    logger.info(f"Custom Claude provider initialized with base_url: {clean_base_url} (original: {self.custom_base_url})")
                else:
                    # ä½¿ç”¨ OpenAI SDK å¤„ç† OpenAI å…¼å®¹çš„ API
                    if not OpenAI:
                        raise ImportError("openai not installed")
                    self.client = OpenAI(
                        api_key=self.custom_api_key,
                        base_url=self.custom_base_url,
                        timeout=120.0,
                        max_retries=2
                    )
                    logger.info(f"Custom provider initialized with base_url: {self.custom_base_url}")

            else:
                raise ValueError(f"Unsupported provider: {self.provider}")

        except Exception as e:
            logger.error(f"Failed to initialize {self.provider} client: {e}")
            raise

    def _build_analysis_prompt(self, analyze_bottlenecks: bool = True) -> str:
        """æ„å»ºé’ˆå¯¹æ¶æ„å›¾ä¼˜åŒ–çš„ Prompt"""
        base_prompt = """
You are an expert software architect. Analyze this architecture diagram and extract the following information:

1. **Nodes**: Identify all components (services, APIs, databases, queues, caches, gateways, etc.)
   For each node, provide:
   - id: unique identifier (use kebab-case, e.g., "api-gateway", "user-service")
   - type: one of ["api", "service", "database", "queue", "cache", "gateway", "default"]
   - label: component name (user-friendly)
   - position: {x, y} coordinates (distribute evenly in a grid, starting from x:100, y:100, spacing 200px)

2. **Edges**: Identify all connections between components
   For each edge, provide:
   - id: unique identifier (e.g., "e1", "e2")
   - source: source node id
   - target: target node id
   - label: connection description (optional, e.g., "HTTP", "gRPC", "data flow")

3. **Mermaid Code**: Generate valid Mermaid.js flowchart code in "graph TD" format
   Use these node formats:
   - API: nodeId["Label"]
   - Service: nodeId[["Label"]]
   - Database: nodeId[("Label")]
   - Default: nodeId["Label"]

Example Mermaid output:
```
graph TD
    api-gateway["API Gateway"]
    user-service[["User Service"]]
    database[("PostgreSQL")]
    api-gateway --> |HTTP| user-service
    user-service --> database
```
"""

        if analyze_bottlenecks:
            base_prompt += """

4. **Architecture Analysis** (if analyze_bottlenecks=true):
   Identify potential issues:
   - Single points of failure (nodes with many dependencies but no redundancy)
   - Performance bottlenecks (high-traffic nodes without load balancing)
   - Scalability issues (monolithic components, tight coupling)
   - Security concerns (direct database access, missing authentication layers)

   For each bottleneck, provide:
   - node_id: affected node
   - type: issue category
   - severity: "critical", "high", "medium", or "low"
   - description: what's wrong
   - recommendation: how to fix it
"""

        base_prompt += """

CRITICAL: Return ONLY a valid JSON object with this EXACT structure (no markdown, no extra text):
{
  "nodes": [
    {
      "id": "api-gateway",
      "type": "api",
      "position": {"x": 250, "y": 100},
      "data": {"label": "API Gateway"}
    }
  ],
  "edges": [
    {
      "id": "e1",
      "source": "api-gateway",
      "target": "user-service",
      "label": "HTTP"
    }
  ],
  "mermaid_code": "graph TD\\n    api-gateway[\\"API Gateway\\"]\\n    ...",
  "ai_analysis": {
    "bottlenecks": [
      {
        "node_id": "api-gateway",
        "type": "single_point_of_failure",
        "severity": "high",
        "description": "API Gateway has no redundancy",
        "recommendation": "Add load balancer and multiple instances"
      }
    ],
    "suggestions": [],
    "confidence": 0.85,
    "model_used": "gemini-2.0-flash-exp"
  }
}

Return ONLY the JSON. No markdown code blocks, no explanations.
"""

        return base_prompt

    def _build_flowchart_prompt_simple(self, preserve_layout: bool = True) -> str:
        """
        ç®€åŒ–ç‰ˆæµç¨‹å›¾è¯†åˆ« Prompt - ç”¨äºå›¾ç‰‡è¯†åˆ«å¿«é€Ÿæ¨¡å¼

        ç‰¹ç‚¹ï¼š
        - æ›´çŸ­çš„æç¤ºè¯ï¼ˆå‡å°‘ token æ¶ˆè€—ï¼‰
        - æ ¸å¿ƒåŠŸèƒ½ä¿ç•™ï¼ˆèŠ‚ç‚¹è¯†åˆ«ã€è¿çº¿æå–ï¼‰
        - å¤„ç†æ—¶é—´çº¦ 60-120 ç§’
        """

        if preserve_layout:
            layout_instruction = """
**ğŸ¯ STRICT PRIORITIES (Non-Negotiable Order):**

1. **PRIORITY 1: Zero Coordinate Collisions**
   - NO overlapping bounding boxes under ANY circumstances
   - Before outputting ANY coordinate, verify it doesn't collide with existing nodes
   - Mental collision check: For every node pair, verify separation â‰¥ minimum spacing

2. **PRIORITY 2: Minimum Spacing Enforcement**
   - **Horizontal spacing: ABSOLUTE MINIMUM 220px** (center to center)
   - **Vertical spacing: ABSOLUTE MINIMUM 160px** (center to center)
   - These are HARD CONSTRAINTS - violating them = automatic collision

3. **PRIORITY 3: Preserve Image Layout Structure**
   - Maintain same flow direction (horizontal/vertical/mixed)
   - Keep relative positions (top/bottom/left/right relationships)
   - Preserve visual hierarchy

**ğŸ“ Node Dimensions (Critical for Collision Calculation):**

**BPMN Shapes:**
- start-event/end-event: **56px Ã— 56px** (small circles)
- task: **140px Ã— 80px** (rounded rectangles)
- diamond (decision): **100px Ã— 100px**

**Common Shapes:**
- rectangle: **180px Ã— 90px** (default flowchart box)
- circle: **80px Ã— 80px** (general purpose)
- hexagon: **120px Ã— 100px** (preparation)
- parallelogram: **140px Ã— 80px** (data I/O)
- cloud: **140px Ã— 80px** (cloud/external)
- cylinder: **100px Ã— 120px** (database)

**ğŸ§® Spacing Calculation Rules:**

**IMPORTANT: Use SMART spacing, not rigid formulas**

The goal is: **Preserve original image layout while preventing overlaps**

1. **Measure the original image spacing first:**
   - If original dx between nodes is 100-150px â†’ keep similar (e.g., 120-160px)
   - If original dx is 200-300px â†’ keep similar (e.g., 220-320px)
   - Only expand if original spacing would cause collision

2. **Minimum Safe Spacing (collision prevention):**
   - For small nodes (start-event, circle): minimum 100px center-to-center
   - For medium nodes (task, parallelogram): minimum 180px center-to-center
   - For large nodes (rectangle): minimum 220px center-to-center
   - Vertical rows: minimum 120px center-to-center

3. **Spacing Formula (when in doubt):**
```
safe_spacing = max(original_spacing, node_width + 40px)

Example 1 (tight layout in image):
- Original: 2 rectangles 120px apart
- Calculation: max(120, 180+40) = 220px âœ… expand to prevent collision

Example 2 (loose layout in image):
- Original: 2 rectangles 350px apart
- Calculation: max(350, 180+40) = 350px âœ… keep original spacing

Example 3 (small nodes):
- Original: 2 start-events 80px apart
- Calculation: max(80, 56+40) = 96px âœ… slightly expand
```

4. **Smart Layout Strategy:**
   - **DON'T force uniform spacing** - original images have natural variation
   - **DO preserve relative density** - if left side is dense, keep it dense (but safe)
   - **DO preserve flow patterns** - horizontal flows, vertical flows, tree structures

**âœ… PRE-OUTPUT VALIDATION (MANDATORY):**

Run this mental checklist BEFORE generating JSON:

1. âœ“ List all node pairs that could potentially collide
2. âœ“ For each pair, calculate: |center1_x - center2_x| and |center1_y - center2_y|
3. âœ“ Verify: horizontal_distance â‰¥ 220px OR vertical_distance â‰¥ 160px
4. âœ“ If ANY pair fails: STOP and adjust coordinates
5. âœ“ Re-check after adjustment before proceeding

**Example Collision Check:**
```
Node A: x=100, y=100, size=180Ã—90 â†’ center=(190, 145), bounds=[100,280]Ã—[100,190]
Node B: x=250, y=100, size=140Ã—80 â†’ center=(320, 140), bounds=[250,390]Ã—[100,180]

Check: |320-190| = 130px
Node A width/2 + Node B width/2 + MIN_GAP = 90 + 70 + 40 = 200px required
130px < 200px â†’ âŒ COLLISION

Adjusted Node B: x=330, y=100 â†’ center=(400, 140)
Check: |400-190| = 210px â‰¥ 200px âœ… SAFE
```

**ğŸ¨ Layout Strategy (UPDATED - Smart Preservation):**

**Step 1: Analyze Original Image**
- Identify layout type: horizontal flow / vertical flow / tree / grid
- Count rows and columns
- Measure average spacing in original image
- Note: dense areas vs sparse areas

**Step 2: Preserve Structure with Smart Spacing**
```
For horizontal flow (e.g., A â†’ B â†’ C â†’ D):
- Preserve left-to-right order
- Use consistent spacing: original_avg + safety_margin
- Example: if original avg 150px, use 180-200px

For vertical flow (e.g., top-to-bottom):
- Preserve top-to-bottom order
- Vertical spacing: 120-160px (enough for 80-90px tall nodes + gap)

For tree structure (branching):
- Preserve parent-child relationships
- Horizontal spacing between siblings: 180-220px
- Vertical spacing parent-to-child: 140-180px

For grid layout:
- Preserve row/column structure
- Row spacing: 140-180px
- Column spacing: 200-250px
```

**Step 3: Coordinate Assignment**
```python
# Pseudo-algorithm
for each row in layout:
    y = base_y + row_index * vertical_spacing
    x = start_x
    for each node in row:
        node.position = (x, y)
        x += node.width + horizontal_gap  # adaptive gap based on density
```

**âš ï¸ Critical Balance:**
- **Priority 1:** Zero collisions (NON-NEGOTIABLE)
- **Priority 2:** Preserve flow structure (directional relationships)
- **Priority 3:** Match original spacing (when collision-free)

If original image is too dense:
- Expand ALL spacing proportionally (don't just push overlapping nodes)
- Keep relative positions (node A still left of node B)
"""
        else:
            layout_instruction = """
**Auto Layout Mode:**
- Use standard vertical layout (top to bottom)
- Nodes at x=100, y values: 100, 280, 460, 640...
- 180px vertical spacing between nodes
"""

        prompt = f"""Analyze this flowchart image and extract nodes and edges.

{layout_instruction}

**ğŸ”— EDGES (Connections/Arrows) - CRITICAL:**

**You MUST extract ALL visible connections between nodes:**

1. **Arrow Detection Rules:**
   - Look for: solid lines, dashed lines, arrows, connectors
   - Follow each line from start node to end node
   - Even simple straight lines between nodes = edges
   - Curved arrows, orthogonal lines (right-angle) = all are edges

2. **Edge Types:**
   - Solid arrow (â†’): Normal flow, no label usually
   - Dashed arrow (â‡¢): Alternative path, optional
   - Line with label: Extract the label text (e.g., "Yes", "No", "Success", "Error")
   - Bidirectional (â†”): Create two edges (Aâ†’B and Bâ†’A)

3. **IMPORTANT - Don't Miss Edges:**
   - **Count carefully:** If you see 5 arrows in image, output 5 edges
   - **Branch points:** Diamond nodes typically have 2+ outgoing edges
   - **Loops:** Node connecting back to itself or previous node = valid edge
   - **Parallel flows:** Multiple nodes at same level may connect to same target

4. **Edge Label Extraction:**
   - Look for text ON or NEAR the arrow
   - Common labels: "æ˜¯"/"å¦", "Yes"/"No", "Success"/"Fail", "é€šè¿‡"/"æ‹’ç»"
   - If no label visible: use empty string ""

5. **Edge ID Convention:**
   - Format: "edge_{number}" or "edge_{source}_{target}"
   - Example: "edge_1", "edge_2" or "edge_node1_node2"

**Example Edge Recognition:**
```
Image shows:
[Start] â†’ [Process A] â†’ [Decision B] â‡’ [End]
                              â†“
                          [Process C] â†’ [End]

Must output:
edges: [
  {"id": "edge_1", "source": "start", "target": "process_a", "label": ""},
  {"id": "edge_2", "source": "process_a", "target": "decision_b", "label": ""},
  {"id": "edge_3", "source": "decision_b", "target": "end", "label": "Yes"},
  {"id": "edge_4", "source": "decision_b", "target": "process_c", "label": "No"},
  {"id": "edge_5", "source": "process_c", "target": "end", "label": ""}
]
```

**Node Types** (by shape):
- Circle â†’ start-event (if text contains "Start"/"å¼€å§‹") or end-event (if "End"/"ç»“æŸ")
- Rectangle â†’ task
- Diamond â†’ decision
- Other shapes â†’ default with shape attribute

**Output JSON** (no markdown wrapper):

{{
  "nodes": [
    {{"id": "node_1", "type": "start-event", "position": {{"x": 100, "y": 50}}, "data": {{"label": "Start", "shape": "circle"}}}},
    {{"id": "node_2", "type": "task", "position": {{"x": 100, "y": 230}}, "data": {{"label": "Task A", "shape": "rectangle"}}}}
  ],
  "edges": [
    {{"id": "edge_1", "source": "node_1", "target": "node_2", "label": ""}}
  ],
  "mermaid_code": "graph TD\\n    node_1((Start))\\n    node_2[Task A]\\n    node_1 --> node_2",
  "warnings": [],
  "analysis": {{"total_nodes": 2, "total_branches": 0}}
}}

**ğŸ” FINAL VALIDATION CHECKLIST (MUST COMPLETE BEFORE OUTPUT):**

**Node Validation:**
Step 1: Count total nodes â†’ N nodes = N*(N-1)/2 pairs to check
Step 2: For each pair (i, j):
   - Calculate horizontal distance: dx = |center_i.x - center_j.x|
   - Calculate vertical distance: dy = |center_i.y - center_j.y|
   - Verify bounding boxes don't overlap (consider node dimensions)
   - If collision: adjust coordinates and re-validate

**Edge Validation:**
Step 3: Count visible arrows/connections in image â†’ Must match edges array length
   - If image has 5 arrows but you only have 3 edges: âŒ MISSING EDGES
   - Review image carefully, extract ALL connections

Step 4: Verify edge connectivity:
   - Each edge.source must match a valid node.id
   - Each edge.target must match a valid node.id
   - No dangling edges (source/target doesn't exist)

Step 5: Check for logical flow:
   - Start nodes should have outgoing edges
   - End nodes should have incoming edges
   - Decision nodes should have 2+ outgoing edges (typically)

**Final Checks:**
Step 6: Zero JSON syntax errors (valid JSON structure)
Step 7: Zero coordinate collisions (validated above)
Step 8: All arrows in image are represented as edges
Step 9: If any validation fails: FIX before output, then re-validate

**Requirements:**
- Zero JSON syntax errors (valid JSON structure)
- Zero coordinate collisions (validated above)
- ALL visible connections extracted as edges
- All nodes must have position (x, y) with safe spacing
- Extract text labels from nodes and edges
- Maintain flow structure while ensuring spacing
- Return pure JSON only (no markdown wrapper)
"""

        return prompt

    def _build_flowchart_prompt_detailed(self, preserve_layout: bool = True) -> str:
        """
        è¯¦ç»†ç‰ˆæµç¨‹å›¾è¯†åˆ« Prompt - ç”¨äºæ–‡æœ¬ç”Ÿæˆé«˜è´¨é‡æ¨¡å¼

        ç‰¹ç‚¹ï¼š
        - å®Œæ•´çš„å½¢çŠ¶è¯†åˆ«è§„åˆ™
        - è¯¦ç»†çš„è¾“å‡ºç¤ºä¾‹
        - æ›´é«˜çš„è¯†åˆ«å‡†ç¡®åº¦
        - å¤„ç†æ—¶é—´è¾ƒé•¿ï¼ˆ200+ ç§’ï¼‰
        """

        # æ”¯æŒçš„å½¢çŠ¶ç±»å‹ï¼ˆä» SvgShapes.tsx æ˜ å°„ï¼‰
        supported_shapes = {
            "circle": "åœ†å½¢ â†’ start-eventï¼ˆå¼€å§‹ï¼‰æˆ– end-eventï¼ˆç»“æŸï¼‰",
            "rectangle": "çŸ©å½¢ â†’ taskï¼ˆå¤„ç†ä»»åŠ¡ï¼‰",
            "diamond": "è±å½¢ â†’ decisionï¼ˆåˆ¤æ–­/å†³ç­–ï¼‰",
            "parallelogram": "å¹³è¡Œå››è¾¹å½¢ â†’ default with shape='parallelogram'ï¼ˆæ•°æ®/è¾“å…¥è¾“å‡ºï¼‰",
            "hexagon": "å…­è¾¹å½¢ â†’ default with shape='hexagon'ï¼ˆå‡†å¤‡ï¼‰",
            "trapezoid": "æ¢¯å½¢ â†’ default with shape='trapezoid'ï¼ˆæ‰‹åŠ¨æ“ä½œï¼‰",
            "cylinder": "åœ†æŸ±ä½“ â†’ databaseï¼ˆæ•°æ®åº“ï¼‰",
            "document": "æ–‡æ¡£å½¢ â†’ default with shape='document'",
            "cloud": "äº‘å½¢ â†’ default with shape='cloud'",
        }

        shapes_desc = "\\n".join([f"   - {shape}: {desc}" for shape, desc in supported_shapes.items()])

        layout_instruction = ""
        if preserve_layout:
            layout_instruction = """
4. **å¸ƒå±€ä¿ç•™**ï¼š
   - è®°å½•æ¯ä¸ªèŠ‚ç‚¹çš„ç›¸å¯¹ä½ç½®ï¼ˆå·¦ä¸Šè§’åæ ‡ï¼‰
   - å‚è€ƒåŸå›¾çš„ç©ºé—´å¸ƒå±€å…³ç³»
   - xåæ ‡ä»100å¼€å§‹ï¼Œyåæ ‡ä»50å¼€å§‹
   - èŠ‚ç‚¹é—´è·å»ºè®®ä¿æŒ150-200px
"""
        else:
            layout_instruction = """
4. **è‡ªåŠ¨å¸ƒå±€**ï¼š
   - å¿½ç•¥åŸå›¾ä½ç½®ï¼Œä½¿ç”¨æ ‡å‡†å¸ƒå±€
   - ä»ä¸Šåˆ°ä¸‹ã€ä»å·¦åˆ°å³æ’åˆ—
   - xåæ ‡ä»100å¼€å§‹ï¼Œyåæ ‡ä»50å¼€å§‹ï¼Œæ¯è¡Œé—´è·200px
"""

        prompt = f"""
ä½ æ˜¯ä¸“ä¸šçš„æµç¨‹å›¾åˆ†æä¸“å®¶ã€‚è¯·åˆ†æè¿™å¼ **æµç¨‹å›¾æˆªå›¾**ï¼Œæå–èŠ‚ç‚¹å’Œè¿çº¿å…³ç³»ã€‚

## è¯†åˆ«è§„åˆ™

1. **èŠ‚ç‚¹ç±»å‹è¯†åˆ«**ï¼ˆæ ¹æ®å½¢çŠ¶åˆ¤æ–­ï¼‰ï¼š
{shapes_desc}

   **é‡è¦ï¼š** åœ†å½¢èŠ‚ç‚¹éœ€åˆ¤æ–­æ˜¯å¼€å§‹è¿˜æ˜¯ç»“æŸï¼š
   - æ–‡æœ¬åŒ…å«"å¼€å§‹"/"Start"/"å¯åŠ¨" â†’ type="start-event"
   - æ–‡æœ¬åŒ…å«"ç»“æŸ"/"End"/"å®Œæˆ" â†’ type="end-event"
   - æ— æ³•åˆ¤æ–­æ—¶é»˜è®¤ â†’ type="start-event"

2. **æ–‡æœ¬è¯†åˆ«**ï¼š
   - æå–æ¯ä¸ªèŠ‚ç‚¹å†…çš„æ–‡æœ¬ä½œä¸º label
   - è¯†åˆ«è¿çº¿ä¸Šçš„æ–‡æœ¬ä½œä¸º edge.label
   - å¦‚æœèŠ‚ç‚¹æ— æ–‡æœ¬ï¼Œlabel è®¾ä¸º "æœªå‘½åèŠ‚ç‚¹"

3. **è¿çº¿è¯†åˆ«**ï¼š
   - è¯†åˆ«ç®­å¤´æ–¹å‘ï¼ˆèµ·ç‚¹èŠ‚ç‚¹ â†’ ç»ˆç‚¹èŠ‚ç‚¹ï¼‰
   - åˆ¤æ–­è¿çº¿ç±»å‹ï¼š
     * è™šçº¿ â†’ animated: falseï¼ˆä¿æŒé»˜è®¤ï¼‰
     * å®çº¿ â†’ animated: false
   - æå–åˆ¤æ–­åˆ†æ”¯æ ‡ç­¾ï¼ˆæ˜¯/å¦ã€Yes/Noã€True/Falseã€Y/Nï¼‰

{layout_instruction}

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰

è¿”å› **çº¯ JSON å¯¹è±¡**ï¼Œä¸è¦ç”¨ markdown ä»£ç å—åŒ…è£¹ï¼š

{{
  "nodes": [
    {{
      "id": "node_1",
      "type": "start-event",
      "position": {{"x": 100, "y": 50}},
      "data": {{
        "label": "å¼€å§‹",
        "shape": "circle"
      }}
    }},
    {{
      "id": "node_2",
      "type": "task",
      "position": {{"x": 100, "y": 200}},
      "data": {{
        "label": "æ‰§è¡Œä»»åŠ¡A",
        "shape": "rectangle"
      }}
    }},
    {{
      "id": "node_3",
      "type": "decision",
      "position": {{"x": 100, "y": 350}},
      "data": {{
        "label": "æ¡ä»¶åˆ¤æ–­",
        "shape": "diamond"
      }}
    }},
    {{
      "id": "node_4",
      "type": "end-event",
      "position": {{"x": 300, "y": 500}},
      "data": {{
        "label": "ç»“æŸ",
        "shape": "circle"
      }}
    }}
  ],
  "edges": [
    {{
      "id": "edge_1",
      "source": "node_1",
      "target": "node_2",
      "label": ""
    }},
    {{
      "id": "edge_2",
      "source": "node_2",
      "target": "node_3",
      "label": ""
    }},
    {{
      "id": "edge_3",
      "source": "node_3",
      "target": "node_4",
      "label": "æ˜¯"
    }},
    {{
      "id": "edge_4",
      "source": "node_3",
      "target": "node_5",
      "label": "å¦"
    }}
  ],
  "mermaid_code": "graph TD\\n    node_1((å¼€å§‹))\\n    node_2[æ‰§è¡Œä»»åŠ¡A]\\n    node_3{{æ¡ä»¶åˆ¤æ–­}}\\n    node_4((ç»“æŸ))\\n    node_1 --> node_2\\n    node_2 --> node_3\\n    node_3 -->|æ˜¯| node_4\\n    node_3 -->|å¦| node_5",
  "warnings": [
    {{"node_id": "node_X", "message": "åŸå›¾ä¸ºå…«è¾¹å½¢ï¼Œå·²æ˜ å°„ä¸ºå…­è¾¹å½¢ï¼ˆhexagonï¼‰"}}
  ],
  "analysis": {{
    "total_nodes": 4,
    "total_branches": 1,
    "flowchart_type": "ä¸šåŠ¡æµç¨‹å›¾",
    "complexity": "ç®€å•",
    "description": "è¿™æ˜¯ä¸€ä¸ªåŒ…å«4ä¸ªèŠ‚ç‚¹çš„ç®€å•æµç¨‹å›¾ï¼Œä»å¼€å§‹åˆ°ç»“æŸç»è¿‡ä¸€ä¸ªåˆ¤æ–­èŠ‚ç‚¹"
  }}
}}

## æ³¨æ„äº‹é¡¹

- **æ‰€æœ‰èŠ‚ç‚¹å¿…é¡»æœ‰ position**ï¼ˆå³ä½¿æ˜¯è‡ªåŠ¨å¸ƒå±€ä¹Ÿè¦ç»™åæ ‡ï¼‰
- **edges çš„ source å’Œ target å¿…é¡»å¯¹åº” nodes çš„ id**
- **èŠ‚ç‚¹ id å¿…é¡»å”¯ä¸€**ï¼ˆå»ºè®®ä½¿ç”¨ node_1, node_2, node_3...ï¼‰
- **ä¸è¦è¾“å‡º markdown ä»£ç å—**ï¼ˆå¦‚ ```json ... ```ï¼‰ï¼Œç›´æ¥è¾“å‡º JSON
- **å¦‚æœå½¢çŠ¶ä¸åœ¨æ”¯æŒåˆ—è¡¨ï¼Œé€‰æ‹©æœ€æ¥è¿‘çš„å½¢çŠ¶ï¼Œå¹¶åœ¨ warnings ä¸­è¯´æ˜**
- **Mermaid ä»£ç æ ¼å¼ï¼š**
  - å¼€å§‹/ç»“æŸèŠ‚ç‚¹ï¼š`nodeId((label))`
  - å¤„ç†èŠ‚ç‚¹ï¼š`nodeId[label]`
  - åˆ¤æ–­èŠ‚ç‚¹ï¼š`nodeId{{label}}`
  - è¿çº¿ï¼š`node1 --> node2` æˆ– `node1 -->|label| node2`

ç°åœ¨å¼€å§‹åˆ†æå›¾ç‰‡ï¼Œè¿”å› JSONï¼š
"""

        return prompt

    def _build_flowchart_prompt(self, preserve_layout: bool = True, fast_mode: bool = True) -> str:
        """
        æ„å»ºæµç¨‹å›¾è¯†åˆ«ä¸“ç”¨ Prompt

        å‚æ•°ï¼š
        - preserve_layout: æ˜¯å¦ä¿ç•™åŸå›¾å¸ƒå±€
        - fast_mode: æ˜¯å¦ä½¿ç”¨å¿«é€Ÿæ¨¡å¼ï¼ˆç®€åŒ– promptï¼‰
          - True: ç”¨äºå›¾ç‰‡è¯†åˆ«ï¼ˆ60-120ç§’ï¼‰
          - False: ç”¨äºæ–‡æœ¬ç”Ÿæˆï¼ˆ200+ç§’ï¼Œé«˜è´¨é‡ï¼‰
        """
        if fast_mode:
            return self._build_flowchart_prompt_simple(preserve_layout)
        else:
            return self._build_flowchart_prompt_detailed(preserve_layout)

    async def analyze_architecture(
        self,
        image_data: bytes,
        analyze_bottlenecks: bool = True
    ) -> ImageAnalysisResponse:
        """åˆ†ææ¶æ„å›¾ç‰‡"""
        prompt = self._build_analysis_prompt(analyze_bottlenecks)

        try:
            if self.provider == "gemini":
                result = await self._analyze_with_gemini(image_data, prompt)
            elif self.provider == "openai":
                result = await self._analyze_with_openai(image_data, prompt)
            elif self.provider == "claude":
                result = await self._analyze_with_claude(image_data, prompt)
            elif self.provider == "siliconflow":
                result = await self._analyze_with_siliconflow(image_data, prompt)
            elif self.provider == "custom":
                result = await self._analyze_with_custom(image_data, prompt)
            else:
                raise ValueError(f"Unsupported provider: {self.provider}")

            return result

        except Exception as e:
            logger.error(f"Analysis failed with {self.provider}: {e}")
            raise

    async def analyze_flowchart(
        self,
        image_data: bytes,
        preserve_layout: bool = True,
        fast_mode: bool = True
    ) -> ImageAnalysisResponse:
        """
        åˆ†ææµç¨‹å›¾æˆªå›¾

        Args:
            image_data: å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®
            preserve_layout: æ˜¯å¦ä¿ç•™åŸå›¾å¸ƒå±€
            fast_mode: æ˜¯å¦ä½¿ç”¨å¿«é€Ÿæ¨¡å¼
                - True: ç®€åŒ– promptï¼Œ60-120ç§’å®Œæˆï¼ˆå›¾ç‰‡è¯†åˆ«æ¨èï¼‰
                - False: è¯¦ç»† promptï¼Œ200+ç§’ï¼Œé«˜è´¨é‡ï¼ˆæ–‡æœ¬ç”Ÿæˆæ¨èï¼‰

        Returns:
            ImageAnalysisResponse: åŒ…å« nodes, edges, mermaid_code, warnings
        """
        prompt = self._build_flowchart_prompt(preserve_layout, fast_mode)

        # æ ¹æ® fast_mode è®¾ç½® max_tokensï¼ˆè¶³å¤Ÿå¤§ä»¥é¿å…JSONæˆªæ–­ï¼‰
        max_tokens = 4096 if fast_mode else 8192

        # è®¾ç½®è¶…æ—¶æ—¶é—´
        self._flowchart_timeout = 240.0 if fast_mode else 300.0

        # å…³é”®ä¼˜åŒ–ï¼šè®¾ç½® detail å‚æ•°ï¼ˆSiliconFlow ä¸“ç”¨ï¼‰
        self._image_detail = "low" if fast_mode else "high"

        try:
            logger.info(f"[FLOWCHART] Starting analysis with {self.provider}, preserve_layout={preserve_layout}, fast_mode={fast_mode}, max_tokens={max_tokens}, timeout={self._flowchart_timeout}s, detail={self._image_detail}")

            if self.provider == "gemini":
                result = await self._analyze_with_gemini(image_data, prompt, max_tokens)
            elif self.provider == "openai":
                result = await self._analyze_with_openai(image_data, prompt, max_tokens)
            elif self.provider == "claude":
                result = await self._analyze_with_claude(image_data, prompt, max_tokens)
            elif self.provider == "siliconflow":
                result = await self._analyze_with_siliconflow(image_data, prompt, max_tokens)
            elif self.provider == "custom":
                result = await self._analyze_with_custom(image_data, prompt, max_tokens)
            else:
                raise ValueError(f"Unsupported provider: {self.provider}")

            logger.info(f"[FLOWCHART] Analysis completed: {len(result.nodes)} nodes, {len(result.edges)} edges")

            # warnings å’Œ analysis å·²ç»åœ¨ JSON ä¸­ï¼Œ_build_response ä¼šå¤„ç†

            return result

        except Exception as e:
            logger.error(f"Flowchart analysis failed with {self.provider}: {e}", exc_info=True)
            raise

    async def _analyze_with_gemini(self, image_data: bytes, prompt: str, max_tokens: int = 4096) -> ImageAnalysisResponse:
        """ä½¿ç”¨ Gemini åˆ†æ"""
        try:
            logger.info(f"[GEMINI] Starting analysis, image size: {len(image_data)} bytes, max_tokens: {max_tokens}")

            # Gemini æ”¯æŒç›´æ¥ä¼  bytes
            image_parts = [
                {
                    "mime_type": "image/jpeg",
                    "data": image_data
                }
            ]

            logger.info("[GEMINI] Calling Gemini API...")
            response = await self.client.generate_content_async(
                [prompt, image_parts[0]],
                generation_config={
                    "temperature": 0.2,
                    "max_output_tokens": max_tokens
                }
            )

            logger.info(f"[GEMINI] Response received, type: {type(response)}")
            logger.info(f"[GEMINI] Response text length: {len(response.text) if hasattr(response, 'text') else 'N/A'}")

            # æå– JSON
            result_json = self._extract_json_from_response(response.text)
            logger.info(f"[GEMINI] JSON extracted successfully")

            # éªŒè¯å¹¶æ„å»ºå“åº”
            result = self._build_response(result_json)
            logger.info(f"[GEMINI] Analysis completed: {len(result.nodes)} nodes, {len(result.edges)} edges")
            return result

        except Exception as e:
            logger.error(f"Gemini analysis failed: {e}", exc_info=True)
            raise

    async def _analyze_with_openai(self, image_data: bytes, prompt: str, max_tokens: int = 4096) -> ImageAnalysisResponse:
        """ä½¿ç”¨ OpenAI GPT-4 Vision åˆ†æ"""
        try:
            logger.info(f"[OPENAI] Starting vision analysis, max_tokens: {max_tokens}")
            image_b64 = base64.b64encode(image_data).decode("utf-8")

            # ä½¿ç”¨ asyncio.to_thread åŒ…è£…åŒæ­¥è°ƒç”¨
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                model="gpt-4-vision-preview",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_b64}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=max_tokens,
                temperature=0.2
            )

            logger.info("[OPENAI] Response received, extracting JSON")
            result_json = self._extract_json_from_response(
                response.choices[0].message.content
            )

            return self._build_response(result_json)

        except Exception as e:
            logger.error(f"OpenAI analysis failed: {e}", exc_info=True)
            raise

    async def _analyze_with_claude(self, image_data: bytes, prompt: str, max_tokens: int = 4096) -> ImageAnalysisResponse:
        """ä½¿ç”¨ Claude 3.5 Sonnet åˆ†æ"""
        try:
            logger.info(f"[CLAUDE] Starting vision analysis, max_tokens: {max_tokens}")
            image_b64 = base64.b64encode(image_data).decode("utf-8")

            # ä½¿ç”¨ asyncio.to_thread åŒ…è£…åŒæ­¥è°ƒç”¨
            response = await asyncio.to_thread(
                self.client.messages.create,
                model="claude-3-5-sonnet-20241022",
                max_tokens=max_tokens,
                temperature=0.2,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": "image/jpeg",
                                    "data": image_b64
                                }
                            }
                        ]
                    }
                ]
            )

            logger.info("[CLAUDE] Response received, extracting JSON")
            result_json = self._extract_json_from_response(
                response.content[0].text
            )

            return self._build_response(result_json)

        except Exception as e:
            logger.error(f"Claude analysis failed: {e}", exc_info=True)
            raise

    async def _analyze_with_siliconflow(self, image_data: bytes, prompt: str, max_tokens: int = 4096) -> ImageAnalysisResponse:
        """ä½¿ç”¨ SiliconFlow è§†è§‰æ¨¡å‹åˆ†æï¼ˆå¦‚ Qwen3-VLï¼‰"""
        try:
            # è·å–è¶…æ—¶é…ç½®ï¼ˆå¦‚æœæ˜¯ flowchart è°ƒç”¨ä¼šè®¾ç½®æ­¤å±æ€§ï¼‰
            timeout = getattr(self, '_flowchart_timeout', 180.0)
            # è·å– detail å‚æ•°ï¼ˆlow/highï¼Œç”¨äºæ§åˆ¶å›¾ç‰‡åˆ†è¾¨ç‡ï¼‰
            detail = getattr(self, '_image_detail', 'high')

            logger.info(f"[SILICONFLOW] Starting vision analysis with model: {self.model_name}, max_tokens: {max_tokens}, timeout: {timeout}s, detail: {detail}")
            image_b64 = base64.b64encode(image_data).decode("utf-8")

            # ä½¿ç”¨ asyncio.to_thread åŒ…è£…åŒæ­¥è°ƒç”¨ï¼Œå¹¶å¢åŠ è¶…æ—¶æ—¶é—´
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    self.client.chat.completions.create,
                    model=self.model_name,  # ä¾‹å¦‚: Qwen/Qwen3-VL-32B-Thinking
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": prompt},
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/jpeg;base64,{image_b64}",
                                        "detail": detail  # å…³é”®å‚æ•°ï¼šlow=å¿«é€Ÿï¼Œhigh=é«˜è´¨é‡
                                    }
                                }
                            ]
                        }
                    ],
                    max_tokens=max_tokens,
                    temperature=0.2
                ),
                timeout=timeout
            )

            logger.info("[SILICONFLOW] Response received, extracting JSON")
            result_json = self._extract_json_from_response(
                response.choices[0].message.content
            )

            logger.info(f"[SILICONFLOW] Analysis completed successfully")
            return self._build_response(result_json)

        except asyncio.TimeoutError:
            logger.error(f"SiliconFlow vision analysis timeout after {timeout}s")
            raise ValueError(f"AI vision analysis timeout ({timeout/60:.1f} minutes). Please try again or use a different provider.")
        except Exception as e:
            logger.error(f"SiliconFlow vision analysis failed: {e}", exc_info=True)
            raise

    async def _analyze_with_custom(self, image_data: bytes, prompt: str, max_tokens: int = 4096) -> ImageAnalysisResponse:
        """ä½¿ç”¨è‡ªå®šä¹‰ provider åˆ†æï¼ˆæ”¯æŒ OpenAI å’Œ Claude æ ¼å¼ï¼‰"""
        try:
            logger.info(f"[CUSTOM] Starting vision analysis, max_tokens: {max_tokens}")
            image_b64 = base64.b64encode(image_data).decode("utf-8")

            # ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º gpt-4-vision-preview
            model = self.custom_model_name or "gpt-4-vision-preview"

            logger.info(f"[CUSTOM] Using model: {model}, base_url: {self.custom_base_url}")

            # æ£€æµ‹æ˜¯å¦æ˜¯ Claude æ¨¡å‹ï¼ˆæ£€æŸ¥æ¨¡å‹åæ˜¯å¦åŒ…å« claudeï¼‰
            is_claude_model = "claude" in model.lower()

            if is_claude_model:
                # Claude API ä½¿ç”¨ Anthropic æ ¼å¼
                logger.info("[CUSTOM] Detected Claude model, using Anthropic image format")

                # æ£€æµ‹å›¾ç‰‡ç±»å‹
                import imghdr
                media_type = imghdr.what(None, h=image_data)
                if media_type == "jpeg":
                    media_type = "image/jpeg"
                elif media_type == "png":
                    media_type = "image/png"
                elif media_type == "webp":
                    media_type = "image/webp"
                elif media_type == "gif":
                    media_type = "image/gif"
                else:
                    media_type = "image/jpeg"  # é»˜è®¤ JPEG

                logger.info(f"[CUSTOM] Image media type: {media_type}")

                # æ£€æµ‹æ˜¯å¦æ˜¯ ikuncode.cc
                use_ikuncode_raw_http = (
                    self.custom_base_url and
                    "ikuncode.cc" in self.custom_base_url.lower()
                )

                if use_ikuncode_raw_http:
                    # ikuncode.cc: ä½¿ç”¨ raw HTTP é¿å… User-Agent é˜»æ‹¦
                    logger.info(f"[CUSTOM] Using raw HTTP for ikuncode.cc: {self.custom_base_url}")
                    import httpx

                    # æ¸…ç† base_url
                    clean_base_url = self.custom_base_url.rstrip('/')
                    if clean_base_url.endswith('/v1'):
                        clean_base_url = clean_base_url[:-3]

                    headers = {
                        "x-api-key": self.custom_api_key,
                        "anthropic-version": "2023-06-01",
                        "content-type": "application/json"
                    }

                    data = {
                        "model": model,
                        "max_tokens": max_tokens,
                        "temperature": 0.2,
                        "messages": [
                            {
                                "role": "user",
                                "content": [
                                    {
                                        "type": "image",
                                        "source": {
                                            "type": "base64",
                                            "media_type": media_type,
                                            "data": image_b64
                                        }
                                    },
                                    {"type": "text", "text": prompt}
                                ]
                            }
                        ]
                    }

                    logger.info(f"[CUSTOM] Sending raw HTTP request to: {clean_base_url}/v1/messages")

                    async with httpx.AsyncClient(timeout=300.0) as http_client:
                        http_response = await http_client.post(
                            f"{clean_base_url}/v1/messages",
                            headers=headers,
                            json=data
                        )

                        if http_response.status_code != 200:
                            error_text = http_response.text
                            logger.error(f"[CUSTOM] Claude API error: {http_response.status_code} - {error_text}")
                            raise ValueError(f"Claude API request failed: {http_response.status_code} - {error_text}")

                        response_json = http_response.json()
                        logger.info(f"[CUSTOM] Raw HTTP response received")

                        # æ„é€ ä¸€ä¸ªå…¼å®¹çš„å“åº”å¯¹è±¡
                        class RawHTTPResponse:
                            def __init__(self, json_data):
                                self.content = json_data.get('content', [])
                                self._json = json_data

                        response = RawHTTPResponse(response_json)
                else:
                    # linkflow.run ç­‰: ä½¿ç”¨ Anthropic SDK
                    logger.info("[CUSTOM] Using Anthropic SDK")
                    response = await asyncio.to_thread(
                        self.client.messages.create,
                        model=model,
                        messages=[
                            {
                                "role": "user",
                                "content": [
                                    {
                                        "type": "image",
                                        "source": {
                                            "type": "base64",
                                            "media_type": media_type,
                                            "data": image_b64
                                        }
                                    },
                                    {"type": "text", "text": prompt}
                                ]
                            }
                        ],
                        max_tokens=max_tokens,
                        temperature=0.2
                    )
            else:
                # OpenAI æ ¼å¼ï¼ˆé»˜è®¤ï¼‰
                logger.info("[CUSTOM] Using OpenAI image_url format")
                response = await asyncio.to_thread(
                    self.client.chat.completions.create,
                    model=model,
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": prompt},
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/jpeg;base64,{image_b64}"
                                    }
                                }
                            ]
                        }
                    ],
                    max_tokens=max_tokens,
                    temperature=0.2
                )

            logger.info(f"[CUSTOM] Response type: {type(response)}")

            # æ‰“å°å®Œæ•´çš„å“åº”å†…å®¹
            try:
                if hasattr(response, 'model_dump'):
                    import json
                    response_dict = response.model_dump()
                    logger.info(f"[CUSTOM] Full response body:\n{json.dumps(response_dict, indent=2, ensure_ascii=False)}")
                else:
                    logger.info(f"[CUSTOM] Full response (str): {str(response)[:2000]}")
            except Exception as e:
                logger.error(f"[CUSTOM] Failed to dump response: {e}")

            logger.info(f"[CUSTOM] Response has 'output': {hasattr(response, 'output')}")
            logger.info(f"[CUSTOM] Response has 'choices': {hasattr(response, 'choices')}")

            if hasattr(response, 'output'):
                logger.info(f"[CUSTOM] response.output value: {response.output}")
            if hasattr(response, 'choices'):
                logger.info(f"[CUSTOM] response.choices value: {response.choices}")

            # å¤„ç†ä¸åŒçš„å“åº”æ ¼å¼
            content = None

            # Anthropic SDK æ ‡å‡†æ ¼å¼ï¼šresponse.content[0].text
            if is_claude_model and hasattr(response, 'content') and response.content:
                logger.info("[CUSTOM] Using Anthropic 'content' format")
                for content_block in response.content:
                    if hasattr(content_block, 'text'):
                        content = content_block.text
                        logger.info(f"[CUSTOM] Extracted from Anthropic format, length: {len(content)}")
                        break
                    elif isinstance(content_block, dict) and 'text' in content_block:
                        content = content_block['text']
                        logger.info(f"[CUSTOM] Extracted from Anthropic dict format, length: {len(content)}")
                        break

            # æŸäº›ä¸­è½¬ç«™çš„æ ¼å¼ï¼šresponse.output[0].content[0].text
            elif hasattr(response, 'output') and response.output:
                logger.info("[CUSTOM] Using 'output' format")
                output_item = response.output[0]
                logger.info(f"[CUSTOM] output_item type: {type(output_item)}")

                # å¤„ç†å­—å…¸æ ¼å¼
                if isinstance(output_item, dict):
                    logger.info("[CUSTOM] output_item is dict")
                    content_list = output_item.get('content', [])
                    logger.info(f"[CUSTOM] content_list length: {len(content_list)}")

                    for i, content_item in enumerate(content_list):
                        logger.info(f"[CUSTOM] content_item[{i}] type: {type(content_item)}")
                        if isinstance(content_item, dict) and 'text' in content_item:
                            content = content_item['text']
                            logger.info(f"[CUSTOM] Extracted content from dict, length: {len(content)}")
                            break

                # å¤„ç†å¯¹è±¡æ ¼å¼
                elif hasattr(output_item, 'content') and output_item.content:
                    logger.info("[CUSTOM] output_item is object")
                    for i, content_item in enumerate(output_item.content):
                        logger.info(f"[CUSTOM] content_item[{i}] type: {type(content_item)}")

                        # å­—å…¸è®¿é—®
                        if isinstance(content_item, dict) and 'text' in content_item:
                            content = content_item['text']
                            logger.info(f"[CUSTOM] Extracted from dict, length: {len(content)}")
                            break
                        # å¯¹è±¡è®¿é—®
                        elif hasattr(content_item, 'text'):
                            content = content_item.text
                            logger.info(f"[CUSTOM] Extracted from attr, length: {len(content)}")
                            break

            # æ ‡å‡† OpenAI æ ¼å¼ï¼šresponse.choices[0].message.content
            elif hasattr(response, 'choices') and response.choices:
                logger.info("[CUSTOM] Using 'choices' format")
                content = response.choices[0].message.content
                logger.info(f"[CUSTOM] Extracted content length: {len(content) if content else 0}")

            # å¦‚æœè¿˜æ˜¯å­—ç¬¦ä¸²ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä»¥é˜²ä¸‡ä¸€ï¼‰
            elif isinstance(response, str):
                logger.info("[CUSTOM] Response is string")
                content = response

            if not content:
                logger.error(f"[CUSTOM] Failed to extract content")
                if hasattr(response, 'model_dump'):
                    logger.error(f"[CUSTOM] Response dump: {response.model_dump()}")
                raise ValueError("Unable to extract content from API response")

            # æ£€æŸ¥æ˜¯å¦å› ä¸ºé•¿åº¦é™åˆ¶è¢«æˆªæ–­
            is_truncated = False
            if hasattr(response, 'choices') and response.choices:
                finish_reason = response.choices[0].finish_reason
                is_truncated = finish_reason == 'length'
                if is_truncated:
                    logger.warning(f"[CUSTOM] Response was truncated due to max_tokens limit (finish_reason='length')")

            result_json = self._extract_json_from_response(content, is_truncated=is_truncated)
            return self._build_response(result_json)

        except Exception as e:
            logger.error(f"Custom provider analysis failed: {e}")
            raise

    def _extract_json_from_response(self, text: str, is_truncated: bool = False) -> Dict[str, Any]:
        """Extract JSON from AI response with aggressive cleaning and multiple fallback strategies."""

        def _repair_truncated_json(raw: str) -> str:
            """å°è¯•ä¿®å¤è¢«æˆªæ–­çš„JSON"""
            # ç§»é™¤markdownä»£ç å—æ ‡è®°
            cleaned = re.sub(r"```(?:json)?", "", raw).strip()

            # æå–JSONè¾¹ç•Œ
            start = cleaned.find("{")
            if start != -1:
                cleaned = cleaned[start:]

            # æ£€æŸ¥å¹¶ä¿®å¤æˆªæ–­çš„å­—ç¬¦ä¸²
            # å¦‚æœæœ€åä¸€ä¸ªå­—ç¬¦ä¸æ˜¯é—­åˆçš„ï¼Œå°è¯•è¡¥å…¨
            if cleaned and not cleaned.endswith("}"):
                # æŸ¥æ‰¾æœ€åä¸€ä¸ªå®Œæ•´çš„ç»“æ„
                # å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„æ•°ç»„æˆ–å¯¹è±¡

                # ç»Ÿè®¡æœªé—­åˆçš„æ‹¬å·
                open_brackets = cleaned.count("[") - cleaned.count("]")
                open_braces = cleaned.count("{") - cleaned.count("}")
                open_quotes = cleaned.count('"') - cleaned.count('\\"')

                # å¦‚æœæœ‰æœªé—­åˆçš„å¼•å·ï¼Œå…ˆé—­åˆå®ƒ
                if open_quotes % 2 != 0:
                    cleaned += '"'

                # é—­åˆæœªé—­åˆçš„æ•°ç»„
                cleaned += "]" * open_brackets

                # é—­åˆæœªé—­åˆçš„å¯¹è±¡
                cleaned += "}" * open_braces

                logger.info(f"[JSON REPAIR] Fixed {open_quotes % 2} quotes, {open_brackets} brackets, {open_braces} braces")

            return cleaned

        def _sanitize_json(raw: str) -> str:
            """Aggressive JSON sanitization with multiple repair strategies."""
            # Strip markdown code blocks
            cleaned = re.sub(r"```(?:json)?", "", raw)

            # Extract JSON boundaries
            start = cleaned.find("{")
            end = cleaned.rfind("}")
            if start != -1 and end != -1 and end > start:
                cleaned = cleaned[start:end + 1]

            # Fix common JSON issues
            cleaned = re.sub(r",\s*([}\]])", r"\1", cleaned)  # Remove trailing commas
            cleaned = re.sub(r"'", '"', cleaned)  # Single quotes to double quotes
            cleaned = re.sub(r'(\w+):', r'"\1":', cleaned)  # Unquoted keys (basic fix)
            cleaned = cleaned.replace("\n", " ")  # Remove newlines

            return cleaned

        # Strategy 0: If truncated, try to repair first
        if is_truncated:
            try:
                logger.info("[JSON REPAIR] Attempting to repair truncated JSON")
                repaired = _repair_truncated_json(text)
                result = json.loads(repaired)
                logger.info("[JSON REPAIR] Successfully parsed repaired truncated JSON")
                return result
            except json.JSONDecodeError as e:
                logger.warning(f"[JSON REPAIR] Failed to parse repaired JSON: {e}")
                # Continue to other strategies

        # Strategy 1: Try direct JSON parse of full text
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # Strategy 2: Extract from ```json...``` blocks
        try:
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(1))
        except json.JSONDecodeError:
            pass

        # Strategy 3: Extract from ```...``` blocks
        try:
            code_match = re.search(r'```\s*(\{.*?\})\s*```', text, re.DOTALL)
            if code_match:
                return json.loads(code_match.group(1))
        except json.JSONDecodeError:
            pass

        # Strategy 4: Find JSON object boundaries
        try:
            json_match = re.search(r'\{.*\}', text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(0))
        except json.JSONDecodeError:
            pass

        # Strategy 5: Aggressive sanitization
        try:
            sanitized = _sanitize_json(text)
            logger.warning(f"Using sanitized JSON (original failed): {sanitized[:200]}...")
            return json.loads(sanitized)
        except json.JSONDecodeError as e:
            logger.error(f"All JSON parse strategies failed: {e}")
            logger.error(f"Raw response (first 500 chars): {text[:500]}")
            logger.error(f"Raw response (last 500 chars): {text[-500:]}")
            raise ValueError(f"Invalid JSON response from AI after all repair attempts: {str(e)}")

    def _build_response(self, result_json: Dict[str, Any]) -> ImageAnalysisResponse:
        """æ„å»ºå“åº”å¯¹è±¡"""
        try:
            # è§£æèŠ‚ç‚¹
            nodes = [
                Node(
                    id=node["id"],
                    type=node.get("type", "default"),
                    position=Position(**node["position"]),
                    data=NodeData(
                        label=node["data"]["label"],
                        shape=node["data"].get("shape")  # æ·»åŠ  shape æ”¯æŒï¼ˆæµç¨‹å›¾è¯†åˆ«ï¼‰
                    )
                )
                for node in result_json.get("nodes", [])
            ]

            # è§£æè¾¹
            edges = [
                Edge(
                    id=edge["id"],
                    source=edge["source"],
                    target=edge["target"],
                    label=edge.get("label")
                )
                for edge in result_json.get("edges", [])
            ]

            # è§£æ AI åˆ†æï¼ˆå¦‚æœæœ‰ï¼‰
            ai_analysis = None
            if "ai_analysis" in result_json and result_json["ai_analysis"]:
                ai_data = result_json["ai_analysis"]
                ai_analysis = AIAnalysis(
                    bottlenecks=[
                        ArchitectureBottleneck(**b)
                        for b in ai_data.get("bottlenecks", [])
                    ],
                    suggestions=[
                        OptimizationSuggestion(**s)
                        for s in ai_data.get("suggestions", [])
                    ],
                    confidence=ai_data.get("confidence"),
                    model_used=ai_data.get("model_used")
                )

            # æå– warningsï¼ˆæµç¨‹å›¾è¯†åˆ«ä¸“ç”¨ï¼‰
            warnings = result_json.get("warnings", [])

            # æå– analysisï¼ˆæµç¨‹å›¾åˆ†æï¼‰
            flowchart_analysis = result_json.get("analysis", {})

            return ImageAnalysisResponse(
                nodes=nodes,
                edges=edges,
                mermaid_code=result_json.get("mermaid_code", ""),
                success=True,
                ai_analysis=ai_analysis,
                warnings=warnings,  # æ–°å¢å­—æ®µ
                flowchart_analysis=flowchart_analysis  # æ–°å¢å­—æ®µ
            )

        except Exception as e:
            logger.error(f"Failed to build response: {e}")
            raise ValueError(f"Invalid response structure: {str(e)}")

    # ========== Vision-to-Text Generation (for Excalidraw/ReactFlow) ==========

    async def generate_with_vision(self, image_data: bytes, prompt: str) -> str:
        """
        Generate text response from image using vision AI.
        Returns raw text output (typically JSON) for diagram generation.

        Args:
            image_data: Image bytes
            prompt: Text prompt with instructions

        Returns:
            Raw text response from AI model
        """
        try:
            logger.info(f"[VISION GEN] Generating with {self.provider}")

            if self.provider == "gemini":
                return await self._generate_with_gemini_vision(image_data, prompt)
            elif self.provider == "openai":
                return await self._generate_with_openai_vision(image_data, prompt)
            elif self.provider == "claude":
                return await self._generate_with_claude_vision(image_data, prompt)
            elif self.provider == "siliconflow":
                return await self._generate_with_siliconflow_vision(image_data, prompt)
            elif self.provider == "custom":
                return await self._generate_with_custom_vision(image_data, prompt)
            else:
                raise ValueError(f"Unsupported provider: {self.provider}")

        except Exception as e:
            logger.error(f"Vision generation failed: {e}", exc_info=True)
            raise

    async def _generate_with_gemini_vision(self, image_data: bytes, prompt: str) -> str:
        """Gemini vision generation"""
        import PIL.Image
        import io

        image = PIL.Image.open(io.BytesIO(image_data))
        response = self.client.generate_content([prompt, image])
        return response.text

    async def _generate_with_openai_vision(self, image_data: bytes, prompt: str) -> str:
        """OpenAI vision generation"""
        import base64

        base64_image = base64.b64encode(image_data).decode()

        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[{
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/png;base64,{base64_image}"}
                    }
                ]
            }],
            max_tokens=16384,
            temperature=0.7
        )

        return response.choices[0].message.content

    async def _generate_with_claude_vision(self, image_data: bytes, prompt: str) -> str:
        """Claude vision generation"""
        import base64

        base64_image = base64.b64encode(image_data).decode()

        response = self.client.messages.create(
            model=self.model_name,
            max_tokens=16384,
            messages=[{
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": "image/png",
                            "data": base64_image
                        }
                    }
                ]
            }]
        )

        return response.content[0].text

    async def _generate_with_siliconflow_vision(self, image_data: bytes, prompt: str) -> str:
        """SiliconFlow vision generation"""
        import base64

        base64_image = base64.b64encode(image_data).decode()

        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[{
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/png;base64,{base64_image}"}
                    }
                ]
            }],
            max_tokens=16384,
            temperature=0.7
        )

        return response.choices[0].message.content

    async def _generate_with_custom_vision(self, image_data: bytes, prompt: str) -> str:
        """Custom provider vision generation (auto-detect Claude vs OpenAI format)"""
        import base64

        base64_image = base64.b64encode(image_data).decode()

        # Check if this is a Claude-native endpoint (linkflow, anthropic)
        is_claude_format = (
            self.custom_base_url and
            ("linkflow" in self.custom_base_url.lower() or
             "anthropic" in self.custom_base_url.lower())
        )

        if is_claude_format:
            # Use Anthropic's native API format
            from anthropic import Anthropic

            # Remove /v1 suffix from base_url if present (Anthropic SDK adds it automatically)
            anthropic_base_url = self.custom_base_url.rstrip("/")
            if anthropic_base_url.endswith("/v1"):
                anthropic_base_url = anthropic_base_url[:-3]

            client = Anthropic(
                api_key=self.custom_api_key,
                base_url=anthropic_base_url
            )

            response = client.messages.create(
                model=self.model_name,
                max_tokens=16384,
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image",
                            "source": {
                                "type": "base64",
                                "media_type": "image/png",
                                "data": base64_image
                            }
                        }
                    ]
                }]
            )

            return response.content[0].text
        else:
            # Use OpenAI-compatible format
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/png;base64,{base64_image}"}
                        }
                    ]
                }],
                max_tokens=16384,
                temperature=0.7
            )

            return response.choices[0].message.content

    # ========== Unified Streaming Methods (for SSE streaming to frontend) ==========

    async def generate_with_stream(self, prompt: str):
        """
        Unified streaming generation entry point supporting all providers.
        Yields text tokens as they are generated by the LLM.

        Returns: AsyncGenerator[str, None]
        """
        import queue
        import threading

        try:
            if self.provider == "openai":
                # OpenAI native streaming - use queue for real-time streaming
                logger.info(f"[STREAM] OpenAI streaming with model: {self.model_name}")
                q = queue.Queue()

                def _openai_stream():
                    try:
                        logger.info(f"[STREAM] Initiating OpenAI stream with model={self.model_name}")
                        stream = self.client.chat.completions.create(
                            model=self.model_name,
                            messages=[{"role": "user", "content": prompt}],
                            stream=True,
                            temperature=0.2,
                            max_tokens=16384,  # Increased to 16K for complete Excalidraw JSON generation
                        )
                        logger.info("[STREAM] OpenAI stream created, starting iteration")
                        for chunk in stream:
                            delta = chunk.choices[0].delta.content
                            if delta:
                                q.put(("data", delta))
                        logger.info("[STREAM] OpenAI stream completed successfully")
                        q.put(("done", None))
                    except Exception as e:
                        logger.error(f"[STREAM] Exception in _openai_stream: {e}", exc_info=True)
                        q.put(("error", e))

                loop = asyncio.get_event_loop()
                # Start streaming in background thread
                loop.run_in_executor(None, _openai_stream)

                while True:
                    msg_type, data = await loop.run_in_executor(None, q.get)
                    if msg_type == "error":
                        raise data
                    elif msg_type == "done":
                        break
                    else:
                        yield data

            elif self.provider == "claude" or (
                self.provider == "custom" and "claude" in (self.model_name or "").lower()
            ):
                # Claude streaming with context manager (supports custom Claude endpoints)
                logger.info(f"[STREAM] Claude streaming with model: {self.model_name} (provider={self.provider})")

                # Detect ikuncode.cc specifically to avoid User-Agent blocking
                use_ikuncode_raw_http = (
                    self.provider == "custom" and
                    self.custom_base_url and
                    "ikuncode.cc" in self.custom_base_url.lower()
                )

                q = queue.Queue()

                if use_ikuncode_raw_http:
                    # Use raw HTTP for ikuncode.cc to avoid User-Agent blocking
                    logger.info(f"[STREAM] Using raw HTTP for ikuncode.cc: {self.custom_base_url}")

                    def _claude_stream_raw_http():
                        try:
                            import httpx
                            import json

                            # Clean base_url to avoid /v1/v1/messages duplication
                            clean_base_url = self.custom_base_url.rstrip('/')
                            if clean_base_url.endswith('/v1'):
                                clean_base_url = clean_base_url[:-3]

                            logger.info(f"[STREAM] Initiating raw HTTP Claude stream to {clean_base_url}/v1/messages")

                            with httpx.Client(timeout=300.0) as client:
                                with client.stream(
                                    "POST",
                                    f"{clean_base_url}/v1/messages",
                                    headers={
                                        "anthropic-version": "2023-06-01",
                                        "x-api-key": self.custom_api_key,
                                        "content-type": "application/json",
                                    },
                                    json={
                                        "model": self.model_name,
                                        "max_tokens": 16384,
                                        "temperature": 0.2,
                                        "messages": [{"role": "user", "content": prompt}],
                                        "stream": True,
                                    },
                                ) as response:
                                    current_event = None
                                    buffer = ""

                                    for chunk in response.iter_bytes():
                                        buffer += chunk.decode('utf-8')

                                        while '\n' in buffer:
                                            line, buffer = buffer.split('\n', 1)
                                            line = line.strip()

                                            if line.startswith("event: "):
                                                current_event = line[7:]
                                            elif line.startswith("data: "):
                                                data = json.loads(line[6:])
                                                if current_event == "content_block_delta":
                                                    text = data.get("delta", {}).get("text", "")
                                                    if text:
                                                        q.put(("data", text))

                            logger.info("[STREAM] Raw HTTP Claude stream completed successfully")
                            q.put(("done", None))
                        except Exception as e:
                            logger.error(f"[STREAM] Exception in _claude_stream_raw_http: {e}", exc_info=True)
                            q.put(("error", e))

                    loop = asyncio.get_event_loop()
                    loop.run_in_executor(None, _claude_stream_raw_http)
                else:
                    # Use standard Anthropic SDK for other providers (linkflow.run, etc.)
                    def _claude_stream():
                        try:
                            logger.info(f"[STREAM] Initiating Claude stream with model={self.model_name}")
                            with self.client.messages.stream(
                                model=self.model_name,
                                max_tokens=16384,  # Increased to 16K for complete Excalidraw JSON generation
                                temperature=0.2,
                                messages=[{"role": "user", "content": prompt}],
                            ) as stream:
                                for text in stream.text_stream:
                                    q.put(("data", text))
                            logger.info("[STREAM] Claude stream completed successfully")
                            q.put(("done", None))
                        except Exception as e:
                            logger.error(f"[STREAM] Exception in _claude_stream: {e}", exc_info=True)
                            q.put(("error", e))

                    loop = asyncio.get_event_loop()
                    loop.run_in_executor(None, _claude_stream)

                while True:
                    msg_type, data = await loop.run_in_executor(None, q.get)
                    if msg_type == "error":
                        raise data
                    elif msg_type == "done":
                        break
                    else:
                        yield data

            elif self.provider == "gemini":
                # Gemini native async streaming
                response = await self.client.generate_content_async(
                    prompt,
                    generation_config={"temperature": 0.2},
                    stream=True,
                )
                async for chunk in response:
                    if chunk.text:
                        yield chunk.text

            elif self.provider == "siliconflow" or self.provider == "custom":
                # SiliconFlow/Custom (OpenAI-compatible) - use queue for real-time streaming
                logger.info(f"[STREAM] {self.provider} streaming with model: {self.model_name}")
                q = queue.Queue()

                def _compatible_stream():
                    try:
                        logger.info(f"[STREAM] Initiating {self.provider} stream")
                        stream = self.client.chat.completions.create(
                            model=self.model_name,
                            messages=[{"role": "user", "content": prompt}],
                            stream=True,
                            max_tokens=16384,
                            temperature=0.2,
                        )
                        logger.info(f"[STREAM] {self.provider} stream created, iterating chunks")
                        for chunk in stream:
                            if not getattr(chunk, "choices", None) or not chunk.choices:
                                continue
                            delta = chunk.choices[0].delta.content if chunk.choices[0].delta else None
                            if delta:
                                q.put(("data", delta))
                        logger.info(f"[STREAM] {self.provider} stream completed")
                        q.put(("done", None))
                    except Exception as e:
                        logger.error(f"[STREAM] Exception in {self.provider}_stream: {e}", exc_info=True)
                        q.put(("error", e))

                loop = asyncio.get_event_loop()
                loop.run_in_executor(None, _compatible_stream)

                while True:
                    msg_type, data = await loop.run_in_executor(None, q.get)
                    if msg_type == "error":
                        raise data
                    elif msg_type == "done":
                        break
                    else:
                        yield data

            else:
                raise ValueError(f"Streaming not supported for provider: {self.provider}")

        except Exception as e:
            logger.error(f"Streaming failed for {self.provider}: {e}", exc_info=True)
            raise

    async def generate_with_vision_stream(self, image_data: bytes, prompt: str):
        """
        ğŸ”¥ NEW: Streaming vision generation supporting image + text input.
        Uses multimodal streaming APIs (Claude/GPT-4 Vision support streaming).
        Yields tokens as they are generated.
        """
        import queue
        import threading

        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ Claude æ¨¡å‹ï¼ˆå®˜æ–¹æˆ– customï¼‰
            is_claude_model = (
                self.provider == "claude" or
                (self.provider == "custom" and "claude" in self.model_name.lower())
            )

            if is_claude_model:
                # Claude Vision streaming with multimodal content
                logger.info(f"[VISION-STREAM] Claude streaming with model: {self.model_name}")

                # æ£€æµ‹æ˜¯å¦æ˜¯ ikuncode.cc
                use_ikuncode_raw_http = (
                    self.provider == "custom" and
                    self.custom_base_url and
                    "ikuncode.cc" in self.custom_base_url.lower()
                )

                q = queue.Queue()

                if use_ikuncode_raw_http:
                    # ikuncode.cc: ä½¿ç”¨ raw HTTP streaming
                    def _claude_vision_stream_raw_http():
                        try:
                            import base64
                            import httpx
                            import json

                            # Detect image format
                            image_b64 = base64.b64encode(image_data).decode('utf-8')

                            # Determine media type from image bytes
                            media_type = "image/jpeg"
                            if image_data.startswith(b'\x89PNG'):
                                media_type = "image/png"
                            elif image_data.startswith(b'GIF'):
                                media_type = "image/gif"
                            elif image_data.startswith(b'\xff\xd8\xff'):
                                media_type = "image/jpeg"
                            elif image_data.startswith(b'RIFF') and b'WEBP' in image_data[:20]:
                                media_type = "image/webp"

                            # Build multimodal content
                            content = [
                                {
                                    "type": "image",
                                    "source": {
                                        "type": "base64",
                                        "media_type": media_type,
                                        "data": image_b64,
                                    },
                                },
                                {
                                    "type": "text",
                                    "text": prompt,
                                }
                            ]

                            # æ¸…ç† base_url
                            clean_base_url = self.custom_base_url.rstrip('/')
                            if clean_base_url.endswith('/v1'):
                                clean_base_url = clean_base_url[:-3]

                            logger.info(f"[VISION-STREAM] Using raw HTTP for ikuncode.cc: {clean_base_url}/v1/messages")

                            with httpx.Client(timeout=300.0) as client:
                                with client.stream(
                                    "POST",
                                    f"{clean_base_url}/v1/messages",
                                    headers={
                                        "anthropic-version": "2023-06-01",
                                        "x-api-key": self.custom_api_key,
                                        "content-type": "application/json",
                                    },
                                    json={
                                        "model": self.model_name,
                                        "max_tokens": 16384,
                                        "temperature": 0.2,
                                        "messages": [{"role": "user", "content": content}],
                                        "stream": True,
                                    },
                                ) as response:
                                    current_event = None
                                    buffer = ""

                                    for chunk in response.iter_bytes():
                                        buffer += chunk.decode('utf-8')

                                        while '\n' in buffer:
                                            line, buffer = buffer.split('\n', 1)
                                            line = line.strip()

                                            if line.startswith("event: "):
                                                current_event = line[7:]
                                            elif line.startswith("data: "):
                                                data = json.loads(line[6:])
                                                if current_event == "content_block_delta":
                                                    text = data.get("delta", {}).get("text", "")
                                                    if text:
                                                        q.put(("data", text))

                            logger.info("[VISION-STREAM] Raw HTTP Claude stream completed")
                            q.put(("done", None))
                        except Exception as e:
                            logger.error(f"[VISION-STREAM] Exception in raw HTTP: {e}", exc_info=True)
                            q.put(("error", e))

                    loop = asyncio.get_event_loop()
                    loop.run_in_executor(None, _claude_vision_stream_raw_http)
                else:
                    # linkflow.run ç­‰: ä½¿ç”¨ Anthropic SDK
                    def _claude_vision_stream():
                        try:
                            import base64
                            # Detect image format
                            image_b64 = base64.b64encode(image_data).decode('utf-8')

                            # Determine media type from image bytes
                            media_type = "image/jpeg"
                            if image_data.startswith(b'\x89PNG'):
                                media_type = "image/png"
                            elif image_data.startswith(b'GIF'):
                                media_type = "image/gif"
                            elif image_data.startswith(b'\xff\xd8\xff'):
                                media_type = "image/jpeg"
                            elif image_data.startswith(b'RIFF') and b'WEBP' in image_data[:20]:
                                media_type = "image/webp"

                            # Build multimodal content
                            content = [
                                {
                                    "type": "image",
                                    "source": {
                                        "type": "base64",
                                        "media_type": media_type,
                                        "data": image_b64,
                                    },
                                },
                                {
                                    "type": "text",
                                    "text": prompt,
                                }
                            ]

                            logger.info(f"[VISION-STREAM] Starting Claude vision stream with SDK")
                            with self.client.messages.stream(
                                model=self.model_name,
                                max_tokens=16384,
                                temperature=0.2,
                                messages=[{"role": "user", "content": content}],
                            ) as stream:
                                for text in stream.text_stream:
                                    q.put(("data", text))
                            logger.info("[VISION-STREAM] Claude stream completed")
                            q.put(("done", None))
                        except Exception as e:
                            logger.error(f"[VISION-STREAM] Exception: {e}", exc_info=True)
                            q.put(("error", e))

                    loop = asyncio.get_event_loop()
                    loop.run_in_executor(None, _claude_vision_stream)

                while True:
                    msg_type, data = await loop.run_in_executor(None, q.get)
                    if msg_type == "error":
                        raise data
                    elif msg_type == "done":
                        break
                    else:
                        yield data

            elif self.provider == "openai" or self.provider == "custom":
                # OpenAI GPT-4 Vision streaming
                logger.info(f"[VISION-STREAM] OpenAI streaming with model: {self.model_name}")
                q = queue.Queue()

                def _openai_vision_stream():
                    try:
                        import base64
                        image_b64 = base64.b64encode(image_data).decode('utf-8')

                        # Detect format
                        if image_data.startswith(b'\x89PNG'):
                            data_url = f"data:image/png;base64,{image_b64}"
                        elif image_data.startswith(b'\xff\xd8\xff'):
                            data_url = f"data:image/jpeg;base64,{image_b64}"
                        else:
                            data_url = f"data:image/jpeg;base64,{image_b64}"

                        content = [
                            {
                                "type": "image_url",
                                "image_url": {"url": data_url}
                            },
                            {
                                "type": "text",
                                "text": prompt
                            }
                        ]

                        logger.info("[VISION-STREAM] Starting OpenAI vision stream")
                        stream = self.client.chat.completions.create(
                            model=self.model_name,
                            messages=[{"role": "user", "content": content}],
                            stream=True,
                            temperature=0.2,
                            max_tokens=16384,
                        )

                        for chunk in stream:
                            delta = chunk.choices[0].delta.content
                            if delta:
                                q.put(("data", delta))
                        logger.info("[VISION-STREAM] OpenAI stream completed")
                        q.put(("done", None))
                    except Exception as e:
                        logger.error(f"[VISION-STREAM] Exception: {e}", exc_info=True)
                        q.put(("error", e))

                loop = asyncio.get_event_loop()
                loop.run_in_executor(None, _openai_vision_stream)

                while True:
                    msg_type, data = await loop.run_in_executor(None, q.get)
                    if msg_type == "error":
                        raise data
                    elif msg_type == "done":
                        break
                    else:
                        yield data

            else:
                raise ValueError(f"Streaming vision not supported for provider: {self.provider}")

        except Exception as e:
            logger.error(f"Vision stream failed: {e}", exc_info=True)
            raise

    # ========== Phase 3: Text-only Prompt Methods (for Prompter System) ==========

    async def _analyze_with_gemini_text(self, prompt: str) -> dict:
        """ä½¿ç”¨ Gemini å¤„ç†çº¯æ–‡æœ¬æç¤ºï¼ˆæ— å›¾ç‰‡ï¼‰"""
        try:
            logger.info("[GEMINI TEXT] Starting text-only analysis")

            response = await self.client.generate_content_async(
                prompt,
                generation_config={
                    "temperature": 0.2,
                    "max_output_tokens": 16384  # Increased to 16K for complete Excalidraw JSON generation
                }
            )

            logger.info(f"[GEMINI TEXT] Response received")
            result_json = self._extract_json_from_response(response.text)
            logger.info(f"[GEMINI TEXT] JSON extracted successfully")

            return result_json

        except Exception as e:
            logger.error(f"Gemini text analysis failed: {e}", exc_info=True)
            raise

    async def _analyze_with_openai_text(self, prompt: str) -> dict:
        """ä½¿ç”¨ OpenAI å¤„ç†çº¯æ–‡æœ¬æç¤ºï¼ˆæ— å›¾ç‰‡ï¼‰"""
        try:
            logger.info("[OPENAI TEXT] Starting text-only analysis")

            response = self.client.chat.completions.create(
                model="gpt-4-turbo-preview",
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=16384,  # Increased to 16K for complete Excalidraw JSON generation
                temperature=0.2
            )

            result_json = self._extract_json_from_response(
                response.choices[0].message.content
            )

            logger.info(f"[OPENAI TEXT] JSON extracted successfully")
            return result_json

        except Exception as e:
            logger.error(f"OpenAI text analysis failed: {e}", exc_info=True)
            raise

    async def _analyze_with_claude_text(self, prompt: str) -> dict:
        """ä½¿ç”¨ Claude å¤„ç†çº¯æ–‡æœ¬æç¤ºï¼ˆæ— å›¾ç‰‡ï¼‰"""
        try:
            logger.info("[CLAUDE TEXT] Starting text-only analysis")

            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=16384,  # Increased to 16K for complete Excalidraw JSON generation
                temperature=0.2,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            )

            result_json = self._extract_json_from_response(
                response.content[0].text
            )

            logger.info(f"[CLAUDE TEXT] JSON extracted successfully")
            return result_json

        except Exception as e:
            logger.error(f"Claude text analysis failed: {e}", exc_info=True)
            raise

    async def _analyze_with_siliconflow_text(self, prompt: str) -> dict:
        """ä½¿ç”¨ SiliconFlow å¤„ç†çº¯æ–‡æœ¬æç¤ºï¼ˆOpenAI å…¼å®¹ chat/completionsï¼‰"""
        try:
            logger.info("[SILICONFLOW TEXT] Starting text-only analysis")

            # Detect if this is an Excalidraw prompt (needs more tokens for element arrays)
            is_excalidraw = "excalidraw" in prompt.lower() or "elements" in prompt.lower()
            max_tokens = 16384 if is_excalidraw else 2000  # Increased to 16K for complete Excalidraw generation

            logger.info(f"[SILICONFLOW TEXT] Using max_tokens={max_tokens}, is_excalidraw={is_excalidraw}")

            # SiliconFlow SDK è°ƒç”¨æ˜¯åŒæ­¥çš„ï¼ŒåŒ…ä¸€å±‚çº¿ç¨‹ + è¶…æ—¶ï¼Œé¿å…è¯·æ±‚é•¿æ—¶é—´æŒ‚èµ·
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    self.client.chat.completions.create,
                    model=self.model_name,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=0.3,
                    top_p=0.7,
                    frequency_penalty=0.5,
                    stream=False,
                    # å¼ºåˆ¶è¿”å›çº¯ JSON å¯¹è±¡ï¼Œé¿å…æ¨¡å‹è¾“å‡ºé¢å¤–è¯´æ˜æ–‡å­—
                    response_format={"type": "json_object"},
                ),
                timeout=self.request_timeout,
            )

            # å½“ response_format ä¸º json_object æ—¶ï¼Œcontent åº”å·²æ˜¯ JSON å¯¹è±¡
            content = response.choices[0].message.content
            if isinstance(content, dict):
                result_json = content
            else:
                result_json = self._extract_json_from_response(content)

            logger.info("[SILICONFLOW TEXT] JSON extracted successfully")
            return result_json

        except Exception as e:
            logger.error(f"SiliconFlow text analysis failed: {e}", exc_info=True)
            raise

    async def _analyze_with_siliconflow_text_stream(self, prompt: str) -> str:
        """SiliconFlow streaming text -> concatenated text output."""
        try:
            logger.info("[SILICONFLOW STREAM] Starting text stream")

            def _stream():
                stream = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=1200,
                    temperature=0.3,
                    top_p=0.7,
                    frequency_penalty=0.5,
                    stream=True,
                    response_format={"type": "text"},
                )
                text_acc = []
                for chunk in stream:
                    delta = chunk.choices[0].delta.content
                    if not delta:
                        continue
                    piece = "".join(delta)
                    text_acc.append(piece)
                return "".join(text_acc)

            full_text = await asyncio.wait_for(asyncio.to_thread(_stream), timeout=self.request_timeout)
            logger.info("[SILICONFLOW STREAM] Collected length=%s", len(full_text))
            if not full_text.strip():
                raise ValueError("Empty stream output")
            return full_text
        except Exception as e:
            logger.error(f"SiliconFlow streaming failed: {e}", exc_info=True)
            raise

    async def _analyze_with_custom_text(self, prompt: str) -> dict:
        """ä½¿ç”¨è‡ªå®šä¹‰ provider å¤„ç†çº¯æ–‡æœ¬æç¤ºï¼ˆæ— å›¾ç‰‡ï¼‰"""
        try:
            logger.info("[CUSTOM TEXT] Starting text-only analysis")

            # æ£€æµ‹æ˜¯å¦æ˜¯ Claude æ¨¡å‹
            model_name = self.custom_model_name or "gpt-3.5-turbo"
            is_claude_model = "claude" in model_name.lower()

            if is_claude_model:
                # Claude æ¨¡å‹ï¼šä½¿ç”¨ raw HTTP è¯·æ±‚é¿å… User-Agent é˜»æ‹¦
                logger.info(f"[CUSTOM TEXT] Detected Claude model: {model_name}, using raw HTTP")
                import httpx

                # æ¸…ç† base_urlï¼Œé¿å…é‡å¤æ‹¼æ¥ /v1
                clean_base_url = self.custom_base_url.rstrip('/')
                if clean_base_url.endswith('/v1'):
                    clean_base_url = clean_base_url[:-3]

                headers = {
                    "x-api-key": self.custom_api_key,
                    "anthropic-version": "2023-06-01",
                    "content-type": "application/json"
                }

                data = {
                    "model": model_name,
                    "max_tokens": 16384,
                    "messages": [
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ]
                }

                logger.info(f"[CUSTOM TEXT] Sending request to: {clean_base_url}/v1/messages")

                async with httpx.AsyncClient(timeout=120.0) as http_client:
                    response = await http_client.post(
                        f"{clean_base_url}/v1/messages",
                        headers=headers,
                        json=data
                    )

                    if response.status_code != 200:
                        error_text = response.text
                        logger.error(f"[CUSTOM TEXT] Claude API error: {response.status_code} - {error_text}")
                        raise ValueError(f"Claude API request failed: {response.status_code} - {error_text}")

                    result = response.json()
                    content = result['content'][0]['text']
                    logger.info(f"[CUSTOM TEXT] Claude response received, length: {len(content)}")

            else:
                # OpenAI å…¼å®¹æ¨¡å‹ï¼šä½¿ç”¨ OpenAI SDK
                logger.info(f"[CUSTOM TEXT] Using OpenAI-compatible format for model: {model_name}")
                response = self.client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    max_tokens=16384,
                    temperature=0.2
                )

                # Handle different response formats
                content = None
                if hasattr(response, 'choices') and response.choices:
                    content = response.choices[0].message.content
                elif hasattr(response, 'output') and response.output:
                    output_item = response.output[0]
                    if isinstance(output_item, dict):
                        content_list = output_item.get('content', [])
                        for content_item in content_list:
                            if isinstance(content_item, dict) and 'text' in content_item:
                                content = content_item['text']
                                break

                if not content:
                    raise ValueError("Unable to extract content from custom provider response")

            result_json = self._extract_json_from_response(content)
            logger.info(f"[CUSTOM TEXT] JSON extracted successfully")
            return result_json

        except Exception as e:
            logger.error(f"Custom text analysis failed: {e}", exc_info=True)
            raise

    def _build_mock_script(self, arch_desc: str, duration: str) -> str:
        """Local fallback script used when provider is unavailable or mocked."""
        intro = arch_desc.replace("\n", " ").strip()
        return (
            f"[Mock {duration} script] {intro}\n"
            "AI provider unavailable or invalid key. Connect a valid provider to generate a full presentation script."
        )

    # ========== Phase 4: Speech Script Generation ==========

    async def generate_speech_script(
        self,
        nodes: List,
        edges: List,
        duration: str = "2min"
    ) -> str:
        """Generate a presentation script for the architecture"""

        # Build architecture description
        arch_desc = f"Architecture with {len(nodes)} components and {len(edges)} connections:\\n\\n"

        arch_desc += "Components:\\n"
        for node in nodes:
            arch_desc += f"- {node.data.label} (type: {node.type or 'default'})\\n"

        arch_desc += "\\nConnections:\\n"
        for edge in edges:
            source_node = next((n for n in nodes if n.id == edge.source), None)
            target_node = next((n for n in nodes if n.id == edge.target), None)
            if source_node and target_node:
                label_text = f" ({edge.label})" if edge.label else ""
                arch_desc += f"- {source_node.data.label} â†’ {target_node.data.label}{label_text}\\n"

        # Duration-specific prompts (ä¸­æ–‡è¾“å‡º)
        duration_prompts = {
            "30s": "ç”Ÿæˆä¸€ä¸ª30ç§’çš„ç”µæ¢¯æ¼”è®²ç¨¿ï¼ˆçº¦150å­—ï¼‰ã€‚èšç„¦æ ¸å¿ƒä»·å€¼ä¸»å¼ ã€‚",
            "2min": "ç”Ÿæˆä¸€ä¸ª2åˆ†é’Ÿçš„æ¼”è®²ç¨¿ï¼ˆçº¦600å­—ï¼‰ã€‚æ¶µç›–æ¶æ„æ¦‚è§ˆã€æ ¸å¿ƒç»„ä»¶å’Œä¼˜åŠ¿ã€‚",
            "5min": "ç”Ÿæˆä¸€ä¸ª5åˆ†é’Ÿçš„è¯¦ç»†æ¼”è®²ç¨¿ï¼ˆçº¦1500å­—ï¼‰ã€‚åŒ…å«å¼€åœºã€æ¶æ„æ¦‚è§ˆã€ç»„ä»¶ç»†èŠ‚ã€æ•°æ®æµå’Œç»“è®ºã€‚"
        }

        prompt = f'''ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æŠ€æœ¯æ¼”è®²è€…ï¼Œæ­£åœ¨åˆ›å»ºä¸€ä»½æ¼”è®²ç¨¿ã€‚

{arch_desc}

{duration_prompts.get(duration, duration_prompts["2min"])}

è¦æ±‚ï¼š
1. ä½¿ç”¨æ¸…æ™°ã€ä¸“ä¸šçš„ä¸­æ–‡è¡¨è¾¾
2. ç”¨é€šä¿—æ˜“æ‡‚çš„æ–¹å¼è§£é‡ŠæŠ€æœ¯æ¦‚å¿µ
3. çªå‡ºæ¶æ„çš„ä¼˜åŠ¿å’Œè®¾è®¡å†³ç­–
4. æ®µè½ä¹‹é—´è¿‡æ¸¡è‡ªç„¶æµç•…
5. ä»¥æœ‰åŠ›çš„ç»“è®ºæ”¶å°¾
6. åªè¿”å›æ¼”è®²ç¨¿æ–‡æœ¬ï¼Œä¸è¦è¿”å›JSONæˆ–å…¶ä»–æ ¼å¼

ç°åœ¨å¼€å§‹åˆ›ä½œæ¼”è®²ç¨¿ï¼š'''

        if self.mock_mode:
            logger.warning("Mock mode enabled for speech script generation (placeholder API key)")
            return self._build_mock_script(arch_desc, duration)

        async def _generate_with_provider():
            if self.provider == "gemini":
                response = await self.client.generate_content_async(
                    prompt,
                    generation_config={"temperature": 0.7}
                )
                return response.text.strip()

            if self.provider == "openai":
                response = await asyncio.to_thread(
                    self.client.chat.completions.create,
                    model=self.model_name,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.7,
                    timeout=self.request_timeout,
                )
                return response.choices[0].message.content.strip()

            if self.provider == "siliconflow":
                response = await asyncio.to_thread(
                    self.client.chat.completions.create,
                    model=self.model_name,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.7,
                    max_tokens=2000,
                    timeout=self.request_timeout,
                )
                return response.choices[0].message.content.strip()

            if self.provider == "claude":
                response = await asyncio.to_thread(
                    self.client.messages.create,
                    model=self.model_name,
                    max_tokens=2000,
                    temperature=0.7,
                    messages=[{"role": "user", "content": prompt}]
                )
                return response.content[0].text.strip()

            # custom provider (OpenAI-compatible)
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=4000,
                timeout=self.request_timeout,
            )
            return response.choices[0].message.content.strip()

        try:
            script = await asyncio.wait_for(_generate_with_provider(), timeout=self.request_timeout)
            logger.info(f"Generated {duration} speech script ({len(script)} characters)")
            return script

        except asyncio.TimeoutError:
            logger.warning("Speech script generation timed out; returning mock script")
            return self._build_mock_script(arch_desc, duration)
        except Exception as e:
            logger.error(f"Speech script generation failed: {e}", exc_info=True)
            return self._build_mock_script(arch_desc, duration)

    async def generate_speech_script_stream(
        self,
        nodes: List,
        edges: List,
        duration: str = "2min"
    ):
        """Generate a presentation script with streaming support"""
        # Build architecture description
        arch_desc = f"Architecture with {len(nodes)} components and {len(edges)} connections:\n\n"

        arch_desc += "Components:\n"
        for node in nodes:
            arch_desc += f"- {node.data.label} (type: {node.type or 'default'})\n"

        arch_desc += "\nConnections:\n"
        for edge in edges:
            source_node = next((n for n in nodes if n.id == edge.source), None)
            target_node = next((n for n in nodes if n.id == edge.target), None)
            if source_node and target_node:
                label_text = f" ({edge.label})" if edge.label else ""
                arch_desc += f"- {source_node.data.label} â†’ {target_node.data.label}{label_text}\n"

        # Duration-specific prompts (ä¸­æ–‡è¾“å‡º)
        duration_prompts = {
            "30s": "ç”Ÿæˆä¸€ä¸ª30ç§’çš„ç”µæ¢¯æ¼”è®²ç¨¿ï¼ˆçº¦150å­—ï¼‰ã€‚èšç„¦æ ¸å¿ƒä»·å€¼ä¸»å¼ ã€‚",
            "2min": "ç”Ÿæˆä¸€ä¸ª2åˆ†é’Ÿçš„æ¼”è®²ç¨¿ï¼ˆçº¦600å­—ï¼‰ã€‚æ¶µç›–æ¶æ„æ¦‚è§ˆã€æ ¸å¿ƒç»„ä»¶å’Œä¼˜åŠ¿ã€‚",
            "5min": "ç”Ÿæˆä¸€ä¸ª5åˆ†é’Ÿçš„è¯¦ç»†æ¼”è®²ç¨¿ï¼ˆçº¦1500å­—ï¼‰ã€‚åŒ…å«å¼€åœºã€æ¶æ„æ¦‚è§ˆã€ç»„ä»¶ç»†èŠ‚ã€æ•°æ®æµå’Œç»“è®ºã€‚"
        }

        prompt = f'''ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æŠ€æœ¯æ¼”è®²è€…ï¼Œæ­£åœ¨åˆ›å»ºä¸€ä»½æ¼”è®²ç¨¿ã€‚

{arch_desc}

{duration_prompts.get(duration, duration_prompts["2min"])}

è¦æ±‚ï¼š
1. ä½¿ç”¨æ¸…æ™°ã€ä¸“ä¸šçš„ä¸­æ–‡è¡¨è¾¾
2. ç”¨é€šä¿—æ˜“æ‡‚çš„æ–¹å¼è§£é‡ŠæŠ€æœ¯æ¦‚å¿µ
3. çªå‡ºæ¶æ„çš„ä¼˜åŠ¿å’Œè®¾è®¡å†³ç­–
4. æ®µè½ä¹‹é—´è¿‡æ¸¡è‡ªç„¶æµç•…
5. ä»¥æœ‰åŠ›çš„ç»“è®ºæ”¶å°¾
6. åªè¿”å›æ¼”è®²ç¨¿æ–‡æœ¬ï¼Œä¸è¦è¿”å›JSONæˆ–å…¶ä»–æ ¼å¼

ç°åœ¨å¼€å§‹åˆ›ä½œæ¼”è®²ç¨¿ï¼š'''

        if self.mock_mode:
            logger.warning("Mock mode enabled for speech script generation (placeholder API key)")
            yield self._build_mock_script(arch_desc, duration)
            return

        try:
            # OpenAI-compatible providers support streaming
            if self.provider in ["openai", "siliconflow", "custom"]:
                logger.info(f"Starting streaming speech script generation with {self.provider}")

                # ç›´æ¥åˆ›å»ºstreamå¹¶åŒæ­¥è¿­ä»£ï¼ˆå‚è€ƒchat_generatorçš„å®ç°ï¼‰
                stream = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.7,
                    max_tokens=4000,
                    stream=True,
                )

                # åŒæ­¥è¿­ä»£stream chunks
                for chunk in stream:
                    if chunk.choices and chunk.choices[0].delta.content:
                        yield chunk.choices[0].delta.content

            elif self.provider == "claude":
                logger.info("Starting streaming speech script generation with Claude")
                # Claude uses async streaming API
                async with self.client.messages.stream(
                    model=self.model_name,
                    max_tokens=4000,
                    temperature=0.7,
                    messages=[{"role": "user", "content": prompt}]
                ) as stream:
                    async for text in stream.text_stream:
                        yield text

            else:
                # Fallback to non-streaming for other providers (like Gemini)
                logger.info(f"Provider {self.provider} doesn't support streaming, using non-streaming")
                full_script = await self.generate_speech_script(nodes, edges, duration)
                # Yield in chunks for simulated streaming
                chunk_size = 20
                for i in range(0, len(full_script), chunk_size):
                    yield full_script[i:i+chunk_size]

        except Exception as e:
            logger.error(f"Streaming speech script generation failed: {e}", exc_info=True)
            yield self._build_mock_script(arch_desc, duration)


# å·¥å‚å‡½æ•°
def create_vision_service(
    provider: str,
    api_key: Optional[str] = None,
    base_url: Optional[str] = None,
    model_name: Optional[str] = None
) -> AIVisionService:
    """åˆ›å»º Vision Service å®ä¾‹"""
    return AIVisionService(
        provider=provider,
        api_key=api_key,
        base_url=base_url,
        model_name=model_name
    )
