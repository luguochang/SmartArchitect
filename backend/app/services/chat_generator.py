import json
import logging
import math
import time
from pathlib import Path
from typing import List, Optional, Tuple, Any

from fastapi import HTTPException
from app.api.mermaid import parse_mermaid_to_graph, graph_to_mermaid
from app.models.schemas import (
    FlowTemplate,
    FlowTemplateList,
    ChatGenerationRequest,
    ChatGenerationResponse,
    Node,
    Edge,
    Position,
    NodeData,
)
from app.services.ai_vision import create_vision_service
from app.services.model_presets import get_model_presets_service
from app.services.session_manager import get_session_manager

logger = logging.getLogger(__name__)


# Architecture type templates configuration
ARCHITECTURE_TEMPLATES = {
    "layered": {
        "name": "åˆ†å±‚æ¶æ„",
        "name_en": "Layered Architecture",
        "layers": ["frontend", "backend", "middleware", "data", "infrastructure"],
        "default_columns": 5,
        "show_edges": False,
        "description": "é€šç”¨å‰åç«¯åˆ†å±‚æ¶æ„ï¼Œé€‚ç”¨äºWebåº”ç”¨ã€å¾®æœåŠ¡ç­‰",
    },
    "business": {
        "name": "ä¸šåŠ¡æ¶æ„",
        "name_en": "Business Architecture",
        "layers": ["capability", "service", "process", "organization"],
        "default_columns": 6,
        "show_edges": False,  # Business architecture focuses on layers, not connections
        "description": "ä¸šåŠ¡èƒ½åŠ›åœ°å›¾ï¼Œå±•ç¤ºä¼ä¸šèƒ½åŠ›ã€æœåŠ¡ã€æµç¨‹å’Œç»„ç»‡å…³ç³»",
    },
    "technical": {
        "name": "æŠ€æœ¯æ¶æ„",
        "name_en": "Technical Architecture",
        "layers": ["presentation", "application", "integration", "data", "infrastructure"],
        "default_columns": 5,
        "show_edges": True,  # Technical architecture shows API calls and data flow
        "edge_type": "data-flow",
        "description": "æŠ€æœ¯ç³»ç»Ÿæ¶æ„ï¼Œå±•ç¤ºåº”ç”¨ç»„ä»¶ä¹‹é—´çš„æŠ€æœ¯ä¾èµ–å’Œæ•°æ®æµ",
    },
    "deployment": {
        "name": "éƒ¨ç½²æ¶æ„",
        "name_en": "Deployment Architecture",
        "layers": ["dmz", "app-tier", "data-tier", "monitoring"],
        "default_columns": 4,
        "show_edges": True,  # Deployment shows network connectivity
        "edge_type": "network",
        "description": "åŸºç¡€è®¾æ–½éƒ¨ç½²æ¶æ„ï¼Œå±•ç¤ºç½‘ç»œæ‹“æ‰‘ã€æœåŠ¡å™¨ã€å®¹å™¨ç¼–æ’",
    },
    "domain": {
        "name": "é¢†åŸŸæ¶æ„",
        "name_en": "Domain-Driven Architecture",
        "layers": ["domain-services", "shared-kernel", "anti-corruption", "infrastructure"],
        "default_columns": 4,
        "show_edges": True,
        "edge_type": "dependency",
        "description": "é¢†åŸŸé©±åŠ¨è®¾è®¡æ¶æ„ï¼Œå±•ç¤ºæœ‰ç•Œä¸Šä¸‹æ–‡å’Œé¢†åŸŸæœåŠ¡",
    },
}


class ChatGeneratorService:
    """Chat-based flowchart generation service (mock-first)."""

    def __init__(self):
        self.templates_path = Path(__file__).parent.parent / "data" / "flow_templates.json"
        self.templates: List[FlowTemplate] = []
        self._load_templates()

    def _load_templates(self):
        """Load templates from JSON file."""
        try:
            with open(self.templates_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                self.templates = [FlowTemplate(**tpl) for tpl in data["templates"]]
            logger.info(f"Loaded {len(self.templates)} flow templates")
        except Exception as e:
            logger.error(f"Failed to load templates: {e}")
            self.templates = []

    def get_all_templates(self) -> FlowTemplateList:
        """Get all templates."""
        return FlowTemplateList(templates=self.templates)

    def get_template(self, template_id: str) -> Optional[FlowTemplate]:
        """Get template by ID."""
        for tpl in self.templates:
            if tpl.id == template_id:
                return tpl
        return None

    def _build_generation_prompt(self, request: ChatGenerationRequest) -> str:
        """Build generation prompt for flow or architecture diagrams."""
        template_context = ""
        if request.template_id:
            template = self.get_template(request.template_id)
            if template:
                template_context = f"""
Reference Template: {template.name}
Template Description: {template.description}
Example Input: "{template.example_input}"
Expected Style/Category: {template.category}

Please generate a similar flowchart following this template's style.
"""

        if request.diagram_type == "architecture":
            # Get architecture type template
            arch_type = request.architecture_type or "layered"
            template = ARCHITECTURE_TEMPLATES.get(arch_type, ARCHITECTURE_TEMPLATES["layered"])

            # Build layer-specific guidance with STRONG differentiation
            layer_examples = ""
            if arch_type == "business":
                layer_examples = """
**ğŸ¢ BUSINESS ARCHITECTURE - Focus on BUSINESS CAPABILITIES:**
CRITICAL: This is a BUSINESS view, NOT technical implementation!

**Required Layers (use EXACTLY these names):**
1. "capability" (èƒ½åŠ›å±‚) - 10-15 business capabilities
   - Examples: è®¿å®¢ç®¡ç†, è½¦è¾†ç®¡ç†, å®‰é˜²ç›‘æ§, èƒ½æºç®¡ç†, ç‰©ä¸šæœåŠ¡, æ™ºèƒ½æ¥¼å®‡, ç¯å¢ƒç›‘æµ‹, èµ„äº§ç®¡ç†, åº”æ€¥æŒ‡æŒ¥, æ•°æ®åˆ†æ, ç»Ÿä¸€è®¤è¯, æ¶ˆæ¯é€šçŸ¥
   - Use category: "service" for all capability items
   - NO tech stack - focus on WHAT the business does

2. "service" (æœåŠ¡å±‚) - 5-8 business service systems
   - Examples: è®¿å®¢é¢„çº¦ç³»ç»Ÿ, è½¦è¾†è¯†åˆ«ç³»ç»Ÿ, è§†é¢‘ç›‘æ§å¹³å°, èƒ½è€—åˆ†æå¹³å°, ç‰©ä¸šå·¥å•ç³»ç»Ÿ
   - Use category: "platform"
   - Add tech_stack ONLY if relevant to business stakeholders

3. "process" (æµç¨‹å±‚) - 3-5 key business processes
   - Examples: è®¿å®¢å…¥å›­æµç¨‹, è½¦è¾†è¿›å‡ºæµç¨‹, åº”æ€¥å“åº”æµç¨‹
   - Use category: "default"

4. "organization" (ç»„ç»‡å±‚) - 3-5 organizational units
   - Examples: è¿è¥ä¸­å¿ƒ, å®‰ä¿éƒ¨é—¨, ç‰©ä¸šéƒ¨é—¨, ITéƒ¨é—¨
   - Use category: "default"

**STRICT RULES:**
- NO database/cache/queue components (those are technical!)
- NO infrastructure components (K8s, Docker, etc.)
- Focus on BUSINESS VALUE and CAPABILITIES
- Use business-friendly language (Chinese preferred)
"""
            elif arch_type == "technical":
                layer_examples = """
**âš™ï¸ TECHNICAL ARCHITECTURE - Focus on TECHNICAL COMPONENTS:**
CRITICAL: This is a TECHNICAL view showing HOW systems are built!

**Required Layers (use EXACTLY these names):**
1. "presentation" (è¡¨ç°å±‚) - 3-5 UI components
   - Examples: Reactå‰ç«¯, Vueç®¡ç†åå°, ç§»åŠ¨ç«¯App, å°ç¨‹åº
   - Use category: "api"
   - tech_stack: ["React", "TypeScript"], ["Vue", "Element UI"], etc.

2. "application" (åº”ç”¨å±‚) - 6-10 application services
   - Examples: ç”¨æˆ·æœåŠ¡, è®¢å•æœåŠ¡, æ”¯ä»˜æœåŠ¡, é€šçŸ¥æœåŠ¡, è®¤è¯æœåŠ¡
   - Use category: "service"
   - tech_stack: ["Spring Boot", "Java"], ["FastAPI", "Python"], etc.

3. "integration" (é›†æˆå±‚) - 4-6 integration components
   - Examples: API Gateway, æ¶ˆæ¯é˜Ÿåˆ—, ESBæ€»çº¿, æœåŠ¡ç½‘æ ¼
   - Use category: "network"
   - tech_stack: ["Kong"], ["Kafka"], ["Istio"], etc.

4. "data" (æ•°æ®å±‚) - 4-6 data storage systems
   - Examples: MySQLä¸»åº“, Redisç¼“å­˜, MongoDBæ–‡æ¡£åº“, Elasticsearchæœç´¢
   - Use category: "database"
   - tech_stack: ["MySQL 8.0"], ["Redis Cluster"], etc.

5. "infrastructure" (åŸºç¡€è®¾æ–½å±‚) - 3-5 infrastructure components
   - Examples: Kubernetesé›†ç¾¤, Dockerå®¹å™¨, Nginxè´Ÿè½½å‡è¡¡, äº‘æœåŠ¡å™¨
   - Use category: "infrastructure"
   - tech_stack: ["K8s"], ["Docker"], ["Nginx"], etc.

**STRICT RULES:**
- MUST include tech_stack for every item
- Use technical terminology (API, Service, Gateway, etc.)
- Show HOW components interact (include edges!)
- Focus on IMPLEMENTATION details
"""
            elif arch_type == "deployment":
                layer_examples = """
**ğŸš€ DEPLOYMENT ARCHITECTURE - Focus on INFRASTRUCTURE TOPOLOGY:**
CRITICAL: This is a DEPLOYMENT view showing WHERE systems run!

**Required Layers (use EXACTLY these names):**
1. "dmz" (DMZåŒº) - 3-4 edge components
   - Examples: Nginxè´Ÿè½½å‡è¡¡, WAFé˜²ç«å¢™, CDNèŠ‚ç‚¹, SSLç»ˆç»“
   - Use category: "network"
   - tech_stack: ["Nginx", "HAProxy"], ["ModSecurity"], etc.
   - note: Include IP ranges or network segments

2. "app-tier" (åº”ç”¨å±‚) - 5-8 application deployment units
   - Examples: K8s Pod (è®¢å•æœåŠ¡), Dockerå®¹å™¨ (ç”¨æˆ·æœåŠ¡), Tomcatå®ä¾‹
   - Use category: "compute"
   - tech_stack: ["K8s Deployment"], ["Docker Compose"], etc.
   - note: Include replica counts (e.g., "3å‰¯æœ¬")

3. "data-tier" (æ•°æ®å±‚) - 3-5 data storage deployments
   - Examples: MySQLä¸»ä»é›†ç¾¤, Rediså“¨å…µé›†ç¾¤, MinIOå¯¹è±¡å­˜å‚¨
   - Use category: "storage"
   - tech_stack: ["MySQL 8.0 ä¸»ä»"], ["Redis Sentinel"], etc.
   - note: Include HA configuration

4. "monitoring" (ç›‘æ§å±‚) - 3-4 monitoring/logging systems
   - Examples: Prometheusç›‘æ§, Grafanaå¯è§†åŒ–, ELKæ—¥å¿—, Jaegeré“¾è·¯è¿½è¸ª
   - Use category: "platform"
   - tech_stack: ["Prometheus"], ["Grafana"], ["Elasticsearch"], etc.

**STRICT RULES:**
- MUST include deployment details (replicas, HA, network)
- Show PHYSICAL/LOGICAL deployment topology
- Include edges for network connections
- Focus on OPERATIONS and INFRASTRUCTURE
"""
            elif arch_type == "domain":
                layer_examples = """
**ğŸ¯ DOMAIN-DRIVEN ARCHITECTURE - Focus on BOUNDED CONTEXTS:**
CRITICAL: This is a DDD view showing domain boundaries!

**Required Layers (use EXACTLY these names):**
1. "domain-services" (é¢†åŸŸæœåŠ¡å±‚) - 5-8 bounded contexts
   - Examples: è®¢å•åŸŸ, ç”¨æˆ·åŸŸ, æ”¯ä»˜åŸŸ, åº“å­˜åŸŸ, ç‰©æµåŸŸ
   - Use category: "service"
   - note: Include domain responsibilities

2. "shared-kernel" (å…±äº«å†…æ ¸å±‚) - 2-4 shared components
   - Examples: é€šç”¨å·¥å…·ç±», é¢†åŸŸäº‹ä»¶æ€»çº¿, å…±äº«å€¼å¯¹è±¡
   - Use category: "platform"

3. "anti-corruption" (é˜²è…å±‚) - 2-3 integration adapters
   - Examples: å¤–éƒ¨æ”¯ä»˜é€‚é…å™¨, ç¬¬ä¸‰æ–¹ç‰©æµé€‚é…å™¨
   - Use category: "network"

4. "infrastructure" (åŸºç¡€è®¾æ–½å±‚) - 3-4 infrastructure services
   - Examples: æ•°æ®æŒä¹…åŒ–, æ¶ˆæ¯å‘å¸ƒ, ç¼“å­˜æœåŠ¡
   - Use category: "infrastructure"

**STRICT RULES:**
- Use DDD terminology (Bounded Context, Aggregate, etc.)
- Show domain boundaries clearly
- Include edges for domain events
"""
            else:  # layered (default/generic)
                layer_examples = """
**ğŸ¢ LAYERED ARCHITECTURE - Generic multi-tier structure:**

**Required Layers (use EXACTLY these names):**
1. "frontend" (å‰ç«¯å±‚) - 2-4 client applications
2. "backend" (åç«¯å±‚) - 4-6 backend services
3. "middleware" (ä¸­é—´ä»¶å±‚) - 3-5 middleware components
4. "data" (æ•°æ®å±‚) - 3-4 data storage systems
5. "infrastructure" (åŸºç¡€è®¾æ–½å±‚) - 2-3 infrastructure components

**RULES:**
- Generic architecture, suitable for most systems
- Balance between business and technical views
"""

            # Edge generation guidance
            edge_guidance = ""
            if template.get("show_edges", False):
                edge_guidance = """
**EDGE/CONNECTION RULES:**
- Include "edges" array to show dependencies/data flow
- Each edge: {{"source": "layer-item-id", "target": "layer-item-id", "label": "connection type"}}
- Example: {{"source": "application-0", "target": "integration-0", "label": "APIè°ƒç”¨"}}
- Keep edge labels concise (APIè°ƒç”¨, æ•°æ®æµ, ç½‘ç»œè¿æ¥, etc.)
"""
            else:
                edge_guidance = """
**EDGE RULE:** Do NOT include "edges" array - this architecture type focuses on layer organization.
"""

            return f"""You are a professional systems architect. Generate a {template['name']} ({template['name_en']}) with clear hierarchy and organization.

**ARCHITECTURE TYPE:** {template['name']}
**DESCRIPTION:** {template['description']}

**CRITICAL OUTPUT FORMAT:**
1. Return ONLY valid JSON (no markdown code blocks)
2. Structure:
   {{
     "layers": [
       {{
         "name": "layer-name",
         "layout": {{ "columns": 4-6 }},
         "items": [
           {{"label": "Component Name", "tech_stack": ["Technology"], "note": "Description", "category": "service"}}
         ]
       }}
     ],
     "edges": [...]  // Only if showing dependencies
   }}

{layer_examples}

3. **GRID LAYOUT SUPPORT:**
   - Each layer can specify "layout": {{ "columns": N }}
   - Default columns: {template.get('default_columns', 4)}
   - For layers with many items (10+), use more columns (5-6)
   - For simple layers (< 5 items), use fewer columns (3-4)

4. **Item Format:**
   - "label": Component/service name (Chinese if user input is Chinese)
   - "tech_stack": Array of technologies used (e.g., ["React", "Next.js"])
   - "note": Brief description or tech stack summary
   - "category": Optional category for icon selection ("service", "database", "api", "platform", etc.)

5. **SMART CARD DISTRIBUTION:**
   - Capability/service layers: 6-12 items across 4-6 columns
   - Application layer: 5-10 items across 3-5 columns
   - Integration layer: 3-6 items across 3-4 columns
   - Data layer: 3-5 items across 3-4 columns
   - Automatically wrap items into multiple rows

{edge_guidance}

**EXAMPLE OUTPUT (Smart Park Business Architecture):**
{{
  "layers": [
    {{
      "name": "capability",
      "layout": {{ "columns": 6 }},
      "items": [
        {{"label": "è®¿å®¢ç®¡ç†", "category": "service", "note": "è®¿å®¢é¢„çº¦ä¸ç™»è®°"}},
        {{"label": "è½¦è¾†ç®¡ç†", "category": "service", "note": "è½¦è¾†è¯†åˆ«ä¸åœè½¦"}},
        {{"label": "å®‰é˜²ç›‘æ§", "category": "service", "note": "è§†é¢‘ç›‘æ§ä¸é¢„è­¦"}},
        {{"label": "èƒ½æºç®¡ç†", "category": "service", "note": "èƒ½è€—åˆ†æä¸ä¼˜åŒ–"}},
        {{"label": "ç‰©ä¸šæœåŠ¡", "category": "service", "note": "å·¥å•ä¸æŠ•è¯‰å¤„ç†"}},
        {{"label": "æ™ºèƒ½æ¥¼å®‡", "category": "service", "note": "æ¥¼å®‡è‡ªæ§ä¸è°ƒèŠ‚"}},
        {{"label": "ç¯å¢ƒç›‘æµ‹", "category": "service", "note": "ç©ºæ°”è´¨é‡ä¸æ¸©æ¹¿åº¦"}},
        {{"label": "èµ„äº§ç®¡ç†", "category": "service", "note": "èµ„äº§ç›˜ç‚¹ä¸è¿½è¸ª"}},
        {{"label": "åº”æ€¥æŒ‡æŒ¥", "category": "service", "note": "åº”æ€¥å“åº”ä¸è°ƒåº¦"}},
        {{"label": "æ•°æ®åˆ†æ", "category": "platform", "note": "æ•°æ®å¯è§†åŒ–ä¸æŠ¥è¡¨"}},
        {{"label": "ç»Ÿä¸€è®¤è¯", "category": "platform", "note": "å•ç‚¹ç™»å½•ä¸æƒé™"}},
        {{"label": "æ¶ˆæ¯é€šçŸ¥", "category": "platform", "note": "æ¶ˆæ¯æ¨é€ä¸æé†’"}}
      ]
    }},
    {{
      "name": "application",
      "layout": {{ "columns": 5 }},
      "items": [
        {{"label": "è®¿å®¢é¢„çº¦ç³»ç»Ÿ", "tech_stack": ["Vue", "Spring Boot"], "note": "è®¿å®¢é¢„çº¦ä¸å®¡æ‰¹"}},
        {{"label": "è½¦è¾†è¯†åˆ«ç³»ç»Ÿ", "tech_stack": ["TensorFlow", "FastAPI"], "note": "è½¦ç‰Œè¯†åˆ«ä¸æŠ“æ‹"}},
        {{"label": "è§†é¢‘ç›‘æ§å¹³å°", "tech_stack": ["WebRTC", "Node.js"], "note": "è§†é¢‘æµå¤„ç†"}},
        {{"label": "èƒ½è€—åˆ†æå¹³å°", "tech_stack": ["Grafana", "InfluxDB"], "note": "èƒ½è€—æ•°æ®åˆ†æ"}},
        {{"label": "ç‰©ä¸šå·¥å•ç³»ç»Ÿ", "tech_stack": ["React", "Django"], "note": "å·¥å•æµè½¬å¤„ç†"}}
      ]
    }},
    {{
      "name": "integration",
      "layout": {{ "columns": 4 }},
      "items": [
        {{"label": "API Gateway", "tech_stack": ["Kong"], "note": "ç»Ÿä¸€ç½‘å…³"}},
        {{"label": "ESBæ€»çº¿", "tech_stack": ["Kafka"], "note": "æ¶ˆæ¯æ€»çº¿"}},
        {{"label": "IoTå¹³å°", "tech_stack": ["EMQ X"], "note": "è®¾å¤‡æ¥å…¥"}},
        {{"label": "æ•°æ®äº¤æ¢", "tech_stack": ["DataX"], "note": "æ•°æ®åŒæ­¥"}}
      ]
    }},
    {{
      "name": "data",
      "layout": {{ "columns": 5 }},
      "items": [
        {{"label": "ä¸šåŠ¡æ•°æ®åº“", "tech_stack": ["PostgreSQL"], "note": "ä¸»æ•°æ®åº“"}},
        {{"label": "æ—¶åºæ•°æ®åº“", "tech_stack": ["InfluxDB"], "note": "IoTæ•°æ®"}},
        {{"label": "å›¾æ•°æ®åº“", "tech_stack": ["Neo4j"], "note": "å…³ç³»å›¾è°±"}},
        {{"label": "ç¼“å­˜å±‚", "tech_stack": ["Redis Cluster"], "note": "åˆ†å¸ƒå¼ç¼“å­˜"}},
        {{"label": "æ•°æ®æ¹–", "tech_stack": ["MinIO"], "note": "å¯¹è±¡å­˜å‚¨"}}
      ]
    }}
  ]
}}

{template_context}
**User Request:** "{request.user_input}"

Generate {template['name']} with optimal layout, clear layer organization, and appropriate item distribution.
"""

        system_prompt = f"""You are a professional flowchart generation expert. Create beautiful, well-organized flowcharts with optimal layout.

**AVAILABLE NODE TYPES:**
Basic Flow (for general processes):
- start: Start point (use shape="start-event" for BPMN style circle)
- end: End point (use shape="end-event" for BPMN style circle)
- process: Process/Task (use shape="task" for modern card style, or "rectangle" for simple box)
- decision: Decision/Gateway (use shape="diamond" for yes/no branches)
- data: Data Input/Output (use shape="parallelogram")
- subprocess: Sub-process (use shape="rounded-rectangle")

Technical Nodes (ONLY for technical architecture):
- api, service, database, cache, queue, storage, client, gateway

**LAYOUT ALGORITHM - CRITICAL (BALANCED GRID):**
1. **Main spine**: Start near center (x=0, y=100). Keep the core flow mostly top-to-bottom.
2. **Horizontal spread**: Use a 3-5 column grid with x positions spaced ~220-280px (e.g., -420, -200, 0, 220, 440). Do NOT stack everything in one column or only on the left.
3. **Decision branches**:
   - Place branch nodes at the SAME y-level as the decision.
   - Mirror positions left/right (e.g., x=-240 for "No", x=+240 for "Yes") to keep symmetry.
   - Rejoin branches after 1-2 steps at a centered node.
4. **Parallel processes**: Keep siblings aligned horizontally on the same y. Use even spacing (Â±240/Â±480 from center) so the layout feels balanced.
5. **Long flows**: After every 3-4 steps, gently shift the spine to an adjacent column (x=+200 or -200) to avoid a single vertical line while keeping flow mostly downward.
6. **Edge directions**:
   - Sequential: Vertical (top to bottom)
   - Branches: Diagonal or horizontal
   - Avoid all edges exiting from the same side; balance entry/exit points.

**LAYOUT EXAMPLE (Decision Flow):**
{{
  "nodes": [
    {{"id": "start", "type": "default", "position": {{"x": 400, "y": 100}},
      "data": {{"label": "å¼€å§‹", "shape": "start-event", "color": "#16a34a"}}}},
    {{"id": "step1", "type": "default", "position": {{"x": 400, "y": 280}},
      "data": {{"label": "æäº¤ç”³è¯·", "shape": "task", "color": "#2563eb"}}}},
    {{"id": "decision1", "type": "gateway", "position": {{"x": 400, "y": 460}},
      "data": {{"label": "å®¡æ‰¹é€šè¿‡?", "shape": "diamond"}}}},
    {{"id": "yes-branch", "type": "default", "position": {{"x": 400, "y": 640}},
      "data": {{"label": "å‘æ”¾é€šçŸ¥", "shape": "task", "color": "#2563eb"}}}},
    {{"id": "no-branch", "type": "default", "position": {{"x": 50, "y": 640}},
      "data": {{"label": "é©³å›å¹¶è¯´æ˜åŸå› ", "shape": "task", "color": "#dc2626"}}}},
    {{"id": "end", "type": "default", "position": {{"x": 400, "y": 820}},
      "data": {{"label": "ç»“æŸ", "shape": "end-event", "color": "#dc2626"}}}}
  ],
  "edges": [
    {{"id": "e1", "source": "start", "target": "step1"}},
    {{"id": "e2", "source": "step1", "target": "decision1"}},
    {{"id": "e3", "source": "decision1", "target": "yes-branch", "label": "æ˜¯"}},
    {{"id": "e4", "source": "decision1", "target": "no-branch", "label": "å¦"}},
    {{"id": "e5", "source": "yes-branch", "target": "end"}},
    {{"id": "e6", "source": "no-branch", "target": "end"}}
  ]
}}

**STYLING RULES:**
1. Use BPMN shapes for clean look:
   - start-event: Green (#16a34a)
   - end-event: Red (#dc2626)
   - task: Blue (#2563eb)
   - gateway/decision: No color override (default gray)
2. DO NOT use iconType field - system provides default icons
3. Label text should be concise (5-15 Chinese chars, 3-8 English words)
4. For loops: Position loop-back nodes to the right (+350px)

**GENERATION RULES:**
1. Analyze user input to identify:
   - Decision points (questions, conditions, branches)
   - Sequential steps
   - Parallel processes
   - Loop logic
2. For simple flows (5-8 nodes):
   - Linear top-to-bottom with 1-2 decisions
3. For complex flows (10-15 nodes):
   - Multiple decision branches
   - Use horizontal space for clarity
4. Avoid cluttered layouts:
   - Max 3 parallel branches
   - Clear visual separation between branches

**OUTPUT FORMAT:**
{{
  "nodes": [...],  // Array of node objects with position/type/data
  "edges": [...],  // Array of edge objects with source/target/label
  "mermaid_code": "graph TB\\n..."  // Optional Mermaid syntax
}}

{template_context}

**User Request:** "{request.user_input}"

Generate a well-laid-out flowchart. Focus on clarity and visual balance. Return ONLY valid JSON, no markdown blocks."""
        return system_prompt

    async def _call_ai_text_generation(self, vision_service, prompt: str, provider: str) -> dict:
        """Call AI provider (placeholder)."""
        if provider == "gemini":
            response = await vision_service._analyze_with_gemini_text(prompt)
        elif provider == "openai":
            response = await vision_service._analyze_with_openai_text(prompt)
        elif provider == "claude":
            response = await vision_service._analyze_with_claude_text(prompt)
        elif provider == "siliconflow":
            response = await vision_service._analyze_with_siliconflow_text(prompt)
        else:
            response = await vision_service._analyze_with_custom_text(prompt)
        return response

    def _bpmn_colors(self):
        return {
            "start": "#16a34a",
            "end": "#dc2626",
            "intermediate": "#ca8a04",
            "task": "#2563eb",
        }

    def _safe_json(self, payload: Any) -> dict:
        """Normalize AI payload into dict."""
        if payload is None:
            return {}
        if isinstance(payload, dict):
            return payload
        if isinstance(payload, str):
            # Clean markdown code blocks (```json ... ``` or ``` ... ```)
            cleaned = payload.strip()
            if cleaned.startswith("```"):
                # Remove opening ```json or ```
                lines = cleaned.split("\n")
                if lines[0].startswith("```"):
                    lines = lines[1:]
                # Remove closing ```
                if lines and lines[-1].strip() == "```":
                    lines = lines[:-1]
                cleaned = "\n".join(lines)
            return json.loads(cleaned)
        if hasattr(payload, "model_dump"):  # pydantic models
            return payload.model_dump()
        return json.loads(json.dumps(payload, default=str))

    def _calculate_frame_size(self, items_count: int, columns: int = 4) -> dict:
        """
        Calculate dynamic frame size based on number of items and columns.

        Args:
            items_count: Number of items in the layer
            columns: Number of columns in grid layout (default: 4)

        Returns:
            dict with 'width' and 'height' in pixels
        """
        item_width = 240  # Width of each component card
        item_height = 100  # Height of each component card
        padding = 60  # Frame padding
        gap = 20  # Gap between items
        header_height = 40  # Layer header height

        # Calculate actual columns (don't exceed items count)
        actual_columns = min(items_count, columns) if items_count > 0 else columns

        # Calculate rows needed
        rows = (items_count + columns - 1) // columns if items_count > 0 else 1

        # Calculate dimensions
        width = actual_columns * item_width + (actual_columns - 1) * gap + padding * 2
        height = rows * item_height + (rows - 1) * gap + padding * 2 + header_height

        return {
            "width": max(width, 800),  # Minimum width for aesthetics
            "height": max(height, 150),  # Minimum height
        }

    def _normalize_architecture_graph(self, ai_data: dict, architecture_type: str = "layered"):
        """
        Normalize architecture JSON (layers/items) into nodes with LayerFrame backgrounds.
        Supports dynamic sizing and grid layout.

        Args:
            ai_data: AI-generated architecture data
            architecture_type: Type of architecture (layered, business, technical, deployment, domain)

        Returns:
            Tuple of (nodes, edges, mermaid_code)
        """
        try:
            layers = (
                ai_data.get("layers")
                or ai_data.get("sections")
                or ai_data.get("architecture")
                or ai_data.get("groups")
                or []
            )
            # Extract edges if provided (for technical/deployment architectures)
            ai_edges = ai_data.get("edges", [])
        except Exception:
            return [], [], ""

        nodes = []
        edges = []

        # Enhanced layer colors with more types
        layer_colors = {
            # Layered architecture
            "frontend": "#f59e0b",      # Orange
            "backend": "#22c55e",       # Green
            "middleware": "#6366f1",    # Indigo
            "data": "#a855f7",          # Purple
            "infrastructure": "#0ea5e9", # Sky blue
            "observability": "#14b8a6", # Teal
            # Business architecture
            "capability": "#f59e0b",    # Orange
            "service": "#22c55e",       # Green
            "process": "#6366f1",       # Indigo
            "organization": "#a855f7",  # Purple
            # Technical architecture
            "presentation": "#f59e0b",  # Orange
            "application": "#22c55e",   # Green
            "integration": "#6366f1",   # Indigo
            # Deployment architecture
            "dmz": "#ef4444",           # Red
            "app-tier": "#22c55e",      # Green
            "data-tier": "#a855f7",     # Purple
            "monitoring": "#14b8a6",    # Teal
            # Domain architecture
            "domain-services": "#22c55e",     # Green
            "shared-kernel": "#6366f1",       # Indigo
            "anti-corruption": "#f59e0b",     # Orange
        }

        # Get template configuration
        template = ARCHITECTURE_TEMPLATES.get(architecture_type, ARCHITECTURE_TEMPLATES["layered"])
        default_columns = template.get("default_columns", 4)

        # Configuration for layout
        frame_padding_x = 60
        frame_padding_y = 100
        layer_spacing_y = 200
        item_width = 240
        item_height = 100
        padding = 60
        gap = 20
        header_height = 40

        if isinstance(layers, dict):
            layers = [{"name": k, "items": v} for k, v in layers.items()]
        elif not isinstance(layers, list):
            return [], [], ""

        # Track cumulative Y position
        current_y = frame_padding_y

        # Generate nodes with LayerFrame backgrounds
        for layer_idx, layer in enumerate(layers):
            layer_name = layer.get("name") if isinstance(layer, dict) else f"layer-{layer_idx}"
            items = layer.get("items", []) if isinstance(layer, dict) else []
            color = layer_colors.get(layer_name.lower(), "#64748b")

            # Get layout configuration for this layer
            layer_layout = layer.get("layout", {}) if isinstance(layer, dict) else {}
            columns = layer_layout.get("columns", default_columns)

            # Calculate dynamic frame size based on items count
            frame_size = self._calculate_frame_size(len(items), columns)

            # Add LayerFrame background node with dynamic sizing
            layer_frame_id = f"{layer_name}-frame"
            nodes.append({
                "id": layer_frame_id,
                "type": "layerFrame",
                "position": {"x": frame_padding_x, "y": current_y},
                "data": {
                    "label": layer_name.capitalize(),
                    "color": color,
                    "width": frame_size["width"],
                    "height": frame_size["height"],
                    "layout": "grid",  # NEW: Indicate grid layout mode
                    "columns": columns,  # NEW: Number of columns for grid
                    "itemsCount": len(items),  # NEW: Track item count
                },
                "draggable": False,
                # CRITICAL: Set style to define the draggable area for child nodes
                "style": {
                    "width": frame_size["width"],
                    "height": frame_size["height"],
                },
            })

            # Position component nodes in grid layout RELATIVE to parent
            for item_idx, item in enumerate(items):
                if isinstance(item, dict):
                    label = item.get("label") or item.get("name") or f"{layer_name}-{item_idx}"
                    tech_stack = item.get("tech_stack", []) or []
                    note = item.get("note") or (", ".join(tech_stack) if tech_stack else "")
                    category = item.get("category", "service")  # NEW: Category for icons
                else:
                    label = str(item)
                    note = ""
                    tech_stack = []
                    category = "service"

                # Calculate grid position
                row = item_idx // columns
                col = item_idx % columns

                # Calculate RELATIVE position (relative to parent LayerFrame)
                # Start from padding, not absolute coordinates
                item_x_relative = padding + col * (item_width + gap)
                item_y_relative = header_height + padding + row * (item_height + gap)

                nodes.append({
                    "id": f"{layer_name}-{item_idx}",
                    "type": "frame",
                    "position": {"x": item_x_relative, "y": item_y_relative},  # RELATIVE position
                    "parentNode": layer_frame_id,  # CRITICAL: Set parent relationship
                    "extent": "parent",  # CRITICAL: Constrain dragging within parent
                    "data": {
                        "label": label,
                        "shape": "task",
                        "color": color,
                        "layer": layer_name,
                        "note": note,
                        "layerColor": color,
                        "tech_stack": tech_stack if isinstance(tech_stack, list) else [],
                        "category": category,  # NEW: For visual enhancements
                    },
                })

            # Update current_y for next layer (with spacing)
            current_y += frame_size["height"] + layer_spacing_y

        # Process edges if provided (for technical/deployment architectures)
        for edge_idx, edge_data in enumerate(ai_edges):
            if isinstance(edge_data, dict):
                source = edge_data.get("source")
                target = edge_data.get("target")
                if source and target:
                    edges.append({
                        "id": edge_data.get("id") or f"e-{edge_idx}",
                        "source": source,
                        "target": target,
                        "label": edge_data.get("label", ""),
                        "type": edge_data.get("type", "dependency"),
                    })

        # Generate mermaid code
        mermaid_lines = [f"# {template['name']} ({template['name_en']})"]
        for layer in layers:
            lname = layer.get("name") if isinstance(layer, dict) else str(layer)
            mermaid_lines.append(f"## {lname.capitalize()}")
            items = layer.get("items", []) if isinstance(layer, dict) else []
            for item in items:
                if isinstance(item, dict):
                    label = item.get("label") or item.get("name") or "component"
                    tech_stack = item.get("tech_stack", []) or []
                    note = item.get("note") or ", ".join(tech_stack)
                    mermaid_lines.append(f"- **{label}**: {note}")
                else:
                    mermaid_lines.append(f"- {item}")
        mermaid_code = "\n".join(mermaid_lines)

        return nodes, edges, mermaid_code

    def _ensure_positions(self, nodes: List[dict]) -> List[dict]:
        """Guarantee each node has position/data/type for React Flow."""
        cleaned = []
        start_x, start_y, step_x, step_y = 120, 120, 260, 180
        for idx, node in enumerate(nodes):
            n = node.model_dump() if hasattr(node, "model_dump") else dict(node)
            label_value = n.pop("label", None)
            if "position" not in n or not isinstance(n.get("position"), dict):
                # Accept legacy x/y fields
                x = n.pop("x", None)
                y = n.pop("y", None)
                col, row = idx % 4, idx // 4
                n["position"] = {
                    "x": x if isinstance(x, (int, float)) else start_x + col * step_x,
                    "y": y if isinstance(y, (int, float)) else start_y + row * step_y + (50 if col % 2 else 0),
                }
            else:
                if "x" not in n["position"]:
                    n["position"]["x"] = start_x + (idx % 4) * step_x
                if "y" not in n["position"]:
                    n["position"]["y"] = start_y + (idx // 4) * step_y

            n.setdefault("type", "default")
            if "data" not in n or not isinstance(n["data"], dict):
                n["data"] = {}
            if label_value is not None:
                n["data"].setdefault("label", label_value)
            n["data"].setdefault("label", n.get("id", "node"))
            cleaned.append(n)
        return cleaned

    def _ensure_edges(self, edges: List[dict]) -> List[dict]:
        """Ensure edges have ids and required fields."""
        cleaned = []
        for idx, edge in enumerate(edges):
            e = edge.model_dump() if hasattr(edge, "model_dump") else dict(edge)
            if not e.get("source") or not e.get("target"):
                continue
            e.setdefault("id", e.get("id") or f"e{idx}")
            cleaned.append(e)
        return cleaned

    def _normalize_ai_graph(self, ai_data: dict) -> Tuple[List[dict], List[dict], str]:
        """Normalize AI response into nodes/edges/mermaid_code."""
        nodes = ai_data.get("nodes") or []
        edges = ai_data.get("edges") or []
        mermaid_code = (
            ai_data.get("mermaid_code")
            or ai_data.get("mermaid")
            or ai_data.get("mermaidCode")
            or ""
        )

        # If only mermaid is returned, parse it
        if (not nodes) and mermaid_code:
            try:
                parsed_nodes, parsed_edges = parse_mermaid_to_graph(mermaid_code)
                nodes = [n.model_dump() if hasattr(n, "model_dump") else dict(n) for n in parsed_nodes]
                edges = [e.model_dump() if hasattr(e, "model_dump") else dict(e) for e in parsed_edges]
            except Exception as e:
                logger.warning(f"Failed to parse mermaid from AI response: {e}")

        nodes = self._ensure_positions(nodes)
        edges = self._ensure_edges(edges)

        # Auto-wire edges sequentially if missing but multiple nodes exist
        if not edges and len(nodes) > 1:
            sorted_nodes = sorted(nodes, key=lambda n: (n["position"]["x"], n["position"]["y"]))
            auto_edges = []
            for idx in range(len(sorted_nodes) - 1):
                src = sorted_nodes[idx]["id"]
                tgt = sorted_nodes[idx + 1]["id"]
                auto_edges.append({"id": f"e{idx}", "source": src, "target": tgt, "label": None})
            edges = self._ensure_edges(auto_edges)

        if not mermaid_code and nodes:
            try:
                mermaid_code = graph_to_mermaid(
                    [Node(**n) for n in nodes],
                    [Edge(**e) for e in edges],
                )
            except Exception as e:
                logger.warning(f"Failed to rebuild mermaid code: {e}")
                mermaid_code = ""

        return nodes, edges, mermaid_code

    def _mock_microservice_architecture(self):
        c = self._bpmn_colors()
        nodes = [
            {"id": "start-1", "type": "default", "position": {"x": 50, "y": 200},
             "data": {"label": "å¼€å§‹", "shape": "start-event", "iconType": "play-circle", "color": c["start"]}},
            {"id": "client-1", "type": "client", "position": {"x": 200, "y": 200},
             "data": {"label": "Web Client"}},
            {"id": "gateway-1", "type": "gateway", "position": {"x": 500, "y": 200},
             "data": {"label": "API Gateway", "shape": "diamond"}},
            {"id": "gateway-2", "type": "gateway", "position": {"x": 800, "y": 100},
             "data": {"label": "è®¤è¯é€šè¿‡?", "shape": "diamond"}},
            {"id": "api-1", "type": "api", "position": {"x": 1100, "y": 100},
             "data": {"label": "Auth Service"}},
            {"id": "cache-1", "type": "cache", "position": {"x": 800, "y": 300},
             "data": {"label": "Redis Cache"}},
            {"id": "service-1", "type": "service", "position": {"x": 1100, "y": 300},
             "data": {"label": "Order Service", "shape": "task", "iconType": "box", "color": c["task"]}},
            {"id": "queue-1", "type": "queue", "position": {"x": 1400, "y": 200},
             "data": {"label": "RabbitMQ"}},
            {"id": "service-2", "type": "service", "position": {"x": 1700, "y": 200},
             "data": {"label": "Inventory Service", "shape": "task", "iconType": "box", "color": c["task"]}},
            {"id": "database-1", "type": "database", "position": {"x": 1400, "y": 400},
             "data": {"label": "MySQL"}},
            {"id": "storage-1", "type": "storage", "position": {"x": 1700, "y": 400},
             "data": {"label": "OSS Storage"}},
            {"id": "end-1", "type": "default", "position": {"x": 2000, "y": 200},
             "data": {"label": "ç»“æŸ", "shape": "end-event", "iconType": "stop-circle", "color": c["end"]}},
        ]
        edges = [
            {"id": "e0", "source": "start-1", "target": "client-1", "label": "ç”¨æˆ·è¯·æ±‚"},
            {"id": "e1", "source": "client-1", "target": "gateway-1", "label": "HTTPS Request"},
            {"id": "e2", "source": "gateway-1", "target": "gateway-2", "label": "éªŒè¯Token"},
            {"id": "e3", "source": "gateway-2", "target": "api-1", "label": "æ˜¯"},
            {"id": "e4", "source": "gateway-2", "target": "cache-1", "label": "å¦"},
            {"id": "e5", "source": "cache-1", "target": "service-1", "label": "Cache Miss"},
            {"id": "e6", "source": "service-1", "target": "queue-1", "label": "å¼‚æ­¥è®¢å•"},
            {"id": "e7", "source": "queue-1", "target": "service-2", "label": "æ¶ˆè´¹æ¶ˆæ¯"},
            {"id": "e8", "source": "service-1", "target": "database-1", "label": "æŸ¥è¯¢è®¢å•"},
            {"id": "e9", "source": "service-2", "target": "database-1", "label": "æ›´æ–°åº“å­˜"},
            {"id": "e10", "source": "service-2", "target": "storage-1", "label": "ä¸Šä¼ å‘è´§å•"},
            {"id": "e11", "source": "service-2", "target": "end-1", "label": "ç»“æŸ"},
        ]
        mermaid_code = """graph LR
    start-1(("å¼€å§‹"))
    client-1["Web Client"]
    gateway-1{"API Gateway"}
    gateway-2{"è®¤è¯é€šè¿‡?"}
    api-1["Auth Service"]
    cache-1["Redis Cache"]
    service-1["Order Service"]
    queue-1["RabbitMQ"]
    service-2["Inventory Service"]
    database-1["MySQL"]
    storage-1["OSS Storage"]
    end-1(("ç»“æŸ"))

    start-1 -->|ç”¨æˆ·è¯·æ±‚| client-1
    client-1 -->|HTTPS Request| gateway-1
    gateway-1 -->|éªŒè¯Token| gateway-2
    gateway-2 -->|æ˜¯| api-1
    gateway-2 -->|å¦| cache-1
    cache-1 -->|Cache Miss| service-1
    service-1 -->|å¼‚æ­¥è®¢å•| queue-1
    queue-1 -->|æ¶ˆè´¹æ¶ˆæ¯| service-2
    service-1 -->|æŸ¥è¯¢è®¢å•| database-1
    service-2 -->|æ›´æ–°åº“å­˜| database-1
    service-2 -->|ä¸Šä¼ å‘è´§å•| storage-1
    service-2 -->|ç»“æŸ| end-1"""
        return {"nodes": nodes, "edges": edges, "mermaid_code": mermaid_code}

    def _mock_high_concurrency(self):
        c = self._bpmn_colors()
        nodes = [
            {"id": "start-1", "type": "default", "position": {"x": -150, "y": 200},
             "data": {"label": "å¼€å§‹", "shape": "start-event", "iconType": "play-circle", "color": c["start"]}},
            {"id": "client-1", "type": "client", "position": {"x": 100, "y": 200}, "data": {"label": "Mobile App"}},
            {"id": "gateway-1", "type": "gateway", "position": {"x": 400, "y": 200},
             "data": {"label": "API Gateway\n(é™æµ 10000 QPS)", "shape": "diamond"}},
            {"id": "cache-1", "type": "cache", "position": {"x": 700, "y": 200},
             "data": {"label": "Redis\n(ç¼“å­˜å‰ç½®)"}},
            {"id": "gateway-2", "type": "gateway", "position": {"x": 1000, "y": 200},
             "data": {"label": "ç¼“å­˜å‘½ä¸­?", "shape": "diamond"}},
            {"id": "queue-1", "type": "queue", "position": {"x": 1300, "y": 100},
             "data": {"label": "Kafka Queue\n(å‰Šå³°)" }},
            {"id": "service-1", "type": "service", "position": {"x": 1600, "y": 100},
             "data": {"label": "Order Service", "shape": "task", "iconType": "box", "color": c["task"]}},
            {"id": "database-1", "type": "database", "position": {"x": 1900, "y": 100},
             "data": {"label": "MySQL\n(è®¢å•è¡¨)"}},
            {"id": "storage-1", "type": "storage", "position": {"x": 1900, "y": 300},
             "data": {"label": "OSS\n(è®¢å•å‡­è¯)"}},
            {"id": "client-2", "type": "client", "position": {"x": 1300, "y": 300},
             "data": {"label": "è¿”å›å¤±è´¥"}},
            {"id": "end-1", "type": "default", "position": {"x": 2200, "y": 200},
             "data": {"label": "ç»“æŸ", "shape": "end-event", "iconType": "stop-circle", "color": c["end"]}},
        ]
        edges = [
            {"id": "e0", "source": "start-1", "target": "client-1", "label": "å…¥å£"},
            {"id": "e1", "source": "client-1", "target": "gateway-1", "label": "é«˜å¹¶å‘è¯·æ±‚"},
            {"id": "e2", "source": "gateway-1", "target": "cache-1", "label": "é™æµé€šè¿‡"},
            {"id": "e3", "source": "cache-1", "target": "gateway-2", "label": "æ£€æŸ¥ç¼“å­˜"},
            {"id": "e4", "source": "gateway-2", "target": "queue-1", "label": "æ˜¯"},
            {"id": "e5", "source": "gateway-2", "target": "client-2", "label": "å¦"},
            {"id": "e6", "source": "queue-1", "target": "service-1", "label": "æ¶ˆè´¹"},
            {"id": "e7", "source": "service-1", "target": "database-1", "label": "åˆ›å»ºè®¢å•"},
            {"id": "e8", "source": "service-1", "target": "storage-1", "label": "å­˜å‚¨å‡­è¯"},
            {"id": "e9", "source": "service-1", "target": "end-1", "label": "ç»“æŸ"},
        ]
        mermaid_code = """graph LR
    start-1(("å¼€å§‹"))
    client-1["Mobile App"]
    gateway-1{"API Gateway<br/>(é™æµ 10000 QPS)"}
    cache-1["Redis<br/>(ç¼“å­˜å‰ç½®)"]
    gateway-2{"ç¼“å­˜å‘½ä¸­?"}
    queue-1["Kafka Queue<br/>(å‰Šå³°)"]
    service-1["Order Service"]
    database-1["MySQL<br/>(è®¢å•è¡¨)"]
    storage-1["OSS<br/>(è®¢å•å‡­è¯)"]
    client-2["è¿”å›å¤±è´¥"]
    end-1(("ç»“æŸ"))

    start-1 -->|å…¥å£| client-1
    client-1 -->|é«˜å¹¶å‘è¯·æ±‚| gateway-1
    gateway-1 -->|é™æµé€šè¿‡| cache-1
    cache-1 -->|æ£€æŸ¥ç¼“å­˜| gateway-2
    gateway-2 -->|æ˜¯| queue-1
    gateway-2 -->|å¦| client-2
    queue-1 -->|æ¶ˆè´¹| service-1
    service-1 -->|åˆ›å»ºè®¢å•| database-1
    service-1 -->|å­˜å‚¨å‡­è¯| storage-1
    service-1 -->|ç»“æŸ| end-1"""
        return {"nodes": nodes, "edges": edges, "mermaid_code": mermaid_code}

    def _mock_oom_investigation(self):
        c = self._bpmn_colors()
        nodes = [
            {"id": "start-1", "type": "default", "position": {"x": 250, "y": 50},
             "data": {"label": "æ£€æµ‹åˆ°æœåŠ¡å™¨ OOM å‘Šè­¦", "shape": "start-event", "iconType": "alert-circle", "color": c["start"]}},
            {"id": "check-1", "type": "default", "position": {"x": 250, "y": 180},
             "data": {"label": "æŸ¥çœ‹ JVM å †å†…å­˜ä½¿ç”¨ç‡", "shape": "task", "iconType": "box", "color": c["task"]}},
            {"id": "decision-1", "type": "gateway", "position": {"x": 250, "y": 310},
             "data": {"label": "å †å†…å­˜ > 90%?", "shape": "diamond"}},
            {"id": "step-heap-yes", "type": "default", "position": {"x": 100, "y": 440},
             "data": {"label": "ç”Ÿæˆ heap dump", "shape": "task", "iconType": "box", "color": c["task"]}},
            {"id": "step-heap-no", "type": "default", "position": {"x": 400, "y": 440},
             "data": {"label": "æ£€æŸ¥ç›´æ¥å†…å­˜/å…ƒç©ºé—´", "shape": "task", "iconType": "box", "color": c["task"]}},
            {"id": "step-analyze", "type": "default", "position": {"x": 100, "y": 570},
             "data": {"label": "ç”¨ MAT åˆ†æ heap dump", "shape": "task", "iconType": "box", "color": c["task"]}},
            {"id": "step-threads", "type": "default", "position": {"x": 400, "y": 570},
             "data": {"label": "åˆ†æçº¿ç¨‹æ ˆå’Œ GC æ—¥å¿—", "shape": "task", "iconType": "box", "color": c["task"]}},
            {"id": "decision-2", "type": "gateway", "position": {"x": 100, "y": 700},
             "data": {"label": "å‘ç°å†…å­˜æ³„æ¼?", "shape": "diamond"}},
            {"id": "step-leak-yes", "type": "default", "position": {"x": 100, "y": 830},
             "data": {"label": "ä¿®å¤æ³„æ¼ä»£ç å¹¶å›å½’", "shape": "task", "iconType": "box", "color": c["task"]}},
            {"id": "step-leak-no", "type": "default", "position": {"x": 250, "y": 830},
             "data": {"label": "ä¼˜åŒ–å†…å­˜é…ç½®/é™æµ", "shape": "task", "iconType": "box", "color": c["task"]}},
            {"id": "end-1", "type": "default", "position": {"x": 175, "y": 960},
             "data": {"label": "é‡å¯æœåŠ¡å¹¶ç›‘æ§", "shape": "end-event", "iconType": "stop-circle", "color": c["end"]}},
        ]
        edges = [
            {"id": "e1", "source": "start-1", "target": "check-1"},
            {"id": "e2", "source": "check-1", "target": "decision-1"},
            {"id": "e3", "source": "decision-1", "target": "step-heap-yes", "label": "æ˜¯"},
            {"id": "e4", "source": "decision-1", "target": "step-heap-no", "label": "å¦"},
            {"id": "e5", "source": "step-heap-yes", "target": "step-analyze"},
            {"id": "e6", "source": "step-heap-no", "target": "step-threads"},
            {"id": "e7", "source": "step-analyze", "target": "decision-2"},
            {"id": "e8", "source": "decision-2", "target": "step-leak-yes", "label": "æ˜¯"},
            {"id": "e9", "source": "decision-2", "target": "step-leak-no", "label": "å¦"},
            {"id": "e10", "source": "step-leak-yes", "target": "end-1"},
            {"id": "e11", "source": "step-leak-no", "target": "end-1"},
            {"id": "e12", "source": "step-threads", "target": "step-leak-no"},
        ]
        mermaid_code = """graph TD
    start-1(("æ£€æµ‹åˆ°æœåŠ¡å™¨ OOM å‘Šè­¦"))
    check-1["æŸ¥çœ‹ JVM å †å†…å­˜ä½¿ç”¨ç‡"]
    decision-1{"å †å†…å­˜ > 90%?"}
    step-heap-yes["ç”Ÿæˆ heap dump"]
    step-heap-no["æ£€æŸ¥ç›´æ¥å†…å­˜/å…ƒç©ºé—´"]
    step-analyze["ç”¨ MAT åˆ†æ heap dump"]
    step-threads["åˆ†æçº¿ç¨‹æ ˆå’Œ GC æ—¥å¿—"]
    decision-2{"å‘ç°å†…å­˜æ³„æ¼?"}
    step-leak-yes["ä¿®å¤æ³„æ¼ä»£ç å¹¶å›å½’"]
    step-leak-no["ä¼˜åŒ–å†…å­˜é…ç½®/é™æµ"]
    end-1(("é‡å¯æœåŠ¡å¹¶ç›‘æ§"))

    start-1 --> check-1
    check-1 --> decision-1
    decision-1 -->|æ˜¯| step-heap-yes
    decision-1 -->|å¦| step-heap-no
    step-heap-yes --> step-analyze
    step-heap-no --> step-threads
    step-analyze --> decision-2
    decision-2 -->|æ˜¯| step-leak-yes
    decision-2 -->|å¦| step-leak-no
    step-leak-yes --> end-1
    step-leak-no --> end-1
        step-threads --> step-leak-no"""
        return {"nodes": nodes, "edges": edges, "mermaid_code": mermaid_code}

    def _mock_architecture_overview(self):
        """Mock layered architecture map with no edges."""
        layers = [
            ("infrastructure", ["VPC/Subnet", "Load Balancer", "Object Storage", "Monitoring"]),
            ("middleware", ["API Gateway", "Kafka", "Redis", "Config Center"]),
            ("backend", ["User Service", "Order Service", "Payment Worker"]),
            ("frontend", ["Web SPA", "Mobile App", "BFF"]),
            ("data", ["MySQL", "ClickHouse", "Data Lake"]),
        ]
        colors = {
            "infrastructure": "#0ea5e9",
            "middleware": "#6366f1",
            "backend": "#22c55e",
            "frontend": "#f59e0b",
            "data": "#a855f7",
        }
        nodes = []
        frame_width = 1100
        frame_height = 180
        for li, (layer, items) in enumerate(layers):
            nodes.append(
                {
                    "id": f"{layer}-frame",
                    "type": "layerFrame",
                    "position": {"x": 60, "y": 100 + li * (frame_height + 20)},
                    "data": {
                        "label": layer.capitalize(),
                        "color": colors.get(layer, "#64748b"),
                        "width": frame_width,
                        "height": frame_height,
                    },
                    "draggable": False,
                }
            )
            for idx, label in enumerate(items):
                layer_tag = layer.capitalize()
                display_label = f"[{layer_tag}] {label}"
                nodes.append(
                    {
                        "id": f"{layer}-{idx}",
                        "type": "frame",
                        "position": {"x": 120 + idx * 240, "y": 140 + li * (frame_height + 20)},
                        "data": {
                            "label": display_label,
                            "shape": "task",
                            "iconType": "box",
                            "color": colors.get(layer, "#64748b"),
                            "layer": layer,
                            "note": layer_tag,
                            "layerColor": colors.get(layer, "#64748b"),
                        },
                    }
                )
        mermaid_lines = ["# Architecture Overview (mock)"]
        for layer, items in layers:
            mermaid_lines.append(f"- {layer}:")
            for label in items:
                mermaid_lines.append(f"  - {label}")
        return {"nodes": nodes, "edges": [], "mermaid_code": "\n".join(mermaid_lines)}

    # ============================================================
    # å¢é‡ç”Ÿæˆè¾…åŠ©æ–¹æ³• (Incremental Generation Helpers)
    # ============================================================

    def _build_architecture_description(
        self,
        nodes: List[Node],
        edges: List[Edge]
    ) -> str:
        """å°†ç°æœ‰æ¶æ„è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼ˆå‚è€ƒ Prompter æ¨¡å¼ï¼‰"""

        desc = f"### Current Architecture Overview\n\n"
        desc += f"**Total**: {len(nodes)} components, {len(edges)} connections\n\n"

        # 1. æŒ‰ç±»å‹åˆ†ç»„èŠ‚ç‚¹
        nodes_by_type = {}
        for node in nodes:
            node_type = node.type or "default"
            if node_type not in nodes_by_type:
                nodes_by_type[node_type] = []
            nodes_by_type[node_type].append(node)

        # 2. æè¿°å„ç±»å‹èŠ‚ç‚¹
        desc += "**Components by Type**:\n"
        for node_type, type_nodes in sorted(nodes_by_type.items()):
            desc += f"\n{node_type.upper()} ({len(type_nodes)}):\n"
            for node in type_nodes:
                desc += f"  - {node.data.label} (id: {node.id})\n"

        # 3. æè¿°è¿æ¥å…³ç³»
        desc += f"\n**Connections** ({len(edges)} total):\n"
        for edge in edges:
            # æŸ¥æ‰¾ source å’Œ target çš„ label
            source_node = next((n for n in nodes if n.id == edge.source), None)
            target_node = next((n for n in nodes if n.id == edge.target), None)

            source_label = source_node.data.label if source_node else edge.source
            target_label = target_node.data.label if target_node else edge.target
            edge_label = f" ({edge.label})" if edge.label else ""

            desc += f"  - {source_label} â†’ {target_label}{edge_label}\n"

        # 4. åˆ†ææ¶æ„ç‰¹å¾
        desc += "\n**Architecture Characteristics**:\n"

        # æ£€æµ‹åˆ†å±‚ç»“æ„ï¼ˆåŸºäº y åæ ‡ï¼‰
        y_coords = sorted(set(n.position.y for n in nodes))
        if len(y_coords) > 1:
            desc += f"  - Layered structure with {len(y_coords)} distinct layers\n"

        # æ£€æµ‹å…³é”®èŠ‚ç‚¹ï¼ˆå…¥åº¦/å‡ºåº¦é«˜çš„ï¼‰
        in_degree = {n.id: 0 for n in nodes}
        out_degree = {n.id: 0 for n in nodes}
        for edge in edges:
            out_degree[edge.source] = out_degree.get(edge.source, 0) + 1
            in_degree[edge.target] = in_degree.get(edge.target, 0) + 1

        hubs = [n for n in nodes if in_degree[n.id] + out_degree[n.id] > 2]
        if hubs:
            desc += f"  - {len(hubs)} hub components with multiple connections\n"
            for hub in hubs[:3]:  # åªæ˜¾ç¤ºå‰ 3 ä¸ª
                desc += f"    * {hub.data.label} ({in_degree[hub.id]} in, {out_degree[hub.id]} out)\n"

        return desc

    def _build_incremental_prompt(
        self,
        request: ChatGenerationRequest,
        existing_nodes: List[Node],
        existing_edges: List[Edge]
    ) -> str:
        """æ„å»ºå¢é‡ç”Ÿæˆçš„ Prompt"""

        # æå–ç°æœ‰èŠ‚ç‚¹ä¿¡æ¯
        existing_ids = [n.id for n in existing_nodes]
        max_x = max((n.position.x for n in existing_nodes), default=0)
        max_y = max((n.position.y for n in existing_nodes), default=0)
        min_y = min((n.position.y for n in existing_nodes), default=0)
        node_count = len(existing_nodes)
        edge_count = len(existing_edges)

        # æŒ‰ç±»å‹åˆ†ç»„èŠ‚ç‚¹
        nodes_by_type = {}
        for node in existing_nodes:
            node_type = node.type or "default"
            if node_type not in nodes_by_type:
                nodes_by_type[node_type] = []
            nodes_by_type[node_type].append(node.data.label)

        # ç”Ÿæˆæ—¶é—´æˆ³ç”¨äºæ–°èŠ‚ç‚¹ ID
        timestamp = int(time.time())

        # æ„å»ºç³»ç»Ÿ Prompt
        system_prompt = f"""You are an expert systems architect tasked with INCREMENTALLY enhancing an existing {"architecture" if request.diagram_type == "architecture" else "flowchart"}.

**CRITICAL CONSTRAINT: DO NOT SIMPLIFY THE EXISTING ARCHITECTURE**

This is an ENHANCEMENT task, NOT a REFACTORING task.

ABSOLUTE RULES:
1. PRESERVE COMPLEXITY: Keep all existing nodes with their EXACT labels, types, and properties
2. NO DELETION: Do NOT delete any existing nodes or edges
3. NO MODIFICATION: Do NOT change existing node labels, types, positions, or colors
4. NO MERGE: Do NOT merge or consolidate existing nodes
5. NO REARRANGEMENT: Do NOT change the existing layout or structure
6. ONLY ADD: You may ONLY add new nodes and new edges

The user wants to ENHANCE (add new features), not REFACTOR (restructure existing).
Treat the existing architecture as IMMUTABLE except for adding new nodes/edges.

If the user request seems to require modifying existing nodes, interpret it as
"add new nodes that complement the existing ones" instead.

---

**CRITICAL RULES FOR INCREMENTAL GENERATION:**
1. **PRESERVE ALL EXISTING NODES**: Keep all {node_count} nodes UNCHANGED (IDs: {', '.join(existing_ids[:10])}{', ...' if len(existing_ids) > 10 else ''})
2. **UNIQUE NEW IDs**: New nodes must use format `{{type}}-{timestamp}-{{sequence}}` (e.g., "service-{timestamp}-1")
3. **SMART POSITIONING**:
   - Existing bounds: x=[0, {max_x}], y=[{min_y}, {max_y}]
   - Place new nodes starting at x={max_x + 300}, y within [{min_y}, {max_y}]
   - Maintain spacing: 300px horizontal, 200px vertical
4. **PRESERVE EDGES**: Keep all {edge_count} existing connections unless explicitly removing
5. **TYPE CONSISTENCY**: Use existing node types where appropriate ({', '.join(nodes_by_type.keys())})

**EXISTING ARCHITECTURE SUMMARY:**
- Total: {node_count} nodes, {edge_count} edges
- Node types:
{chr(10).join(f'  - {t}: {len(nodes)} nodes ({", ".join(nodes[:3])}{"..." if len(nodes) > 3 else ""})' for t, nodes in nodes_by_type.items())}

**EXISTING ARCHITECTURE (NATURAL LANGUAGE)**:
{self._build_architecture_description(existing_nodes, existing_edges)}

**COMPLETE EXISTING STRUCTURE (JSON FORMAT)**:
```json
{json.dumps({
    "nodes": [n.model_dump() for n in existing_nodes],
    "edges": [e.model_dump() for e in existing_edges]
}, indent=2, ensure_ascii=False)}
```

**USER ENHANCEMENT REQUEST:**
"{request.user_input}"

**DIAGRAM TYPE:** {request.diagram_type}
**ARCHITECTURE TYPE:** {request.architecture_type if request.diagram_type == "architecture" else "N/A"}

**OUTPUT FORMAT:**
Return ONLY valid JSON (no markdown, no code blocks) with the COMPLETE architecture (existing + new):
{{
  "nodes": [
    ...ALL existing nodes UNCHANGED...,
    ...NEW nodes with unique IDs and proper positioning...
  ],
  "edges": [
    ...ALL existing edges...,
    ...NEW edges connecting new nodes...
  ]
}}

**VALIDATION CHECKLIST:**
- [ ] All {node_count} existing node IDs are present
- [ ] New node IDs use timestamp format ({timestamp})
- [ ] New nodes positioned at x >= {max_x + 300}
- [ ] All edges reference valid node IDs
- [ ] No duplicate node IDs

Generate the enhanced architecture now."""

        return system_prompt

    def _extract_semantic_keywords(self, label: str) -> set:
        """ä»èŠ‚ç‚¹labelæå–å…³é”®è¯­ä¹‰è¯"""
        import re

        # ç§»é™¤å¸¸è§ä¿®é¥°è¯
        noise_words = {'service', 'module', 'layer', 'system', 'component',
                       'æœåŠ¡', 'æ¨¡å—', 'å±‚', 'ç³»ç»Ÿ', 'ç»„ä»¶', 'api', 'db', 'database', 'server'}

        words = set()

        # è‹±æ–‡åˆ†è¯
        for word in re.findall(r'[A-Za-z]+', label.lower()):
            if word not in noise_words and len(word) > 2:
                words.add(word)

        # ä¸­æ–‡åˆ†è¯ï¼ˆæå–è¿ç»­ä¸­æ–‡å­—ç¬¦ï¼‰
        for word in re.findall(r'[\u4e00-\u9fff]+', label):
            if word not in noise_words and len(word) >= 2:
                words.add(word)

        return words

    def _validate_semantic_coverage(self, original_nodes: List[Node], final_nodes: List[Node]) -> bool:
        """éªŒè¯è¯­ä¹‰è¦†ç›–ç‡ - ç¡®ä¿åŸæœ‰æ¦‚å¿µæ²¡æœ‰ä¸¢å¤±"""

        # æå–åŸå§‹èŠ‚ç‚¹çš„æ‰€æœ‰è¯­ä¹‰å…³é”®è¯
        original_keywords = set()
        original_node_keywords = {}  # {node_id: keywords}

        for node in original_nodes:
            keywords = self._extract_semantic_keywords(node.data.label)
            if keywords:  # åªè€ƒè™‘æœ‰å®é™…è¯­ä¹‰çš„èŠ‚ç‚¹
                original_keywords.update(keywords)
                original_node_keywords[node.id] = {
                    'label': node.data.label,
                    'keywords': keywords
                }

        # æå–æœ€ç»ˆèŠ‚ç‚¹çš„æ‰€æœ‰è¯­ä¹‰å…³é”®è¯
        final_keywords = set()
        for node in final_nodes:
            keywords = self._extract_semantic_keywords(node.data.label)
            final_keywords.update(keywords)

        # æ£€æŸ¥ä¸¢å¤±çš„å…³é”®è¯
        lost_keywords = original_keywords - final_keywords

        if lost_keywords:
            logger.warning(
                f"Semantic content lost: keywords {lost_keywords} are missing in final architecture"
            )

            # æ‰¾å‡ºå“ªäº›èŠ‚ç‚¹çš„è¯­ä¹‰å®Œå…¨ä¸¢å¤±äº†
            lost_semantic_nodes = []
            for node_id, info in original_node_keywords.items():
                # å¦‚æœè¿™ä¸ªèŠ‚ç‚¹çš„æ‰€æœ‰å…³é”®è¯éƒ½ä¸åœ¨æœ€ç»ˆæ¶æ„ä¸­ï¼Œè¯´æ˜è¿™ä¸ªæ¦‚å¿µå®Œå…¨ä¸¢å¤±äº†
                if info['keywords'] and not info['keywords'].intersection(final_keywords):
                    lost_semantic_nodes.append(info['label'])

            if lost_semantic_nodes:
                logger.error(
                    f"CRITICAL: {len(lost_semantic_nodes)} nodes lost semantic content: {lost_semantic_nodes}"
                )
                logger.error(
                    "This means AI simplified the architecture instead of appending to it!"
                )
                return False

        # è®¡ç®—è¯­ä¹‰è¦†ç›–ç‡
        if original_keywords:
            coverage = len(final_keywords.intersection(original_keywords)) / len(original_keywords)
            logger.info(f"Semantic coverage: {coverage * 100:.1f}%")

            if coverage < 0.8:
                logger.error(
                    f"Semantic coverage too low ({coverage * 100:.1f}%), "
                    "significant content loss detected"
                )
                return False

        return True

    def _validate_incremental_result(
        self,
        original_nodes: List[Node],
        ai_nodes: List[Node]
    ) -> List[Node]:
        """æ›´ä¸¥æ ¼çš„éªŒè¯å’Œä¿®å¤ AI çš„å¢é‡ç”Ÿæˆç»“æœ"""
        original_id_map = {n.id: n for n in original_nodes}
        ai_id_map = {n.id: n for n in ai_nodes}

        # 1. æ£€æŸ¥ç¼ºå¤±èŠ‚ç‚¹ï¼ˆAI è¯¯åˆ ï¼‰
        missing_ids = set(original_id_map.keys()) - set(ai_id_map.keys())
        if missing_ids:
            logger.warning(f"AI deleted {len(missing_ids)} nodes: {missing_ids}, restoring them")
            # æ¢å¤ç¼ºå¤±èŠ‚ç‚¹
            for node_id in missing_ids:
                ai_nodes.append(original_id_map[node_id])
            # é‡å»º ai_id_map
            ai_id_map = {n.id: n for n in ai_nodes}

        # 2. æ£€æŸ¥ç°æœ‰èŠ‚ç‚¹çš„å±æ€§æ˜¯å¦è¢«ä¿®æ”¹
        position_modified_count = 0
        for node_id in original_id_map.keys():
            if node_id in ai_id_map:
                original_node = original_id_map[node_id]
                ai_node = ai_id_map[node_id]

                # æ£€æŸ¥å…³é”®å±æ€§
                if ai_node.data.label != original_node.data.label:
                    logger.warning(
                        f"Node label changed: {node_id} "
                        f"({original_node.data.label} â†’ {ai_node.data.label}), "
                        f"reverting to original"
                    )
                    ai_node.data.label = original_node.data.label

                if ai_node.type != original_node.type:
                    logger.warning(
                        f"Node type changed: {node_id} "
                        f"({original_node.type} â†’ {ai_node.type}), "
                        f"reverting to original"
                    )
                    ai_node.type = original_node.type

                # ğŸ”§ åœ¨å¢é‡æ¨¡å¼ä¸‹ï¼ŒåŸå§‹èŠ‚ç‚¹ä½ç½®ä¸åº”è¯¥å˜åŒ–ï¼ˆé™¤ééå¸¸å°çš„åç§» Â±5pxï¼‰
                pos_diff = abs(ai_node.position.x - original_node.position.x) + \
                           abs(ai_node.position.y - original_node.position.y)
                if pos_diff > 5:  # ä¸¥æ ¼é™åˆ¶ï¼šè¶…è¿‡ 5px å°±è®¤ä¸ºæ˜¯ç§»åŠ¨äº†
                    logger.warning(
                        f"Node position changed: {node_id} "
                        f"({original_node.position.x}, {original_node.position.y}) â†’ "
                        f"({ai_node.position.x}, {ai_node.position.y}), diff={pos_diff:.0f}px, "
                        f"reverting to original"
                    )
                    ai_node.position = original_node.position
                    position_modified_count += 1

        # å¦‚æœå¤§é‡èŠ‚ç‚¹ä½ç½®è¢«ä¿®æ”¹ï¼Œè¯´æ˜ AI é‡æ–°æ’åˆ—äº†æ•´ä¸ªæ¶æ„
        if position_modified_count > len(original_nodes) * 0.3:  # è¶…è¿‡ 30% çš„èŠ‚ç‚¹è¢«ç§»åŠ¨
            logger.error(
                f"âš ï¸ {position_modified_count}/{len(original_nodes)} nodes had positions changed! "
                f"AI appears to have reorganized the entire architecture instead of appending."
            )

        # 3. æ£€æŸ¥é‡å¤ ID
        seen_ids = set()
        deduplicated = []
        for node in ai_nodes:
            if node.id in seen_ids:
                # é‡å‘½åé‡å¤èŠ‚ç‚¹
                original_id = node.id
                node.id = f"{node.id}-dup-{int(time.time())}"
                logger.warning(f"Duplicate node ID: {original_id} â†’ {node.id}")
            seen_ids.add(node.id)
            deduplicated.append(node)

        # 4. æ£€æŸ¥ä½ç½®é‡å 
        deduplicated = self._resolve_position_overlaps(deduplicated)

        # 5. ğŸ†• è¯­ä¹‰å®Œæ•´æ€§æ£€æŸ¥ - ç¡®ä¿åŸæœ‰æ¦‚å¿µæ²¡æœ‰ä¸¢å¤±
        semantic_ok = self._validate_semantic_coverage(original_nodes, deduplicated)
        if not semantic_ok:
            logger.error(
                "Semantic validation failed! AI simplified architecture. "
                "Keeping ALL original nodes to preserve content."
            )
            # å¦‚æœè¯­ä¹‰ä¸¢å¤±ä¸¥é‡ï¼Œä¿ç•™æ‰€æœ‰åŸå§‹èŠ‚ç‚¹ï¼Œåªæ·»åŠ æ–°èŠ‚ç‚¹
            final_node_ids = {n.id for n in deduplicated}
            original_node_ids = {n.id for n in original_nodes}
            new_nodes = [n for n in deduplicated if n.id not in original_node_ids]

            logger.warning(
                f"Falling back to safe mode: keeping all {len(original_nodes)} original nodes + "
                f"{len(new_nodes)} new nodes"
            )
            deduplicated = list(original_nodes) + new_nodes

        # 6. ğŸ†• æ£€æŸ¥æ˜¯å¦çœŸçš„æ–°å¢äº†èŠ‚ç‚¹ - å…³é”®éªŒè¯ï¼
        original_node_ids = set(original_id_map.keys())
        final_node_ids = {n.id for n in deduplicated}
        new_node_ids = final_node_ids - original_node_ids

        if len(new_node_ids) == 0:
            logger.error(
                f"âŒ CRITICAL: No new nodes were added! "
                f"AI just rearranged existing {len(original_nodes)} nodes without adding requested content."
            )
            logger.error(
                "This is a failed incremental generation - user requested to ADD something, "
                "but AI only reorganized what already exists."
            )
        else:
            logger.info(f"âœ“ Successfully added {len(new_node_ids)} new nodes: {list(new_node_ids)[:5]}...")

        return deduplicated

    def _resolve_position_overlaps(self, nodes: List[Node]) -> List[Node]:
        """è§£å†³èŠ‚ç‚¹ä½ç½®é‡å """
        overlap_threshold = 100  # 100px ä»¥å†…è§†ä¸ºé‡å 

        for i, node in enumerate(nodes):
            for other in nodes[:i]:
                distance = math.sqrt(
                    (node.position.x - other.position.x)**2 +
                    (node.position.y - other.position.y)**2
                )
                if distance < overlap_threshold:
                    # å‘å³åç§»
                    node.position.x += 300
                    logger.info(f"Position overlap: shifted {node.id} to x={node.position.x}")

        return nodes

    def _merge_edges(self, original_edges: List[Edge], ai_edges: List[Edge]) -> List[Edge]:
        """æ™ºèƒ½åˆå¹¶è¾¹ï¼Œç¡®ä¿åŸå§‹è¾¹ä¸ä¸¢å¤±"""

        # 1. ä¸ºåŸå§‹è¾¹å»ºç«‹ç´¢å¼•ï¼ˆä½¿ç”¨ sourceâ†’target ä½œä¸ºç­¾åï¼‰
        original_edge_map = {}
        for edge in original_edges:
            sig = (edge.source, edge.target)
            original_edge_map[sig] = edge

        # 2. æ£€æŸ¥ AI æ˜¯å¦åˆ é™¤äº†åŸå§‹è¾¹
        ai_edge_sigs = {(e.source, e.target) for e in ai_edges}
        original_edge_sigs = set(original_edge_map.keys())

        missing_edge_sigs = original_edge_sigs - ai_edge_sigs
        if missing_edge_sigs:
            logger.warning(
                f"AI deleted {len(missing_edge_sigs)} edges: {missing_edge_sigs}, "
                f"restoring them"
            )

        # 3. åˆå¹¶ï¼šåŸå§‹è¾¹ + AI æ–°å¢çš„è¾¹
        merged = list(original_edges)  # ç¡®ä¿æ‰€æœ‰åŸå§‹è¾¹éƒ½ä¿ç•™

        for edge in ai_edges:
            sig = (edge.source, edge.target)
            if sig not in original_edge_map:
                # è¿™æ˜¯ AI æ–°å¢çš„è¾¹
                merged.append(edge)
            else:
                # è¿™æ˜¯åŸå§‹è¾¹ï¼Œæ£€æŸ¥ AI æ˜¯å¦ä¿®æ”¹äº† label
                original_edge = original_edge_map[sig]
                if edge.label != original_edge.label and original_edge.label:
                    logger.warning(
                        f"Edge label changed: {sig} "
                        f"({original_edge.label} â†’ {edge.label}), "
                        f"keeping original"
                    )
                    # å·²ç»åœ¨ merged ä¸­ä¿ç•™äº†åŸå§‹è¾¹ï¼Œä¸éœ€è¦é¢å¤–æ“ä½œ

        return merged

    async def generate_flowchart(
        self,
        request: ChatGenerationRequest,
        provider: str = "gemini",
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model_name: Optional[str] = None,
    ) -> ChatGenerationResponse:
        """Generate flowchart or architecture diagram via AI."""

        logger.info(f"Generating flowchart for input: {request.user_input[:50]}...")
        logger.info(f"Template ID: {request.template_id}")

        try:
            selected_provider = provider or request.provider or "gemini"
            logger.info(f"[CHAT-GEN] === START generate_flowchart ===")
            logger.info(f"[CHAT-GEN] user_input: {request.user_input[:100] if request.user_input else 'None'}...")
            logger.info(f"[CHAT-GEN] template_id: {request.template_id}")
            logger.info(f"[CHAT-GEN] diagram_type from request: {request.diagram_type}")

            # auto-detect architecture mode by template category if diagram_type not explicitly set
            effective_diagram_type = request.diagram_type or "flow"
            if request.template_id:
                tpl = self.get_template(request.template_id)
                if tpl and tpl.category == "architecture":
                    effective_diagram_type = "architecture"

            logger.info(f"[CHAT-GEN] Effective diagram type: {effective_diagram_type}")

            # è·å–æœ‰æ•ˆé…ç½®ï¼ˆä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤é¢„è®¾ï¼‰
            presets_service = get_model_presets_service()
            config = presets_service.get_active_config(
                provider=selected_provider,
                api_key=api_key or request.api_key,
                base_url=base_url or request.base_url,
                model_name=model_name or request.model_name
            )

            if not config:
                return ChatGenerationResponse(
                    success=False,
                    nodes=[],
                    edges=[],
                    mermaid_code="",
                    message="No AI configuration found. Please configure AI model in settings or provide API key."
                )

            vision_service = create_vision_service(
                provider=config["provider"],
                api_key=config["api_key"],
                base_url=config.get("base_url"),
                model_name=config.get("model_name"),
            )

            # ğŸ”§ Fix: ä½¿ç”¨é…ç½®ä¸­çš„å®é™… providerï¼Œè€Œä¸æ˜¯è¯·æ±‚ä¸­çš„ provider
            # è¿™æ ·å¯ä»¥ç¡®ä¿ _call_ai_text_generation ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•
            selected_provider = config["provider"]

            # ğŸ†• å¢é‡ç”Ÿæˆæ¨¡å¼ï¼šæ£€æŸ¥å¹¶è·å–ç°æœ‰æ¶æ„
            existing_nodes = []
            existing_edges = []
            session_id = request.session_id

            if request.incremental_mode and request.session_id:
                logger.info(f"[INCREMENTAL] Incremental mode enabled, loading session: {request.session_id}")
                session_manager = get_session_manager()
                session_data = session_manager.get_session(request.session_id)

                if session_data:
                    existing_nodes = [Node(**n) for n in session_data["nodes"]]
                    existing_edges = [Edge(**e) for e in session_data["edges"]]
                    logger.info(
                        f"[INCREMENTAL] Loaded {len(existing_nodes)} nodes, {len(existing_edges)} edges"
                    )
                else:
                    logger.warning(
                        f"[INCREMENTAL] Session {request.session_id} not found or expired, "
                        "falling back to full generation"
                    )
                    request.incremental_mode = False

            # æ„å»º Promptï¼ˆå¢é‡æˆ–å…¨æ–°ï¼‰
            prompt_request = request.model_copy(update={"diagram_type": effective_diagram_type})
            if request.incremental_mode and existing_nodes:
                logger.info("[INCREMENTAL] Building incremental prompt")
                prompt = self._build_incremental_prompt(prompt_request, existing_nodes, existing_edges)
            else:
                prompt = self._build_generation_prompt(prompt_request)

            logger.info(f"[CHAT-GEN] Calling AI with provider: {selected_provider}")
            logger.info(f"[CHAT-GEN] Prompt (first 200 chars): {prompt[:200]}...")
            ai_raw = await self._call_ai_text_generation(vision_service, prompt, selected_provider)
            logger.info(f"[CHAT-GEN] AI raw response type: {type(ai_raw)}, keys: {list(ai_raw.keys()) if isinstance(ai_raw, dict) else 'N/A'}")
            ai_data = self._safe_json(ai_raw)
            logger.info(f"[CHAT-GEN] Parsed AI data keys: {list(ai_data.keys())}")

            if effective_diagram_type == "architecture":
                # Pass architecture_type to normalization
                arch_type = request.architecture_type or "layered"
                nodes, edges, mermaid_code = self._normalize_architecture_graph(ai_data, arch_type)

                # Only suppress edges if template says not to show them
                template = ARCHITECTURE_TEMPLATES.get(arch_type, ARCHITECTURE_TEMPLATES["layered"])
                if not template.get("show_edges", False):
                    edges = []  # Business and layered architectures don't show edges
                # Technical and deployment architectures will keep their edges
            else:
                nodes, edges, mermaid_code = self._normalize_ai_graph(ai_data)

            logger.info(f"[CHAT-GEN] After normalization: {len(nodes)} nodes, {len(edges)} edges")

            # ğŸ†• å¢é‡æ¨¡å¼éªŒè¯å’Œåˆå¹¶
            if request.incremental_mode and existing_nodes:
                logger.info("[INCREMENTAL] Validating and merging incremental results")
                nodes = self._validate_incremental_result(existing_nodes, nodes)
                edges = self._merge_edges(existing_edges, edges)
                logger.info(
                    f"[INCREMENTAL] After merge: {len(nodes)} nodes (+{len(nodes) - len(existing_nodes)} new), "
                    f"{len(edges)} edges (+{len(edges) - len(existing_edges)} new)"
                )

            if not nodes:
                logger.warning(
                    "[CHAT-GEN] AI response missing nodes; raw keys: %s",
                    list(ai_data.keys()),
                )
                raise ValueError("AI response missing nodes; please retry with clearer input.")

            # If AI result is completely empty, use template mock as last resort
            # Changed from < 3 nodes to == 0 nodes to be less aggressive
            should_use_fallback = (
                effective_diagram_type == "flow"
                and len(nodes) == 0
                and request.template_id  # Only use template mock if a template was selected
            )

            if should_use_fallback:
                logger.warning(
                    "[CHAT-GEN] AI returned NO nodes; using template fallback"
                )
                if request.template_id == "microservice-architecture":
                    mock_result = self._mock_microservice_architecture()
                elif request.template_id == "high-concurrency-system":
                    mock_result = self._mock_high_concurrency()
                else:
                    mock_result = self._mock_oom_investigation()
                nodes = mock_result["nodes"]
                edges = mock_result["edges"]
                mermaid_code = mock_result["mermaid_code"]
                message = (
                    f"AI returned no nodes; using template mock"
                )
            else:
                message = f"Generated via {selected_provider}"
                logger.info(f"[CHAT-GEN] Using AI result (not falling back to mock)")

            logger.info(
                f"[CHAT-GEN] Generated via {selected_provider}: {len(nodes)} nodes, {len(edges)} edges"
            )

            # ğŸ†• æ›´æ–°ä¼šè¯ï¼ˆå¢é‡æ¨¡å¼æˆ–é¦–æ¬¡ä¿å­˜ï¼‰
            if request.incremental_mode or session_id:
                session_manager = get_session_manager()
                session_id = session_manager.create_or_update_session(
                    session_id=session_id,
                    nodes=nodes,
                    edges=edges
                )
                logger.info(f"[SESSION] Updated session: {session_id}")

            return ChatGenerationResponse(
                nodes=nodes,
                edges=edges,
                mermaid_code=mermaid_code,
                success=True,
                message=message,
                session_id=session_id  # ğŸ†• è¿”å› session_id
            )

        except Exception as e:
            logger.error(f"Flowchart generation failed: {e}", exc_info=True)
            # DO NOT return mock data - let the error propagate to the client
            raise HTTPException(
                status_code=500,
                detail=f"AI generation failed: {str(e)}. Please check your API key and try again."
            )


def create_chat_generator_service() -> ChatGeneratorService:
    """Create service instance."""
    return ChatGeneratorService()
