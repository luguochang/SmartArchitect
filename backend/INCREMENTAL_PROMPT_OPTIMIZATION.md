# Incremental Generation Prompt ä¼˜åŒ–æŠ¥å‘Š

**æ—¥æœŸ**: 2026-02-09
**é—®é¢˜**: AI å¢é‡ç”Ÿæˆæ—¶ JSON å“åº”è¢«æˆªæ–­ï¼ˆ1716 charsï¼‰
**æ ¹æœ¬åŸå› **: Prompt æ¶ˆè€—è¿‡å¤š tokensï¼Œè¶…å‡º Claude max_tokens é™åˆ¶

---

## é—®é¢˜è¯Šæ–­

### åŸå§‹é—®é¢˜
ç”¨æˆ·åé¦ˆå¢é‡ç”Ÿæˆå¤±è´¥ï¼ŒæŠ¥é”™ï¼š
```
ValueError: Invalid JSON response from AI after all repair attempts
[CUSTOM TEXT] Raw AI response length: 1716 chars
[CUSTOM TEXT] Last 500 chars: ...  "data
```

AI å“åº”åœ¨ JSON ä¸­é€”è¢«æˆªæ–­ï¼Œå³ä½¿ `stop_reason: end_turn`ï¼ˆé max_tokensï¼‰ã€‚

### æ ¹æœ¬åŸå› 
é€šè¿‡ `verify_prompt_size.py` è¯Šæ–­å‘ç°ï¼š

**Before ä¼˜åŒ–:**
- Prompt æ€»é•¿åº¦: 16,413 chars
- é¢„ä¼° tokens: 8,206
- Claude max_tokens: 8,192
- **å‰©ä½™ç»™ AI çš„ tokens: -14** âŒ

Prompt æœ¬èº«å·²ç»è¶…å‡º max_tokens é™åˆ¶ï¼ŒAI æ ¹æœ¬æ²¡æœ‰ç©ºé—´ç”Ÿæˆå“åº”ï¼

---

## ä¼˜åŒ–æªæ–½

### 1. ä¿®å¤ Claude max_tokens é™åˆ¶ (ai_vision.py:2275)

**é—®é¢˜**: ä»£ç è®¾ç½® `max_tokens: 16384`ï¼Œä½† Claude Sonnet 3.5 çš„å®é™…è¾“å‡ºé™åˆ¶æ˜¯ **8192 tokens**

**ä¿®å¤**:
```python
data = {
    "model": model_name,
    "max_tokens": 8192,  # Changed from 16384
    ...
}
```

### 2. ç´§å‡‘ JSON æ ¼å¼ (chat_generator.py:1165-1183)

**é—®é¢˜**: ä½¿ç”¨ `json.dumps(indent=2)` ç”Ÿæˆ pretty-printed JSONï¼Œæµªè´¹å¤§é‡ç©ºé—´

**Before** (Pretty-printed):
```json
{
  "nodes": [
    {
      "id": "api-1",
      "type": "api",
      "position": {
        "x": 100,
        "y": 100
      },
      "data": {
        "label": "API Gateway"
      }
    },
    ...
  ]
}
```
- 25 nodes: ~8,900 chars (~4,450 tokens)

**After** (Compact, æ¯ä¸ªèŠ‚ç‚¹ä¸€è¡Œ):
```json
{
  "nodes": [
    {"id":"api-1","type":"api","position":{"x":100,"y":100},"data":{"label":"API Gateway"}},
    ...
  ]
}
```
- 25 nodes: ~5,459 chars (~2,729 tokens)
- **èŠ‚çœ: 3,441 chars (38.7%)**

**å®ç°**:
```python
def _format_nodes_compact(self, nodes: List[Node], indent: int = 4) -> str:
    """æ¯ä¸ªèŠ‚ç‚¹ä¸€è¡Œï¼Œä½¿ç”¨ separators=(',', ':') å»é™¤ç©ºæ ¼"""
    indent_str = " " * indent
    lines = []
    for i, node in enumerate(nodes):
        node_json = json.dumps(node.model_dump(), ensure_ascii=False, separators=(',', ':'))
        comma = "," if i < len(nodes) - 1 else ""
        lines.append(f"{indent_str}{node_json}{comma}")
    return "\n".join(lines)
```

### 3. åˆ é™¤å†—ä½™çš„è‡ªç„¶è¯­è¨€æè¿° (chat_generator.py:1237-1239)

**é—®é¢˜**: Prompt åŒæ—¶åŒ…å«è‡ªç„¶è¯­è¨€æè¿°å’Œå®Œæ•´ JSONï¼Œé‡å¤ä¿¡æ¯

**Before**:
```
**ğŸ” COMPACT JSON OF EXISTING ARCHITECTURE (FOR YOUR REFERENCE):**

### Current Architecture Overview

**Total**: 25 components, 24 connections

**Components by Type**:

API (2):
  - API Gateway (id: api-1)
  - Auth API (id: api-2)

SERVICE (5):
  - User Service (id: service-1)
  - Order Service (id: service-2)
  ...

**Connections** (24 total):
  - API Gateway â†’ User Service
  - API Gateway â†’ Order Service
  ...

**CRITICAL: YOUR OUTPUT MUST INCLUDE ALL 25 NODES BELOW:**
{JSON}
```

**After** (åªä¿ç•™ JSON):
```
**CRITICAL: YOUR OUTPUT MUST INCLUDE ALL 25 NODES BELOW (copy them EXACTLY):**
{JSON}

**âš ï¸ VALIDATION:** Output count MUST be > 25 (you must ADD at least 1 node)
```

**èŠ‚çœ**: ~2,000+ chars (~1,000 tokens)

### 4. å‹ç¼© Few-Shot ç¤ºä¾‹ (chat_generator.py:1274-1295)

**é—®é¢˜**: Few-Shot ç¤ºä¾‹ç”¨ 5 èŠ‚ç‚¹è¯¦ç»†å±•ç¤ºæ­£ç¡®/é”™è¯¯è¾“å‡ºï¼Œå ç”¨å¤§é‡ç©ºé—´

**Before** (5 nodes, pretty-printed):
```
**Example Scenario:**

Existing Architecture (5 nodes):
{å®Œæ•´ JSON 5 nodes}

User Request: "åœ¨æœåŠ¡å’Œæ•°æ®åº“ä¹‹é—´æ·»åŠ Redisç¼“å­˜å±‚"

âœ… CORRECT OUTPUT (ALL 5 existing nodes preserved + 2 new nodes added):
{å®Œæ•´ JSON 7 nodesï¼Œå¸¦è¯¦ç»†æ³¨é‡Š}

Why this is correct:
- âœ… All 5 original nodes...
- âœ… 2 NEW nodes...
...

âŒ WRONG OUTPUT (modifying existing node labels):
{å®Œæ•´ JSON 5 nodesï¼Œå¸¦é”™è¯¯ç¤ºèŒƒ}

Why this is wrong:
- âŒ Modified existing node labels...
- âŒ No NEW nodes...
...

KEY LESSON:
When user says "add X"...
```
- é•¿åº¦: ~2,000 chars (~1,000 tokens)

**After** (3 nodes, minimal format):
```
**ğŸ“š FEW-SHOT EXAMPLE:**

Existing (3 nodes):
{minimal JSON, one line}

Request: "Add cache between service and DB"

âœ… CORRECT (3 kept + 1 new = 4 total):
{minimal JSON, one line with ...}

âŒ WRONG (modified labels, no new node):
{minimal JSON, one line}

**KEY:** "add" = CREATE new nodes, NOT modify labels.
```
- é•¿åº¦: ~400 chars (~200 tokens)
- **èŠ‚çœ: 1,600 chars (80%)**

### 5. æ·»åŠ  Prompt é•¿åº¦æ—¥å¿— (chat_generator.py:1807-1821)

**ç›®çš„**: å®æ—¶ç›‘æ§ Prompt çš„ token æ¶ˆè€—ï¼Œä¾¿äºè¯Šæ–­é—®é¢˜

**å®ç°**:
```python
prompt_length = len(prompt)
estimated_tokens = prompt_length // 2
max_output_tokens = 8192
logger.warning(
    f"[PROMPT-LENGTH] Total: {prompt_length} chars "
    f"(~{estimated_tokens} tokens, max_output={max_output_tokens}, "
    f"leaves ~{max_output_tokens - estimated_tokens} tokens for AI response)"
)
```

**ç¤ºä¾‹è¾“å‡º**:
```
[PROMPT-LENGTH] Total: 10880 chars (~5440 tokens, max_output=8192, leaves ~2752 tokens for AI response)
```

---

## ä¼˜åŒ–æ•ˆæœ

### Token æ¶ˆè€—å¯¹æ¯”

| é¡¹ç›® | Before | After | èŠ‚çœ |
|------|--------|-------|------|
| Prompt æ€»é•¿åº¦ | 16,413 chars | 10,880 chars | -5,533 chars (-33.7%) |
| é¢„ä¼° tokens | 8,206 | 5,440 | -2,766 tokens (-33.7%) |
| Claude max_tokens | 8,192 | 8,192 | - |
| **å‰©ä½™ AI å¯ç”¨ tokens** | **-14** âŒ | **+2,752** âœ… | **+2,766 tokens** |

### å„éƒ¨åˆ†ä¼˜åŒ–è´¡çŒ®

| ä¼˜åŒ–é¡¹ | èŠ‚çœ tokens | å æ¯” |
|--------|-------------|------|
| ç´§å‡‘ JSON æ ¼å¼ | ~1,720 | 62% |
| åˆ é™¤è‡ªç„¶è¯­è¨€æè¿° | ~1,000 | 36% |
| å‹ç¼© Few-Shot ç¤ºä¾‹ | ~800 | 29% |
| **æ€»è®¡ (æœ‰é‡å )** | **~2,766** | **100%** |

---

## éªŒè¯ç»“æœ

### éªŒè¯è„šæœ¬ (verify_prompt_size.py)

**æµ‹è¯•æ¡ä»¶**:
- 25 èŠ‚ç‚¹åˆå§‹æ¶æ„
- 24 æ¡è¾¹
- è¯·æ±‚: "åœ¨æœåŠ¡å±‚å’Œæ•°æ®åº“ä¹‹é—´æ·»åŠ  Redis ç¼“å­˜å±‚"

**è¾“å‡º**:
```
JSON æ ¼å¼å¤§å°å¯¹æ¯”
Pretty-printed:  8,900 chars  (~4,450 tokens)
Compact (1/line): 5,459 chars  (~2,729 tokens)
Minimal (no ws):  5,209 chars  (~2,604 tokens)

Compact èŠ‚çœ:  3,441 chars  (38.7%)

å®Œæ•´å¢é‡ Prompt å¤§å°åˆ†æ
æµ‹è¯•æ¶æ„: 23 èŠ‚ç‚¹, 24 æ¡è¾¹

[Prompt ç»Ÿè®¡]
  - æ€»é•¿åº¦: 10,880 chars
  - é¢„ä¼° tokens: 5,440
  - Claude max_tokens: 8,192
  - å‰©ä½™ç»™ AI çš„ tokens: 2,752

âœ… æ­£å¸¸: å‰©ä½™ 2,752 tokens è¶³å¤Ÿ AI ç”Ÿæˆå“åº”
```

### é¢„æœŸæ•ˆæœ

**Before ä¼˜åŒ–**:
- AI æ— æ³•ç”Ÿæˆå“åº”ï¼ˆ-14 tokens å‰©ä½™ï¼‰
- JSON è¢«æˆªæ–­åˆ° 1716 chars
- æŠ¥é”™: `Invalid JSON response from AI`

**After ä¼˜åŒ–**:
- AI æœ‰ 2,752 tokens ç”Ÿæˆç©ºé—´
- å¯ä»¥ç”Ÿæˆçº¦ 5,500 chars çš„ JSON
- è¶³å¤ŸåŒ…å« 25 ä¸ªåŸå§‹èŠ‚ç‚¹ + æ–°å¢èŠ‚ç‚¹

---

## æ–‡ä»¶ä¿®æ”¹æ¸…å•

### åç«¯ä¿®æ”¹ (3 ä¸ªæ–‡ä»¶)

| æ–‡ä»¶ | ä¿®æ”¹å†…å®¹ | è¡Œæ•° |
|------|----------|------|
| `backend/app/services/ai_vision.py` | ä¿®å¤ Claude max_tokens: 16384 â†’ 8192 | 2273-2284 |
| `backend/app/services/chat_generator.py` | æ–°å¢ `_format_nodes_compact()` å’Œ `_format_edges_compact()` | 1165-1183 |
| `backend/app/services/chat_generator.py` | åˆ é™¤è‡ªç„¶è¯­è¨€æè¿°ï¼Œä½¿ç”¨ç´§å‡‘ JSON | 1252-1270 |
| `backend/app/services/chat_generator.py` | å‹ç¼© Few-Shot ç¤ºä¾‹ (5 nodes â†’ 3 nodes) | 1274-1295 |
| `backend/app/services/chat_generator.py` | æ·»åŠ  Prompt é•¿åº¦æ—¥å¿— | 1807-1821 |

### æ–°å¢å·¥å…·è„šæœ¬

| æ–‡ä»¶ | ç”¨é€” |
|------|------|
| `backend/verify_prompt_size.py` | éªŒè¯ Prompt å¤§å°å’Œ token æ¶ˆè€— |

---

## åç»­å»ºè®®

### çŸ­æœŸ (æœ¬æ¬¡ä¿®å¤)
1. âœ… å·²å®Œæˆ: ä¿®å¤ max_tokens é™åˆ¶
2. âœ… å·²å®Œæˆ: ç´§å‡‘ JSON æ ¼å¼
3. âœ… å·²å®Œæˆ: åˆ é™¤å†—ä½™æè¿°
4. âœ… å·²å®Œæˆ: å‹ç¼© Few-Shot ç¤ºä¾‹
5. â³ å¾…æ‰§è¡Œ: è¿è¡Œ E2E æµ‹è¯•éªŒè¯å®é™…æ•ˆæœ

### ä¸­æœŸä¼˜åŒ–ï¼ˆå¦‚æœä»æœ‰é—®é¢˜ï¼‰
1. **åŠ¨æ€ Few-Shot ç¤ºä¾‹**: æ ¹æ®ç°æœ‰èŠ‚ç‚¹æ•°é‡è°ƒæ•´ç¤ºä¾‹å¤§å°
   - < 10 nodes: å®Œæ•´ç¤ºä¾‹
   - 10-20 nodes: ç®€åŒ–ç¤ºä¾‹
   - > 20 nodes: æœ€å°ç¤ºä¾‹æˆ–ç§»é™¤

2. **åˆ†æ®µ Prompt**: å¯¹äºè¶…å¤§æ¶æ„ï¼ˆ50+ nodesï¼‰ï¼Œè€ƒè™‘ï¼š
   - åªå‘é€å…³é”®èŠ‚ç‚¹ï¼ˆhub nodesï¼‰
   - æˆ–åˆ†æ‰¹å¤„ç†ï¼ˆå…ˆæ·»åŠ åˆ°å°èŒƒå›´ï¼Œå†åˆå¹¶ï¼‰

3. **è‡ªé€‚åº” max_tokens**: æ ¹æ® Prompt é•¿åº¦åŠ¨æ€è°ƒæ•´
   ```python
   max_output_tokens = min(8192, context_window - prompt_tokens - 500)
   ```

### é•¿æœŸä¼˜åŒ–
1. **ä½¿ç”¨ Claude Opus 4.5**: æ›´å¤§çš„ context window (200K â†’ å®é™…å¯èƒ½æ›´é«˜)
2. **Two-Stage Generation**: å…ˆç”ŸæˆèŠ‚ç‚¹åˆ—è¡¨ï¼Œå†ç”Ÿæˆå®Œæ•´ JSON
3. **Diff-based Output**: AI åªè¿”å›æ–°å¢èŠ‚ç‚¹ï¼Œåç«¯åˆå¹¶ï¼ˆè§ diff-based-feasibility-analysis.mdï¼‰

---

## é™„å½•ï¼šå…³é”®ä»£ç ç‰‡æ®µ

### ç´§å‡‘ JSON æ ¼å¼åŒ–

```python
def _format_nodes_compact(self, nodes: List[Node], indent: int = 4) -> str:
    """å°†èŠ‚ç‚¹åˆ—è¡¨æ ¼å¼åŒ–ä¸ºç´§å‡‘çš„ JSONï¼ˆæ¯ä¸ªèŠ‚ç‚¹ä¸€è¡Œï¼ŒèŠ‚çœ tokensï¼‰"""
    indent_str = " " * indent
    lines = []
    for i, node in enumerate(nodes):
        node_json = json.dumps(node.model_dump(), ensure_ascii=False, separators=(',', ':'))
        comma = "," if i < len(nodes) - 1 else ""
        lines.append(f"{indent_str}{node_json}{comma}")
    return "\n".join(lines)
```

### Prompt é•¿åº¦ç›‘æ§

```python
# ğŸ” DEBUG: Log prompt length to diagnose token consumption
prompt_length = len(prompt)
estimated_tokens = prompt_length // 2  # Conservative estimate
max_output_tokens = 8192  # Claude Sonnet 3.5 limit
logger.warning(
    f"[PROMPT-LENGTH] Total: {prompt_length} chars "
    f"(~{estimated_tokens} tokens, max_output={max_output_tokens}, "
    f"leaves ~{max_output_tokens - estimated_tokens} tokens for AI response)"
)
```

---

## æ€»ç»“

**é—®é¢˜**: Prompt è¿‡é•¿ï¼ˆ16,413 charsï¼‰è¶…å‡º Claude max_tokens (8,192)ï¼ŒAI æ— æ³•ç”Ÿæˆå“åº”

**è§£å†³æ–¹æ¡ˆ**:
1. ä¿®å¤ max_tokens é…ç½®ï¼ˆ16384 â†’ 8192ï¼‰
2. ç´§å‡‘ JSON æ ¼å¼ï¼ˆèŠ‚çœ 38.7%ï¼‰
3. åˆ é™¤å†—ä½™è‡ªç„¶è¯­è¨€æè¿°
4. å‹ç¼© Few-Shot ç¤ºä¾‹ï¼ˆèŠ‚çœ 80%ï¼‰
5. æ·»åŠ å®æ—¶ Prompt é•¿åº¦ç›‘æ§

**æ•ˆæœ**: Prompt å¤§å°ä» 16,413 â†’ 10,880 chars (-33.7%)ï¼Œå‰©ä½™ tokens ä» -14 â†’ +2,752 âœ…

**çŠ¶æ€**: âœ… å·²ä¼˜åŒ–ï¼Œç­‰å¾… E2E æµ‹è¯•éªŒè¯

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2026-02-09
**ä½œè€…**: Claude Code
**ç›¸å…³æ–‡æ¡£**: `INCREMENTAL_GENERATION_TEST_REPORT.md`, `diff-based-feasibility-analysis.md`
